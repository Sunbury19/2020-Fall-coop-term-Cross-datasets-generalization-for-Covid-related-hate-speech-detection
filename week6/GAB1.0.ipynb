{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gab Model 1.0\n",
    "2020.10.10\n",
    "\n",
    "Use the data extract from gab.ai website to train a model, and test it on two data sets.\n",
    "\n",
    "Student: Xuanyu Su                                                                 \n",
    "Supervisor: Isar Nejadgholi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "GAB = pd.read_csv('Data/GabHateCorpus_annotations.tsv', encoding='utf-8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate</th>\n",
       "      <th>HD</th>\n",
       "      <th>CV</th>\n",
       "      <th>VO</th>\n",
       "      <th>REL</th>\n",
       "      <th>RAE</th>\n",
       "      <th>SXO</th>\n",
       "      <th>GEN</th>\n",
       "      <th>IDL</th>\n",
       "      <th>NAT</th>\n",
       "      <th>POL</th>\n",
       "      <th>MPH</th>\n",
       "      <th>EX</th>\n",
       "      <th>IM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27044</td>\n",
       "      <td>4</td>\n",
       "      <td>Ah the PSYOPS antifa crew is back. That‚Äôs how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27044</td>\n",
       "      <td>15</td>\n",
       "      <td>Ah the PSYOPS antifa crew is back. That‚Äôs how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27044</td>\n",
       "      <td>10</td>\n",
       "      <td>Ah the PSYOPS antifa crew is back. That‚Äôs how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27044</td>\n",
       "      <td>8</td>\n",
       "      <td>Ah the PSYOPS antifa crew is back. That‚Äôs how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27045</td>\n",
       "      <td>4</td>\n",
       "      <td>Get the new Android app update released today ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Annotator                                               Text  Hate  \\\n",
       "0  27044          4  Ah the PSYOPS antifa crew is back. That‚Äôs how ...     0   \n",
       "1  27044         15  Ah the PSYOPS antifa crew is back. That‚Äôs how ...     0   \n",
       "2  27044         10  Ah the PSYOPS antifa crew is back. That‚Äôs how ...     0   \n",
       "3  27044          8  Ah the PSYOPS antifa crew is back. That‚Äôs how ...     0   \n",
       "4  27045          4  Get the new Android app update released today ...     0   \n",
       "\n",
       "   HD  CV  VO  REL  RAE  SXO  GEN  IDL  NAT  POL  MPH  EX  IM  \n",
       "0   0   0   0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  \n",
       "1   0   0   0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  \n",
       "2   0   0   0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  \n",
       "3   0   0   0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  \n",
       "4   0   0   0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah the PSYOPS antifa crew is back. That‚Äôs how I know we are winning.\n",
      "Ah the PSYOPS antifa crew is back. That‚Äôs how I know we are winning.\n",
      "Ah the PSYOPS antifa crew is back. That‚Äôs how I know we are winning.\n",
      "Ah the PSYOPS antifa crew is back. That‚Äôs how I know we are winning.\n",
      "Get the new Android app update released today here:¬†https://gab.ai/about/android\n",
      "Get the new Android app update released today here:¬†https://gab.ai/about/android\n",
      "Get the new Android app update released today here:¬†https://gab.ai/about/android\n",
      "Study: Google Pushes Liberal News in Top 5 Search Suggestions https://www.newsbusters.org/blogs/techwatch/corinn...\n",
      "Study: Google Pushes Liberal News in Top 5 Search Suggestions https://www.newsbusters.org/blogs/techwatch/corinn...\n",
      "Study: Google Pushes Liberal News in Top 5 Search Suggestions https://www.newsbusters.org/blogs/techwatch/corinn...\n",
      "Study: Google Pushes Liberal News in Top 5 Search Suggestions https://www.newsbusters.org/blogs/techwatch/corinn...\n",
      "#Breaking: Claiming bias against conservatives, @FreedomWatchUSA has filed a class action lawsuit against Google, Facebook, Instagram, Twitter, and Apple.\n",
      "#Breaking: Claiming bias against conservatives, @FreedomWatchUSA has filed a class action lawsuit against Google, Facebook, Instagram, Twitter, and Apple.\n",
      "#Breaking: Claiming bias against conservatives, @FreedomWatchUSA has filed a class action lawsuit against Google, Facebook, Instagram, Twitter, and Apple.\n",
      "#Breaking: Claiming bias against conservatives, @FreedomWatchUSA has filed a class action lawsuit against Google, Facebook, Instagram, Twitter, and Apple.\n",
      "‚ÄúThese [companies] are run by sociopaths,‚Äù he said. ‚ÄúThese people are complete narcissists. These people ought to be controlled, they ought to be regulated.‚Äù At one point during the phone call, Bannon said, ‚ÄúThese people are evil.' https://www.vanityfair.com/news/2018/08/steve-bann...\n"
     ]
    }
   ],
   "source": [
    "[print(i) for i in GAB.loc[:15]['Text']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = GAB.groupby('ID')['Hate'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in new_label:\n",
    "    if i is True:\n",
    "        temp.append(1.0)\n",
    "    else:\n",
    "        temp.append(0.0)\n",
    "new_label = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_GAB = GAB.drop_duplicates(subset=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-a2733beb0c54>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_GAB['Hate'] = temp\n"
     ]
    }
   ],
   "source": [
    "new_GAB['Hate'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_GAB = new_GAB[['Text','Hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ah the PSYOPS antifa crew is back. That‚Äôs how ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Get the new Android app update released today ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Study: Google Pushes Liberal News in Top 5 Sea...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#Breaking: Claiming bias against conservatives...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>‚ÄúThese [companies] are run by sociopaths,‚Äù he ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Hate\n",
       "0   Ah the PSYOPS antifa crew is back. That‚Äôs how ...   0.0\n",
       "4   Get the new Android app update released today ...   0.0\n",
       "7   Study: Google Pushes Liberal News in Top 5 Sea...   0.0\n",
       "11  #Breaking: Claiming bias against conservatives...   1.0\n",
       "15  ‚ÄúThese [companies] are run by sociopaths,‚Äù he ...   0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_GAB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_GAB = new_GAB.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Through the Arches, Mission San Juan Capistrano' üñºÔ∏è   by Joseph Kleitsch  #fineart #architecture #impressionism\n",
      "'Look Maggie, I'm walkin, Maggie' üòáüòç   https://youtu.be/whS9K5rL_FU  #cuteness #babyfever\n",
      "'Night Slid Down' by Emma Florence Harrison üé®   #ArtNouveau #illustration #MotherhoodInArt #NightSky\n",
      "'Old laguna' by Joseph Kleitsch üá≠üá∫üá∫üá∏  #arte #impressionist #landscape #StreetScene #ColorfulArt\n",
      "üé® Joseph Kleitsch, selfportrait   (Hungarian - American, 1885-1931)    #selfie #MenInArt #hats #painter\n",
      "'Under the sun' by Joseph Kleitsch üá≠üá∫üá∫üá∏  #art #impressionism #ColorfulArt #gardens #fineart #WomenInArt\n"
     ]
    }
   ],
   "source": [
    "[print(i) for i in new_GAB.loc[30:35]['Text']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Look Maggie, I'm walkin, Maggie' üòáüòç   https://youtu.be/whS9K5rL_FU  #cuteness #babyfever\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(new_GAB.loc[31]['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Look Maggie, I'm walkin, Maggie' üòá üòç   https://youtu.be/whS9K5rL_FU  #cuteness #babyfever\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sam = str(new_GAB.loc[31]['Text'])[:35] + ' ' + str(new_GAB.loc[31]['Text'])[35:]\n",
    "new_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "\n",
    "class Word_Preprocessing():\n",
    "    def eliminate_url(self,df,target):\n",
    "        print('Start eliminate url: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        text = df_temp[target_column_name]\n",
    "        for i in tqdm(text):\n",
    "            urls = re.findall(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', i)\n",
    "            for i in urls:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_username(self,df,target):\n",
    "        print('Start eliminate username: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'@\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    \n",
    "    def eliminate_hashtag(self, df, target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'#\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_symbol(self,df,target):\n",
    "        print('Start eliminate symbol: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        symbol_list = ['!','@','#','$','%','^','&','*','(',')','-','+','?','>','<','=','/','.',':',';','  ','   ','    ','      ','      ','  ']\n",
    "        for i in tqdm(symbol_list):\n",
    "            df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, ' '))\n",
    "        return df_temp\n",
    "    \n",
    "    def to_Lower(self,df,target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        print('Start eliminate lower: : )')\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', ' ', x).lower())\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_emoji(self,df,target):\n",
    "        print('Start transfer emoji: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        preprocessed_text = []\n",
    "        df_temp[target_column_name] = df_temp.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "        return df_temp\n",
    "    \n",
    "    def process_all(self, df,target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        df_remove_url = self.eliminate_url(df_temp,target_column_name)\n",
    "        df_transfer_emoji = self.eliminate_emoji(df_remove_url, target_column_name)\n",
    "        df_eliminate_hashtag = self.eliminate_hashtag(df_transfer_emoji, target_column_name)\n",
    "        df_remove_username = self.eliminate_username(df_eliminate_hashtag, target_column_name)\n",
    "        df_remove_symbol = self.eliminate_symbol(df_remove_username, target_column_name)\n",
    "        df_to_lower = self.to_Lower(df_remove_symbol, target_column_name)\n",
    "        print(\"finished!!\")\n",
    "        return df_to_lower\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction,XLNetTokenizer\n",
    "import torch\n",
    "tokenizer =  XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "inputs = tokenizer.tokenize(new_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅ',\n",
       " \"'\",\n",
       " 'Look',\n",
       " '‚ñÅMaggie',\n",
       " ',',\n",
       " '‚ñÅI',\n",
       " \"'\",\n",
       " 'm',\n",
       " '‚ñÅwalk',\n",
       " 'in',\n",
       " ',',\n",
       " '‚ñÅMaggie',\n",
       " \"'\",\n",
       " '‚ñÅ',\n",
       " 'üòá',\n",
       " '‚ñÅ',\n",
       " 'üòç',\n",
       " '‚ñÅhttp',\n",
       " 's',\n",
       " '://',\n",
       " 'you',\n",
       " 'tu',\n",
       " '.',\n",
       " 'be',\n",
       " '/',\n",
       " 'w',\n",
       " 'h',\n",
       " 'S',\n",
       " '9',\n",
       " 'K',\n",
       " '5',\n",
       " 'r',\n",
       " 'L',\n",
       " '_',\n",
       " 'FU',\n",
       " '‚ñÅ',\n",
       " '#',\n",
       " 'cut',\n",
       " 'e',\n",
       " 'ness',\n",
       " '‚ñÅ',\n",
       " '#',\n",
       " 'baby',\n",
       " 'f',\n",
       " 'ever']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "temp = np.array(inputs.input_ids)\n",
    "len(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Word_Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/27665 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate url: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27665/27665 [00:42<00:00, 655.66it/s]\n",
      "  0%|                                                                                        | 0/27665 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start transfer emoji: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27665/27665 [00:00<00:00, 40453.69it/s]\n",
      "  1%|‚ñã                                                                           | 239/27665 [00:00<00:12, 2184.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate username: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27665/27665 [00:00<00:00, 38881.70it/s]\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 25/26 [00:00<00:00, 129.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate symbol: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:00<00:00, 119.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate lower: : )\n",
      "finished!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = preprocessor.process_all(new_GAB, 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 16/26 [00:00<00:00, 155.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate symbol: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:00<00:00, 133.44it/s]\n"
     ]
    }
   ],
   "source": [
    "final_data = preprocessor.eliminate_symbol(final_data, 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " through the arches mission san juan capistrano by joseph kleitsch \n",
      " look maggie i m walkin maggie \n",
      " night slid down by emma florence harrison \n",
      " old laguna by joseph kleitsch \n",
      " joseph kleitsch selfportrait hungarian american 1885 1931 \n",
      " under the sun by joseph kleitsch \n"
     ]
    }
   ],
   "source": [
    "[print(i) for i in final_data.loc[30:35]['Text']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = final_data['Text']\n",
    "label = final_data['Hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, x_valid, y_text, y_valid = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_data(text, label,name):\n",
    "    GAB = {'text':text,'hate':label}\n",
    "    GAB_final = pd.DataFrame(GAB)\n",
    "    file_name = name+'.csv'\n",
    "    GAB_final.to_csv(file_name,index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_data(x_train, y_train, 'Data/GABtrain')\n",
    "pack_data(x_test, y_text, 'Data/GABtest')\n",
    "pack_data(x_valid, y_valid, 'Data/GABvalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'Data'\n",
    "destination_folder = 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "# Preliminaries\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c21e9e7340>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in final_data['Text']]\n",
    "pd.Series(seq_len).hist(bins = 50,range=[0,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 75\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('text', text_field),('hate', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='GABtrain.csv', validation='GABvalid.csv',\n",
    "                                           test='GABtest.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "        \n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 10,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_no = 1\n",
    "        for (text, hate), _ in train_loader:\n",
    "            hate = hate.type(torch.LongTensor)           \n",
    "            hate = hate.to(device)\n",
    "            text = text.type(torch.LongTensor)  \n",
    "            text = text.to(device)\n",
    "            output = model(text, hate)\n",
    "            loss, _ = output\n",
    "            print('batch_no [{}/{}]:'.format(batch_no, int(len(x_train)/16)),'training_loss:',loss)\n",
    "            batch_no+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                    # validation loop\n",
    "                    for (text, hate), _ in valid_loader:\n",
    "                        hate = hate.type(torch.LongTensor)           \n",
    "                        hate = hate.to(device)\n",
    "                        text = text.type(torch.LongTensor)  \n",
    "                        text = text.to(device)\n",
    "                        output = model(text, hate)\n",
    "                        loss, _ = output\n",
    "                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1/1383]: training_loss: tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.4900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.3632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.8692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.4379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.3894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.5408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.5330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [89/1383]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.5154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.5510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.4093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.4089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.4144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.3632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.8106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.4097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.4201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.5772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.4882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1383]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [262/1383]: training_loss: tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.8136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.7317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1383]: training_loss: tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [349/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.5412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.6863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.5296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.3749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.3554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.9340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1383]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [436/1383]: training_loss: tensor(0.5735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.5822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.7959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.4163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.8424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.5419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.5341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.5071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [523/1383]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.5506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.3786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.4000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.2649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [610/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.5289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.5206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.5173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.5173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.5266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.5278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/10], Step [692/13840], Train Loss: 0.2999, Valid Loss: 0.2950\n",
      "Model saved to ==> Model/model.pt\n",
      "Model saved to ==> Model/metrics.pt\n",
      "batch_no [693/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [695/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.5189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.8179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.5229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.5399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.5227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.5299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.5549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [782/1383]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.8586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(1.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.7106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.5243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.8473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [869/1383]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.4106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.4230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.3796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.7494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.4266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.4014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [956/1383]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.4163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.4225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.4170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1383]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1383]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1043/1383]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.9185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.5599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.2538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.4952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.5441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.5147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.5299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.4014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1129/1383]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.7445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.5423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.3997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.5599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.4124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.5822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1300/1383]: training_loss: tensor(0.2585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.5235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.5399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.4057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/10], Step [1384/13840], Train Loss: 0.2854, Valid Loss: 0.2959\n",
      "batch_no [1/1383]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2/1383]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.5445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.8620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.4145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.5325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.5131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [90/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.5070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.3625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.2538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.4332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.4141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.7652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [177/1383]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.5316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.5307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.5360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.8194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1383]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [264/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.5226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.3786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.8589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.7623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.5293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1383]: training_loss: tensor(0.5228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [351/1383]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.5423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.5264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.5377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.5194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.5132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.3588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.4924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.4115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.9456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1383]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.7406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.4172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.8937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.4117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.5260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.5121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.5387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [524/1383]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.5307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.5184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.7104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.4261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.4246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.5549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [611/1383]: training_loss: tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.5197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.5333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.5293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.5157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.5436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/10], Step [2076/13840], Train Loss: 0.2977, Valid Loss: 0.2955\n",
      "batch_no [693/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1383]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [697/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.5369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.8732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.2708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.5389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.5121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1383]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.5252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1383]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.5245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.8578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(1.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.3997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.5102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.5227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.5043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [870/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.7674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.4085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [957/1383]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.4093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.5221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.4116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.4083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.5560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.5359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.4116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1383]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1043/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.8436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.5620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.3786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.5207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.5441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.4161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.5632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.4139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.5601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1213/1383]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.5629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.5348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.2586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1299/1383]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.5377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.5329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.5652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.2585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.3650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [2768/13840], Train Loss: 0.2841, Valid Loss: 0.2955\n",
      "batch_no [1/1383]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.8872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.5632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.4922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.5144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.4066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [88/1383]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.4830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.5342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.5071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.7488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.5506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.5083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.8366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [261/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.3484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.8493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.7594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.5077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [348/1383]: training_loss: tensor(0.5098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.5203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.5179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.8668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [435/1383]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.8691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.7378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.5029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.5122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.5310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.5469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.5102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.2630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [608/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.5604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.5280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.5145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/10], Step [3460/13840], Train Loss: 0.2965, Valid Loss: 0.2950\n",
      "Model saved to ==> Model/model.pt\n",
      "Model saved to ==> Model/metrics.pt\n",
      "batch_no [693/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [694/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1383]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.5398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.5188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.3920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.9117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(1.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.5083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.6833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.8478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.4088, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [867/1383]: training_loss: tensor(0.5412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.4218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.4161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.7749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [954/1383]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.5748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.4022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.4123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1383]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1040/1383]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1383]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.9171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.5214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.5283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.5177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.5355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.5450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.3952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1126/1383]: training_loss: tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.7088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.4144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.7105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.3786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.5480, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1212/1383]: training_loss: tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.5717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.5632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.3952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.5506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.3786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.5358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.5127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.8291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.5237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.5256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.5430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.6189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.3416, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1383/1383]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/10], Step [4152/13840], Train Loss: 0.2844, Valid Loss: 0.2970\n",
      "batch_no [1/1383]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.9001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.5188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [86/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.5025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.4989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.7392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.4225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [174/1383]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.4064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.5216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.8044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [261/1383]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.4190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.8185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [348/1383]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.5147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.5300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.3642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.9126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [435/1383]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.4254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.7224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.9122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.5505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.5402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.5248, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [522/1383]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.4988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.5175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.5021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [609/1383]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.4036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.5415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.5475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.5169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/10], Step [4844/13840], Train Loss: 0.2973, Valid Loss: 0.2959\n",
      "batch_no [693/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [695/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.4036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.4117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.8041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.5060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.5252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.4308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.5245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1383]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [782/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.5636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.8887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.4094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(1.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.5097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.8556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.5127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.5292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.5677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.5296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.2557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.7394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [954/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.4047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1040/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.8885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.5647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.5312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.5269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.5372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.5390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.5452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.4218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.4191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.2538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.3796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.5495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.5452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.3805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1210/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.5371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.5432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.5718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.8231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.3642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1296/1383]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.5162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.3942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.4551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.7980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/10], Step [5536/13840], Train Loss: 0.2817, Valid Loss: 0.2976\n",
      "batch_no [1/1383]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.4107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.5614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.5345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [84/1383]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.5132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.5139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [171/1383]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.5579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.7985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1383]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.5081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.3551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.8353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.7567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.4900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [344/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1383]: training_loss: tensor(0.5560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.5219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.5445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.5300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.5394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.5140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.5117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.9026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1383]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.5541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.4357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.5688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.5412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.4083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.3894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.5060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [517/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.5162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.4981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.5020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.3997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.5367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.5391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.3796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.4145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.5423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.5408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.5169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.3702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [690/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/10], Step [6228/13840], Train Loss: 0.2975, Valid Loss: 0.2985\n",
      "batch_no [693/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1383]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.5092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.4326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.4089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.8383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.5186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.3688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.5282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.5271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [776/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.5239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1383]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.5147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.8532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(1.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.7869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.5491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.5629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [949/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.5679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.4230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.4325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1383]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1035/1383]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1383]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1383]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.8824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.5951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.3674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.5011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.5128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.4191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1121/1383]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.5581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.4180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.7064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.7251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1207/1383]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.5595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.5497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.6453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.8048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.3427, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1293/1383]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.5117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.4645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.4540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.2816, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1379/1383]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/10], Step [6920/13840], Train Loss: 0.2761, Valid Loss: 0.3360\n",
      "batch_no [1/1383]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.9467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.5408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.4985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.5330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.7312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [168/1383]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.7625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.4123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.5342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.7933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1383]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.4862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.5266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.7843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [341/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1383]: training_loss: tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.5374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.5020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.5063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.4071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.4927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.4933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.9112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.4141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1383]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.7676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.8771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.7232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.5044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.5132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.5087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.5283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [514/1383]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.5018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.7400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.3704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [601/1383]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.4742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.4116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.4145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.5218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.5138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.4914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.5247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.2100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.4586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.4411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [688/1383]: training_loss: tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [6/10], Step [7612/13840], Train Loss: 0.2964, Valid Loss: 0.3062\n",
      "batch_no [693/1383]: training_loss: tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.5086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.8683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.4879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.4591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.2728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.3501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.7552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.5217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [774/1383]: training_loss: tensor(0.4616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.5415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.8918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.5683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(1.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.4961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.4284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [861/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.4537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.3478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.2052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.4138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.4063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [948/1383]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1383]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.3222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.4575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.4066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.4139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1383]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1034/1383]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1383]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.7994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.7847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.5599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.5170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.5581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.6863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.2659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.3580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.3612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.3627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1205/1383]: training_loss: tensor(0.5369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.4273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.5509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.5163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.5312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.3597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.5422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.4980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.4472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.4285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.4593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.3266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.3378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.2922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.4089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.2816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.4097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1375/1383]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.4942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [6/10], Step [8304/13840], Train Loss: 0.2546, Valid Loss: 0.6093\n",
      "batch_no [1/1383]: training_loss: tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.4924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.9333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.5436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.5136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.5756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.5247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.4022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [164/1383]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.4089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.4085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.5290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.8092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1383]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.4975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.5164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.8077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.5293, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [336/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1383]: training_loss: tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.3551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.3771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.5428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.5138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.5008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.4824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.4063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.4625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.9398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [423/1383]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1383]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.7394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.2722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.4268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.4504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.3632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.5170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.4703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.3308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.5008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.4547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.3533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.4437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.4456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.4286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.3533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.5909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.4587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [596/1383]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.2691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.4479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.4980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.4426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.4748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [7/10], Step [8996/13840], Train Loss: 0.2861, Valid Loss: 0.3454\n",
      "batch_no [693/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1383]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.1864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.2649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.5186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.4474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.3441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.2156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.4803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [768/1383]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1383]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.5080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.8340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.3404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.5484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(0.8241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.4622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.4283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.4561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [855/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.4899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.3254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.2859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1383]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1383]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1027/1383]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1383]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1383]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.4327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.5678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.4585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.5001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.4071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.4906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.3749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.4319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.2877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.2802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.2616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.5209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.5028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.4496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1197/1383]: training_loss: tensor(0.4958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.2098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.4853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.2892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.4339, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1283/1383]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.4775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [7/10], Step [9688/13840], Train Loss: 0.1814, Valid Loss: 1.0880\n",
      "batch_no [1/1383]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.4951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.3894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.4170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.9621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.5038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.4402, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [71/1383]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.4832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.4341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.4102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.5245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.4286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.4304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [158/1383]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.4174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.4563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.5381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.4424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.4927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.4934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.8302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1383]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.4827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.4997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.7966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5372, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [330/1383]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1383]: training_loss: tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.4325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.4604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.4865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.5059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.4097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.4784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.4613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.3264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.2156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [417/1383]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.9978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1383]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.4492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.9217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.4521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.4863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.3262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.4339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [589/1383]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.4516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.3642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [676/1383]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [8/10], Step [10380/13840], Train Loss: 0.2555, Valid Loss: 0.4702\n",
      "batch_no [693/1383]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1383]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.4668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.2671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [763/1383]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.1977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.2728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1383]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.2915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.3581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.4604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [936/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1383]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.2156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1023/1383]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1383]: training_loss: tensor(0.4516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.4845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.4886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.2722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.5132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1109/1383]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.3222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1195/1383]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.4312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1281/1383]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1367/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [8/10], Step [11072/13840], Train Loss: 0.0996, Valid Loss: 0.6970\n",
      "batch_no [1/1383]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.5147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.3308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.8678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.4680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.4247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.4420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.4478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.4899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1383]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.4510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.4827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.4819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.4296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.4706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.4380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1383]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [157/1383]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.3674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.4453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.3642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.4289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.4857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.4102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.4437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.3260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.5208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.4997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.5174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1383]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [244/1383]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.8125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1383]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.4628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.6055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.5425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [331/1383]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1383]: training_loss: tensor(0.4844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.4334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.4839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.4628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.2892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.5975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1383]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.8038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.4267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.4141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.4000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.2877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.4913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1383]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [504/1383]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.4066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.3632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1383]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.2839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1383]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [677/1383]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [9/10], Step [11764/13840], Train Loss: 0.2175, Valid Loss: 0.5350\n",
      "batch_no [693/1383]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1383]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.2785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1383]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [763/1383]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1383]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1383]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [850/1383]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [937/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1023/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1383]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.5759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.4412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.5408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.4826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.5260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1383]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1193/1383]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1363/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [9/10], Step [12456/13840], Train Loss: 0.0557, Valid Loss: 0.6214\n",
      "batch_no [1/1383]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1383]: training_loss: tensor(0.4089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1383]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1383]: training_loss: tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1383]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1383]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1383]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1383]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1383]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1383]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1383]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1383]: training_loss: tensor(0.5296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1383]: training_loss: tensor(0.4276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1383]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1383]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1383]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1383]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1383]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1383]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1383]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1383]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1383]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1383]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1383]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1383]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1383]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1383]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1383]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1383]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1383]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1383]: training_loss: tensor(0.8207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1383]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1383]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1383]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1383]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1383]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1383]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1383]: training_loss: tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1383]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1383]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1383]: training_loss: tensor(0.5038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1383]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1383]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1383]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1383]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1383]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1383]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1383]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1383]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1383]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1383]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1383]: training_loss: tensor(0.4817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1383]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1383]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1383]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1383]: training_loss: tensor(0.5794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1383]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1383]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1383]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1383]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1383]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1383]: training_loss: tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [66/1383]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1383]: training_loss: tensor(0.2899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1383]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1383]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1383]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1383]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1383]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1383]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1383]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1383]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1383]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1383]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1383]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1383]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1383]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1383]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1383]: training_loss: tensor(0.2844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1383]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1383]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1383]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1383]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1383]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1383]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1383]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1383]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1383]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1383]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1383]: training_loss: tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1383]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1383]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1383]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1383]: training_loss: tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1383]: training_loss: tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1383]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1383]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1383]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1383]: training_loss: tensor(0.5219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1383]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1383]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1383]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1383]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1383]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1383]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1383]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1383]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1383]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1383]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1383]: training_loss: tensor(0.3243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1383]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1383]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1383]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1383]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1383]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1383]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1383]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1383]: training_loss: tensor(0.4595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1383]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1383]: training_loss: tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1383]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1383]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1383]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1383]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1383]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1383]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1383]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1383]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1383]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1383]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1383]: training_loss: tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1383]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1383]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1383]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1383]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1383]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1383]: training_loss: tensor(0.4742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1383]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1383]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1383]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1383]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1383]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1383]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1383]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1383]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1383]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1383]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [153/1383]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1383]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1383]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1383]: training_loss: tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1383]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1383]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1383]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1383]: training_loss: tensor(0.4226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1383]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1383]: training_loss: tensor(0.5394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1383]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1383]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1383]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1383]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1383]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1383]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1383]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1383]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1383]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1383]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1383]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1383]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1383]: training_loss: tensor(0.6781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1383]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1383]: training_loss: tensor(0.5086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1383]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1383]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1383]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1383]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1383]: training_loss: tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1383]: training_loss: tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1383]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1383]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1383]: training_loss: tensor(0.3598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1383]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1383]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1383]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1383]: training_loss: tensor(0.4260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1383]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1383]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1383]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1383]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1383]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1383]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1383]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1383]: training_loss: tensor(0.4985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1383]: training_loss: tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1383]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1383]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1383]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1383]: training_loss: tensor(0.2649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1383]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1383]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1383]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1383]: training_loss: tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1383]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1383]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1383]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1383]: training_loss: tensor(0.3206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1383]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1383]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1383]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1383]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1383]: training_loss: tensor(0.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1383]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1383]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1383]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1383]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1383]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1383]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1383]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1383]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1383]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1383]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1383]: training_loss: tensor(0.5510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1383]: training_loss: tensor(0.6073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1383]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1383]: training_loss: tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1383]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1383]: training_loss: tensor(0.4720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1383]: training_loss: tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1383]: training_loss: tensor(0.4139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1383]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1383]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1383]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1383]: training_loss: tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [240/1383]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1383]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1383]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1383]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1383]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1383]: training_loss: tensor(0.4572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1383]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1383]: training_loss: tensor(0.7921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1383]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1383]: training_loss: tensor(0.4462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1383]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1383]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1383]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1383]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1383]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1383]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1383]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1383]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1383]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1383]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1383]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1383]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1383]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1383]: training_loss: tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1383]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1383]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1383]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1383]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1383]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1383]: training_loss: tensor(0.5475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1383]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1383]: training_loss: tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1383]: training_loss: tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1383]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1383]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1383]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1383]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1383]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1383]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1383]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1383]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1383]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1383]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1383]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1383]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1383]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1383]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1383]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1383]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1383]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1383]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1383]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1383]: training_loss: tensor(0.3805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1383]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1383]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1383]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1383]: training_loss: tensor(0.7085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1383]: training_loss: tensor(0.5004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1383]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1383]: training_loss: tensor(0.4555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1383]: training_loss: tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1383]: training_loss: tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1383]: training_loss: tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1383]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1383]: training_loss: tensor(0.2671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1383]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1383]: training_loss: tensor(0.3286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1383]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1383]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1383]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1383]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1383]: training_loss: tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1383]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1383]: training_loss: tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1383]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1383]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1383]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1383]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1383]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1383]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1383]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1383]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1383]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1383]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1383]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1383]: training_loss: tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1383]: training_loss: tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1383]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1383]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1383]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1383]: training_loss: tensor(0.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1383]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1383]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1383]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1383]: training_loss: tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1383]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1383]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1383]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1383]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1383]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1383]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1383]: training_loss: tensor(0.4256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1383]: training_loss: tensor(0.2044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1383]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1383]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1383]: training_loss: tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1383]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1383]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1383]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1383]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1383]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1383]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1383]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1383]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1383]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1383]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1383]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1383]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1383]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1383]: training_loss: tensor(0.3627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1383]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1383]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1383]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1383]: training_loss: tensor(0.4845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1383]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1383]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1383]: training_loss: tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1383]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1383]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1383]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1383]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1383]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1383]: training_loss: tensor(0.4403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1383]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1383]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1383]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1383]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1383]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1383]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1383]: training_loss: tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1383]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1383]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1383]: training_loss: tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1383]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1383]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1383]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1383]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1383]: training_loss: tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1383]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1383]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1383]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1383]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1383]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1383]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1383]: training_loss: tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1383]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1383]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1383]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1383]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1383]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1383]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1383]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1383]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1383]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1383]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1383]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1383]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1383]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1383]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1383]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [412/1383]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1383]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1383]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1383]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1383]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1383]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1383]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1383]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1383]: training_loss: tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1383]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1383]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1383]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1383]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1383]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1383]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1383]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1383]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1383]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1383]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1383]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1383]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1383]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1383]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1383]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1383]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1383]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1383]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1383]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1383]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1383]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1383]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1383]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1383]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1383]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1383]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1383]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1383]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1383]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1383]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1383]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1383]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1383]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1383]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1383]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1383]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1383]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1383]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1383]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1383]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1383]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1383]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1383]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1383]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1383]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1383]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1383]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1383]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1383]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1383]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1383]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1383]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1383]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1383]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1383]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1383]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1383]: training_loss: tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1383]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1383]: training_loss: tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1383]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1383]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1383]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1383]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1383]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1383]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1383]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1383]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1383]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1383]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1383]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [500/1383]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1383]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1383]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1383]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1383]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1383]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1383]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1383]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1383]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1383]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1383]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1383]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1383]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1383]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1383]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1383]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1383]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1383]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1383]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1383]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1383]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1383]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1383]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1383]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1383]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1383]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1383]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1383]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1383]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1383]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1383]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1383]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1383]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1383]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1383]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1383]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1383]: training_loss: tensor(0.3092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1383]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1383]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1383]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1383]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1383]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1383]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1383]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1383]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1383]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1383]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1383]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1383]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1383]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1383]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1383]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1383]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1383]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1383]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1383]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1383]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1383]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1383]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1383]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1383]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1383]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1383]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1383]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1383]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1383]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1383]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1383]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1383]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1383]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1383]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1383]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1383]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1383]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1383]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1383]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1383]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1383]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1383]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1383]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1383]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [587/1383]: training_loss: tensor(0.3470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1383]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1383]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1383]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1383]: training_loss: tensor(0.5348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1383]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1383]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1383]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1383]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1383]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1383]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1383]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1383]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1383]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1383]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1383]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1383]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1383]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1383]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1383]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1383]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1383]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1383]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1383]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1383]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1383]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1383]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1383]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1383]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1383]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1383]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1383]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1383]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1383]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1383]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1383]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1383]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1383]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1383]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1383]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1383]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1383]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1383]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1383]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1383]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1383]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1383]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1383]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1383]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1383]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1383]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1383]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1383]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1383]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1383]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1383]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1383]: training_loss: tensor(0.2044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1383]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1383]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1383]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1383]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1383]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1383]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1383]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1383]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1383]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1383]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1383]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1383]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1383]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1383]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1383]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1383]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1383]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1383]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [674/1383]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1383]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1383]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1383]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1383]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1383]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1383]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1383]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1383]: training_loss: tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1383]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1383]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1383]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [10/10], Step [13148/13840], Train Loss: 0.1838, Valid Loss: 0.5962\n",
      "batch_no [693/1383]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1383]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1383]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1383]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1383]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1383]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1383]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1383]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1383]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1383]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1383]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1383]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1383]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1383]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1383]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1383]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1383]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1383]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1383]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1383]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1383]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1383]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1383]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1383]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1383]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1383]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1383]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1383]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1383]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1383]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1383]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1383]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1383]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1383]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1383]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1383]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1383]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1383]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1383]: training_loss: tensor(0.4308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1383]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1383]: training_loss: tensor(0.4266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1383]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1383]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1383]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1383]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1383]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1383]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1383]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1383]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1383]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1383]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1383]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1383]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [760/1383]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1383]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1383]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1383]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1383]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1383]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1383]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1383]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1383]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1383]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1383]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1383]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1383]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1383]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1383]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1383]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1383]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1383]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1383]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1383]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1383]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1383]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1383]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1383]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1383]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1383]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1383]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1383]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1383]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1383]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1383]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1383]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1383]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1383]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1383]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1383]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1383]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1383]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1383]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1383]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1383]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1383]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1383]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1383]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1383]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1383]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1383]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1383]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1383]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1383]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1383]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1383]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1383]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1383]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1383]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1383]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1383]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1383]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1383]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1383]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1383]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1383]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1383]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1383]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1383]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1383]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1383]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1383]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1383]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1383]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1383]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [847/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1383]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1383]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1383]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1383]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1383]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1383]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1383]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1383]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1383]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1383]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1383]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1383]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1383]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1383]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1383]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1383]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1383]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1383]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1383]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1383]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1383]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1383]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1383]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1383]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1383]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1383]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1383]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1383]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1383]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1383]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1383]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1383]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1383]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1383]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1383]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1383]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1383]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1383]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1383]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1383]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1383]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1383]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1383]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1383]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1383]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1383]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1383]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1383]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1383]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1383]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1383]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1383]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1383]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1383]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1383]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1383]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1383]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1383]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1383]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1383]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1383]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1383]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1383]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1383]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1383]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1383]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1383]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1383]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1383]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1383]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1383]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1383]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1383]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1383]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1383]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1383]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1383]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1383]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1383]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1383]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1383]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1383]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1383]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1383]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1383]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1383]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/1383]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1383]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1383]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1383]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1383]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1383]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1383]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1383]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1019/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1383]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1383]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1383]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1383]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1383]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1383]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1383]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1383]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1383]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1383]: training_loss: tensor(0.5030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1383]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1383]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1383]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1383]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1383]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1383]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1383]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1383]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1383]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1383]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1383]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1383]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1383]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1383]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1383]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1383]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1383]: training_loss: tensor(0.4450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1383]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1383]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1383]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1383]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1383]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1383]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1383]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1383]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1383]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1383]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1383]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1383]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1383]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1383]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1383]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1383]: training_loss: tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1383]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1383]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1383]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1383]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1383]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1383]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1383]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1383]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1383]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1383]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1383]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1383]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1383]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1383]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1383]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1383]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1383]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1383]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1383]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1383]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1383]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1383]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1383]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1383]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1383]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1383]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1383]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1383]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1383]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1383]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1383]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1383]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1383]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1383]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1383]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1383]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1383]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1383]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1383]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1383]: training_loss: tensor(0.3297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1383]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1383]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1383]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1383]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1383]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1383]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1383]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1383]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1383]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1383]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1383]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1383]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1383]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1383]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1383]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1383]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1383]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1383]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1383]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1383]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1383]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1383]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1383]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1383]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1383]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1383]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1383]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1383]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1383]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1383]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1383]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1383]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1383]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1383]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1383]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1383]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1383]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1383]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1189/1383]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1383]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1383]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1383]: training_loss: tensor(0.2728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1383]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1383]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1383]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1383]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1383]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1383]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1383]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1383]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1383]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1383]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1383]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1383]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1383]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1383]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1383]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1383]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1383]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1383]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1383]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1383]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1383]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1383]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1383]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1383]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1383]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1383]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1383]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1383]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1383]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1383]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1383]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1383]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1383]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1383]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1383]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1383]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1383]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1383]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1383]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1383]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1383]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1383]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1383]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1383]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1383]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1383]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1383]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1275/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1383]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1383]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1383]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1383]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1383]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1383]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1383]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1383]: training_loss: tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1383]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1383]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1383]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1383]: training_loss: tensor(0.1717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1383]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1383]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1383]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1383]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1383]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1383]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1383]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1383]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1383]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1383]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1383]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1383]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1383]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1383]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1383]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1383]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1383]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1383]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1383]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1383]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1383]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1383]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1383]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1383]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1383]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1383]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1383]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1383]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1383]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1383]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1383]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1383]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1383]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1383]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1383]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1383]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1383]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1383]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1383]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1383]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1383]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1361/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1383]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1383]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1383]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1383]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1383]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1383]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1383]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1383]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1383]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1383]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1383]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1383]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [10/10], Step [13840/13840], Train Loss: 0.0390, Valid Loss: 0.6771\n",
      "Model saved to ==> Model/metrics.pt\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "model = BERT().to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']    \n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/metrics.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5dn48e+dhYTsCzsBkrAHZTMi7mxVRIG6Vamta2vdXmutorZubd+21u1nrVttX7WLigsqoiiKqLihLCLIakCWCQFCgEnIvjy/P54ZSMIkmUnmTCbh/lzXXDM5c8557rDMPc8uxhiUUkopr4j2DkAppVR40cSglFKqAU0MSimlGtDEoJRSqgFNDEoppRqIau8AAtWtWzeTmZnZ3mEopVSHsmLFir3GmO7+nNvhEkNmZibLly9v7zCUUqpDEZFt/p6rTUlKKaUa0MSglFKqAU0MSimlGuhwfQxKKRWo6upqXC4XFRUV7R2K42JjY8nIyCA6OrrV99DEoJTq9FwuF4mJiWRmZiIi7R2OY4wxFBUV4XK5yMrKavV9tClJKdXpVVRUkJ6e3qmTAoCIkJ6e3uaakSYGpdRRobMnBa9g/J6aGJRS/ln9CpTta+8oVAhoYlBKtcztgtd+Bl//p70j6ZCKiooYPXo0o0ePplevXvTt2/fQz1VVVc1eu3z5cm688cYQRWpp57NSqmUHdtjnfVvaN44OKj09nVWrVgFw7733kpCQwC233HLo/ZqaGqKifH8c5+bmkpubG5I4vbTGoJRqWXG+fd73ffvG0Ylcfvnl3HzzzUycOJHbbruNr776ipNOOokxY8Zw0kknsXHjRgA++ugjzjnnHMAmlSuvvJIJEyaQnZ3No48+6khsWmNQSrXM7akx7O/4ieF389eybmdxUO+Z0yeJe6aPCPi6TZs2sWjRIiIjIykuLmbJkiVERUWxaNEifvOb3zB37twjrtmwYQMffvghJSUlDB06lGuvvbZNcxZ80cSglGqZ21NjcLugpgqiurRvPJ3EhRdeSGRkJABut5vLLruM7777DhGhurra5zVnn302MTExxMTE0KNHD3bv3k1GRkZQ49LEoJRqmbcpydTBge3QbVD7xtMGrflm75T4+PhDr++66y4mTpzI66+/ztatW5kwYYLPa2JiYg69joyMpKamJuhxaR+DUqpl7h0Qm2Jfd4LmpHDkdrvp27cvAM8991y7xqKJQSnVMnc+ZJ5iX2sHtCNmz57NHXfcwcknn0xtbW27xiLGmHYNIFC5ublGN+pRKoSqyuBPvWHSnfDJw3Dc5TD1z+0dVUDWr1/P8OHD2zuMkPH1+4rICmOMX+NetcaglGqet38huT+kZulchqOAJgalVPPcLvuc3BfSsrQp6SigiUEp1TxvjSGpL6Rmwv6tUFfXnhEph2liUEo1z+0CBJL6QFo21FZCSUF7R6UcpIlBKdU8twsSekBUjG1KAh2y2sk5lhhE5BkR2SMi3zbxvojIoyKSJyKrRWSsU7EopdqgON82I4HtfAbtgO7knKwxPAdMbeb9s4DBnsfVwJMOxqKUai13vu14BkjuBxFR2gEdoAkTJrBw4cIGxx555BGuu+66Js/3DsufNm0aBw4cOOKce++9lwcffDD4weJgYjDGLAGa29VjJvBvYy0FUkSkt1PxKKVawRjblJTcz/4cGWVfa1NSQGbNmsWcOXMaHJszZw6zZs1q8doFCxaQkpLiVGg+tWcfQ19gR72fXZ5jRxCRq0VkuYgsLywsDElwSimg4gBUlx5uSgLbAa01hoBccMEFvPXWW1RWVgKwdetWdu7cyQsvvEBubi4jRozgnnvu8XltZmYme/fuBeCPf/wjQ4cOZcqUKYeW5XZCey6i52tjUp/TsI0xTwNPg5357GRQSql6vKuqJtdPDFngWm5rEx1xH+V3bodda4J7z17Hwln3Nfl2eno648aN491332XmzJnMmTOHiy66iDvuuIO0tDRqa2uZPHkyq1evZuTIkT7vsWLFCubMmcPXX39NTU0NY8eO5bjjjgvu7+HRnjUGF9Cv3s8ZwM52ikUp5cuhyW31/qumZkGlG8r3t09MHVT95iRvM9LLL7/M2LFjGTNmDGvXrmXdunVNXv/JJ59w7rnnEhcXR1JSEjNmzHAs1vasMbwJ3CAic4ATALcxRgdHKxVOij2JIalRjQFsc1JcWuhjaqtmvtk76Yc//CE333wzK1eupLy8nNTUVB588EGWLVtGamoql19+ORUVFc3eQ0JUQ3NyuOqLwBfAUBFxichVInKNiFzjOWUBsAXIA/4B+O6eV0q1H3e+HYWU0OPwsbRs+6wd0AFJSEhgwoQJXHnllcyaNYvi4mLi4+NJTk5m9+7dvPPOO81ef9ppp/H6669TXl5OSUkJ8+fPdyxWx2oMxphmu9uNXdb1eqfKV0oFgdtlZzxHRB4+lpppn7UDOmCzZs3ivPPOY86cOQwbNowxY8YwYsQIsrOzOfnkk5u9duzYsVx00UWMHj2aAQMGcOqppzoWpy67rZRq2rPTbCfzlY2+zT40DLInwrkdY/qRLruty24rpYLF7Wo4IskrNUubkjoxTQxKKd/q6qB4Z8OOZy9dfrtT08SglPKtdA/UVUNyxpHvpWXBwV1QVRr6uFqpozWbt1Ywfk9NDEop3w5NbvORGLyL6e3fGrJw2iI2NpaioqJOnxyMMRQVFREbG9um+7TnPAalVDjzNYfBq/5chp4jQhdTK2VkZOByuTgaltSJjY0lI8NHMg+AJgallG+HZj03V2PoGP0M0dHRZGVltXcYHYY2JSmlfHPnQ3QcdE098r24NIhN0Q7oTkoTg1LKt2KXbUZqahmGtCzdsKeT0sSglPLN7fLdjOSlcxk6LU0MSinf6u/c5ktaFhzYAbXVoYtJhYQmBqXUkWqq4OBuSGqmxpCWDaYW3DuaPkd1SJoYlFJHKtkJmOZrDKn1hqyqTkUTg1LqSM1NbvM6NJdBO6A7G00MSqkjFXsSQ3NNSQm9ICq2w8x+Vv7TxKCUOpK336C5pqSICLs3gzYldTqaGJRSR3Ln24ltXeKbPy8tW4esdkKaGJRSRyrOb74ZySvVs/x2J1+c7mijiUEpdaSW5jB4pWVBTTmU7HI+JhUymhiUUkdy72h+RJJXB1tMT/lHE4NSqqGqUqg44Hu57cbSdC5DZ6SJQSnVkD9zGLxS+oNEao2hk9HEoJRq6NBQVT8SQ2S0PU8nuXUqmhiUUg0dmtzmR1MSeJbf1hpDZ6KJQSnVkDsfEEjq49/5uvx2p6OJQSnVkNsFCT1tM5E/0rKhfL99qE5BE4NSqqHiFjboaUxHJnU6mhiUUg35O7nNS+cydDqOJgYRmSoiG0UkT0Ru9/F+sojMF5FvRGStiFzhZDxKqRYYY5uS/FkOwys10z5rjaHTcCwxiEgk8DhwFpADzBKRnEanXQ+sM8aMAiYAD4lIF6diUkq1oHy/XeIikKakmATbJ6E1hk7DyRrDOCDPGLPFGFMFzAFmNjrHAIkiIkACsA+ocTAmpVRz3C77HEhTEngW09sa9HBU+3AyMfQF6m8G6/Icq+8xYDiwE1gD/NIYU+dgTEqp5vizQY8vaVk6ya0TcTIxiI9jjdfmPRNYBfQBRgOPiUjSETcSuVpElovI8sLCwuBHqpSyDtUYAkwMqVl2n+jq8uDHpELOycTgAvrV+zkDWzOo7wrgNWPlAd8DwxrfyBjztDEm1xiT2717d8cCVuqo53ZBRDTEB/j/zDtkdf+24MekQs7JxLAMGCwiWZ4O5YuBNxudsx2YDCAiPYGhgNZHlWovxfl2xnNEgB8Nadn2WTugO4Uop25sjKkRkRuAhUAk8IwxZq2IXON5/yngD8BzIrIG2/R0mzFmr1MxKaVa4A5wcpuXdy6D9jN0Co4lBgBjzAJgQaNjT9V7vRM4w8kYlFIBcOfDgBMDvy4uDWKSdC5DJ6Ezn5VSVl2t7UD2d1XV+kTsRDdtSuoUNDEopayDu6GuJvA5DF5p2Vpj6CQ0MSilrEM7t/Vr/rympGXBge1Qq3NUOzpNDEopq9gzh6E1TUlgO6Drqg/fR3VYmhiUUlZrl8Pw0uW3Ow1NDEopy50PXRIgNqV11+tchk5DE4NSyip22WYk8bWajR8S+0BkjNYYOgFNDEopy+1qfTMS2NnSqQN0klsnoIlBKWW581vf8eyVmgX7twYlHNV+NDEopaCmEkr3tH6oqldalm1KMo0XUlYdiSYGpRQUexY+bktTEtgO6OpSKNXl8TsyTQxKqcNDVYPRlATaAd3BaWJQSh3euS0YTUmgHdAdnCYGpVS9GkOftt0npT8gwZ/LUFMFL1wMmxcH977KJ00MSimbGLqmQZe4tt0nKsbWOoLdlPTdQtj0Diy4VddiCgFNDEop25TUmg16fEnLDH6NYdULENkFivLgmxeDe291BE0MSik7hyFYiSE1K7h9DAf3wKaFcMI10Pc4+Og+qK4I3v07iqLNUFsdkqI0MSilbFNSW0ckeaVlQVkRVBQH535rXgFTC6Mvgcl326U7VjwbnHt3FK4V8I9J8N6dISlOE4NSR7vKEqh0t30Og5d3yGowmpOMga+ftzWFHsMgewJknQZLHoTKg22/f0ew9TP490yITYbx14akSE0MSh3t2rpBT2PeVVaD0QG9azXsWQujf3z42OR7oGwvLH2y7fcPd3mL4L/nQ1JvuPJdu31qCGhiUOpoF6zJbV5pQawxeDudjzn/8LGMXBh6Nnz+KJTta3sZ4Wr9W/DiLOg2CC5f0PahxAHQxKDU0a64jRv0NBaTCHHd2t4BXVMFq1+GYWdD19SG70260zaBffbXtpURrla/DC9fCr1HwWXzIaF7SIvXxKDU0c6dDxIBib2Dd0/vYnpt8d1CKN9nO50b65kDI38EX/4dSna1rZxws/xZeO1qGHAS/PT1I5NiCGhiUOpoV5wPCb0gMjp490zLbvvy26tesHFlT/T9/oTb7R7TSx5oWznh5IvH4a2bYPAP4JJXbO2rHWhiUOpo594RvGYkr9Qs23dRU9m6671zF0ZdBJFRvs9Jy4axl8KK5zr+on3GwMf3w8LfQM5MuOh5iO7abuFoYlDqaBfMyW1eaVmAgf3bWne9d+7CqB83f95pt0JElJ301lEZA+/fDR/+EUbNgvOfgagu7RqSJgaljmbG2KakYI1I8mrLXAbv3IU+Y+3cheYk9YFxV8Pql2DP+sDLam91dfD2r+0Iq9yrYOYTTdeQQkgTg1JHs7IiqKlwqMZA65p4fM1daM4pv4IuCbD4fwMvy1+1NfDl07bZqrggePecdx0s/z846UY4+yG7b3YYaP/UpJRqP945DMFODPHd7Yd1a2oMvuYuNCcuDU76H/joT5C/ws6SDqa6WvsBvvqlw8d6j4IhU2HImdB7TOAf6DVVMPcqWP8mTPytbRITCW7cbeBoehKRqSKyUUTyROT2Js6ZICKrRGStiHzsZDxKdWg7VwV/yWnvBj3BbkoSad1iet65C0On2Q98f514HcSlwwe/D6y8ltTVwbwbbFKYdCdct9TOvI7qakdD/WMSPDQU5l0P6960cytaUl0Oc35sk8IZf4TTZ4dVUgAHawwiEgk8DvwAcAHLRORNY8y6euekAE8AU40x20Wkh1PxKNWhFW6Ep0+Hs+6HE34RvPs6VWMAu/z2ng2BXdPc3IXmxCTCqb+2o3q2fAzZpwd2vS91dTD/RvjmBZhwh/1WD9BjOJx6s511nbcINr0L6+bD1/+FiGjIPOVwbcLbpOZVWWJnM2/9FM55BHKvaHucDvCrxiAi8SIS4Xk9RERmiEhLg57HAXnGmC3GmCpgDjCz0Tk/Bl4zxmwHMMbsCSx8pY4S371nn9e+Edz7ul222SauW3DvC7bGcGCbbYrx16oXIKEnDJwUeHm5V9maz+I/2A7stqirg7d/BV//xyaE02878py4NDvJ7oJnYPZmuPxtGH+NrYW9exs8OhoeGwfv3WUXwivdC//+IWz7HM57OmyTAvjflLQEiBWRvsAHwBXAcy1c0xfYUe9nl+dYfUOAVBH5SERWiMilvm4kIleLyHIRWV5YWOhnyEp1InmL7PP2L6Bkd/Du6x2R5ESnZ1o21FZB8U7/zj9YaBPgyGbmLjQnOtZ+gLuWwcZ3Ar/eyxh451bb0XzKr2wfQEtNPZGemsIZ/ws3LIP/WQlT77OL3y19Ep6bBg8Oth3rP/qXTShhzN9/DWKMKQPOA/5mjDkXyGnpGh/HGqfxKOA44GzgTOAuERlyxEXGPG2MyTXG5HbvHto1Q5Rqd1Wl9lvmwEmAgY1vB+/ebpczzUgQ+GJ6a16Buhr/RyP5MvrHkDbQ1hrq6gK/3hh49w5Y9k/boT35nta1/6cPtEtkXzoPZm+BH/0bjv85/OQ1GD498PuFmN+JQUROBC4BvP8qW0rpLqD+Or4ZQOOvDi7gXWNMqTFmL7ZmMsrPmJQ6Omz91H7zPvEG+6G37s3g3dvtwBwGL+9cBn87oFe94Jm7MLz1ZUZGw8TfwJ518O3cwK41xm6E8+WTMP46+MEfgtMpHJtkZzNPux+yTm37/ULA38RwE3AH8LoxZq2IZAMftnDNMmCwiGSJSBfgYqDxv+h5wKkiEiUiccAJQAecpaKUg/IW2VEwA0623za3fhKc5abraqGkwLkaQ3KG7Yz1Zy5DwWrYvaZttQWvEedBz2PtTGJ/t8I0BhbdA188ZifMnfmnsBspFEp+JQZjzMfGmBnGmL94OqH3GmNubOGaGuAGYCH2w/5lT1K5RkSu8ZyzHngXWA18BfzTGPNtG34fpTqfvA/sN83oWMiZYZtbNr3b9vuW7LLLTgR7nSSviEhIHeBfU1KgcxeaLTcCJt9ly/36Py2fb4xtevrsr7YD+6z7j+qkAP6PSnpBRJJEJB5YB2wUkVtbus4Ys8AYM8QYM9AY80fPsaeMMU/VO+cBY0yOMeYYY8wjrf1FlOqU9m2BfZth0BT7c5+xkJQB6+e3/d6HNuhxqMYAnrkMLSSGmipY04q5C80ZfAb0O8EuTFdd3vy5H90HnzwEYy+DaQ8e9UkB/G9KyjHGFAM/BBYA/YGfOhaVUsrK+8A+D5xsn0Vsc1LeB23f87jYwTkMXt59GZobPvrde3ZpjkDnLjRHxHYclxTAV/9o+ryP74eP74PRP7HzCsJkSYr25u+fQrRn3sIPgXnGmGqOHGGklAq2vA8gZYAd5eI1fDrUVh6e29Bah/Z6dqgpCWyNoarEfvA3pS1zF5qTebJNqJ8+DBXFR77/yUOHVzSd8agmhXr8/ZP4O7AViAeWiMgAwMeftFIqaGqq4PslthmpfvNG//F2LaK2Nie5XdAlEWKT23af5rS0mN7BQjvbubVzF1oy+S4o3283wKnvs0ft8hnHXggzH7f9IeoQfzufHzXG9DXGTDPWNqCJbZWUUkGxYylUlx7uX/CKiLT7IH/3HlRXtP7+xfnO1hbATnKDpjuggzF3oTl9xtihol88ZmceA3zxBLx/lx299MOnNCn44G/nc7KIPOydfSwiD2FrD0opp+QtssM9fY19Hz4dqg7ClpZGjTfDycltXikDAGm6xhCMuQstmfhbqC6DT/+fXTp74R02WZz3j7DY+yAc+duU9AxQAvzI8ygGnnUqKKUUtn+h/3jf+/5mnmabgNoy2c3tcm5ym1d0rN1Mx9ckt2DOXWhO96G2H+HLp+xSF8POgfP/T5NCM/xNDAONMfd4FsTbYoz5HZDtZGBKHdWKC2D3tzBosu/3o7rAkLNg4wL/J3HVV10BZXudrzGA7YD21ZQUzLkLLZlwuy1ryFlwwbN2hrRqkr+JoVxETvH+ICInAy0MDlZKtdpmzzDVxv0L9eXMgIoDdiZ0oLz7MIQiMaT5mMvgxNyF5qT0h1+thYtfaPf9lDsCf+tS1wD/FhHv8IX9wGXOhKSUIu8DO4Sz5zFNnzNwEkTH29FJgQ71PDS5zeGmJLCJoXSPnXcRk2CPOTF3oSWhSECdhL+jkr4xxowCRgIjjTFjgCAPOlZKAXYNo82Ljxym2lh0Vxj8A1j/VmB7HkBoawypPlZZdWruggqKgGZ0GGOKPTOgAW52IB6lVP5K20Tkz4fm8On22/iOrwIrwzu5LalP4PEFqvFcBqfnLqg2a8tUP11QRCkn5C0CxL/EMPgM26ka6GS3YpfdtS26a6tCDEjjGoPTcxdUm7UlMeiSGEo5YfMH0Pc4/9rEY5NsAlk/P7DtLN0u5ye3eXVNga5ph2sMoZi7oNqk2cQgIiUiUuzjUQKEoA6q1FGmbB/kr2h+NFJjw6eDezsUrPL/Gne+s6uqNpbmGbIaqrkLqk2aTQzGmERjTJKPR6IxRhsHlQq2LR+CqQssMQydBhIZ2GS34vzQdDx7pWbZSW6hnLugWk2XE1QqnOR9ALEp0Hes/9fEpdmN6Ne/6V9zUoUbKotD15QEtsbgdsHql2DoWTp0NMxpYlAqXBhjE8PAiYEv7JYzA4ryoHBDy+ceGpEUysSQbWtC5ftCO3dBtYomBqXCxe61cHBXYM1IXsPOAcS/0UmH5jD0C7yc1vKOTIrvcXjTIRW2NDEoFS7yFtnn1nxwJvayW1n608/g3mGfQ9mU5N1oaJTOXegINDEoFS7yFtklMJJ6t+764dPtiB9fK5nW584HiYCEXq0rpzUSesCl8+D020NXpmo1TQxKhYPKEti+tG1LRAw/xz6vf6v584rzIbFP6L+5Z084vFaSCmuaGJQKB99/AnXVretf8ErNhN6j7Oik5oRycpvqkDQxKBUONn9gV0rtP75t9xk+HVzLoHhn0+eEYoMe1aFpYlCqvRkD370PWadBVEzb7jV8pn1uqjmprs4mDa0xqGZoYlCqve3bAge2Nb1bWyC6D4FuQ5tuTirbC7WVoR2qqjocTQxKtTfvMNVgJAawk922fQale498L5Qb9KgOSxODUu0tb5GdGZwWpG3Uh0+3s4w3LjjyvUOT2zQxqKZpYlCqPVVXwNZP2zYaqbFeIyFlgO/Jbu52mPWsOhxNDEq1p+1fQHVZcBODiK01bPnILphXn3sHRMVCXHrwylOdjqOJQUSmishGEckTkSanPIrI8SJSKyIXOBmPUmEnb5FdhjrzlODeN2emnRex6b2Gx4vz7Xaeze0lrY56jiUGEYkEHgfOAnKAWSKS08R5fwEWOhWLUmEr7wPofyJ0iQ/uffvm2iUv1s9reNwd4n0YVIfkZI1hHJBnjNlijKkC5gAzfZz3P8BcYI+DsSgVftz5ULg+uM1IXhERdomM7xZBVVm9Ml2h3blNdUhOJoa+wI56P7s8xw4Rkb7AucBTzd1IRK4WkeUisrywsDDogSrVLjZ/YJ+dSAwAw2dATfnh4bC1NXZZbx2RpFrgZGLw1YjZeHupR4DbjDG1zd3IGPO0MSbXGJPbvXv3oAWoVLvKW2QXs+sx3Jn7DzgZuqYd3qOhpMAOY9WmJNUCJ5dXdAH1x8RlAI0XcMkF5ojtCOsGTBORGmPMGw7GpVT7q62BzR9BznTnOoIjo2DYNDtstaay3uQ2TQyqeU7WGJYBg0UkS0S6ABcDDQZWG2OyjDGZxphM4FXgOk0K6qiQvxwq3c41I3kNn2H3d/5+iU5uU35zrMZgjKkRkRuwo40igWeMMWtF5BrP+832KyjVqeV9YDfLyZ7gbDnZE6BLIqybB+mD7DFdDkO1wNGdOowxC4AFjY75TAjGmMudjEWpsJK3CDKOh66pzpYTFQNDzrTLY+TMhJhkiE1ytkzV4enMZ6VCrXQv7Pza+WYkr5wZUFZk+xq0GUn5QRODUqG2+UPAwMAgrabakkFT7DIYZXu1GUn5RRODUqGWt8gOI+0zOjTldYk/XDvRoarKD5oYlAqlujrYvBgGToKIyNCVO3yGfdamJOUHTQxKhdLuNVC6J3T9C15Dp9rO7szTQluu6pAcHZWkVKdQXQ6rX4K1b0DvUXDshdBzROsmpnmXpxg4KbgxtiQ2GX62KLRlqg5LE4NSTSnZBcv+CcufsaN6UjPtRLHPHoHuw+HY8+GYCyAty/975n0AvY6FxJ6Oha1UW2liUKqxgm/giyfg27lQVwNDp8GJ19m1h8qKYO3r9r3F/2sfGcfbWsSIcyGhR9P3rSiGHV/CSf8Tut9FqVbQxKAUQF0tbHrXJoRtn0J0POReCSf8AtIHHj4vvhuM+7l9HNhuE8SaV+Gd2fDu7Xam8TEX2CWvY5MblvH9EptoQt2/oFSAxJjGC56Gt9zcXLN8+fL2DkN1FpUHYdXzsPRJ2P+93Qv5hF/AmJ9C1xT/77NnvU0Qa16BA9sg0jPj+NgLYfAZEB0L82+y58zeAlFdnPudlPJBRFYYY3L9OVdrDOrodGA7fPl3WPkfu5hdxjiYcg8Mm25XJQ1Uj+Ew+S6YdCfkr7AJ4tvXYP2bEJNk92DevBiyT9ekoMLeUZMYqmvrAIiO1BG6R7UdX8EXjx/eoyBnJpx4PWT49UWqZSL2Xhm5cMYfYesSWDPXJojKYlt7UCrMHTWJYcm6fH4z51Oy07qS3a0rg7rFkd0tluz0rvRJjiGSOtvObOrA1Hpem3qv6+wDPMMUpeGzRDQ6BpU1hoLiSnYeqCDfXcG+0mp6JsXSLy2OfmlxdE/ognjP98XXcEiJqFdWhKdsaXQs4tAxAxSV1bB9XwV7SirpGR9Jv5RI0mMEqau26/TXVkNtlefhfe3reLXnvpEQEWW3j5RIO1Hr0LFIW77nWFmNoaCkhl0lVewtraFHQhQDUrrQMyGSyLoa2+ZeW22f66rtPgV1nrLqauu99v5c4/n78Pyd1NX/+6r/bI/X1dVQVllNWUUVUrqX7mXfUReTTMRJN8DxP4eUfkf+GQdLZJQdljpwEgWn/i8F675g4LCJJLd8pVLt6qhJDMPdn/Bl9LVQgn1873yZMUCm59FeBLsDUrd2Kj8OGOh5tFpEFEREH048hxKR51kiICKCWiKoqhMqaqC8BspqoLzaUIN9r9J04dG6y5lbcRp913RjfKmbEwdGcUJWGukJMcH5hQFjDFv2lrLs+318tXUfX32/D9f+cgASFy3hipMzufKULFLitElJhascry0AABrGSURBVKejp/O5aLNt4/V8m62shd0l1RSUVFHgriK/uIqd7ir2ltVgEGqJIDoqml4pXemTmkDf1ATSE2MoOljF7uJydrsr2OUuZ19pJWAQDBEYEmMi6ZkUQ89EzyOpCz2TYuiRGEPXKKG4ooZd7goKiivY5bnHLncF+8uqDoUaFSH0SIyhV1IsvZJj6Z1sn9PiotlfVklhcQWFxWUUllSyt6ScopJKyiqriRBbo4nEkBYfRff4LnSPjyY9IZpu8dEkxUbhrhIKy+rYXVpHwcFaCg7WsbOklnITRbWJpJooIqNj6J6SSM/URHqlJdM3PYm+6cn0TElgl7uMrXtL2OF5uPYdpOBAKZhaIqkjilrSukbRPzWG/ikxZKTEkJHchb7JXUiPj2JXSQ1b9lWypaiC74oq2FRYyd7yOmpMJLVE0i05noG9UhjcK5UhfdIY1ieVAenxREQcrj0ZY9ixr5x1BW7WFZSwbmcx6wuKyT9Qfuic9Pgu5PRJIqd3EsN7J5HTJ4n+aXGsLyhm6ZZ9fLGliOVb91FWZXeVHdozkfHZaYzPTueE7HTS4v3/0K6tM6wvKOar7/exbKt97D1YdSiOcVlpHJ+ZRnb3eF5atoN3vt1FfJdILjspk5+dmh1QWa3lLqtmyXeFnD60O0mx0Y6Xp8JPIJ3PR09i8JO7vJq8PSVs3HWQTbtLPI+D7D1YeeicpNgosrrFk9ktnsz0+EOvs9LjSY5r3X+6kopqNheWsnnPQfIKD5K35yCb9xxk274yaut8/x31Sools1scWd0SyKr33C8tjpgo/9fhqamtI/9AOVuLyti6t5StRaVs3VvKtqIytu8ro8ZH+TFREWR1iye7ezzZ3RIavA7kz8AYw56SStYV2A/3DQUlrC8oZsve0kO/d1yXSIb2SiQrPZ4d+8vYUFBCSWUNABECWd3iyemTzPDeieT0tsmge2KMp5muadW1dazJd/PF5iKWbili+db9lFfbRDGsVyLjs9MZn53GCVnppNb78K6ormW1y80yT21gxbb9HPTEk5HalXFZaYzLTOP4rDSyu8UfEceGXcU8tjiPt9cU0DU6kp+OH8DPTs2me2Lwai3e32/JpkLmrnSxaN0eqmrrOGVQN5674niitK/tqKOJwQH7SqvYeaCcPildSY2LbvFDJ1gqa2rZVlRG3p6D7DxQTu/krp5EFEdcF+dbAmtq69h5oILvi0rZ5bblZ3ePp09y1wbf4oOtorqW73YfZH1B8aGksbWolH6pcYdqAMN7JzG0ZyJduwRnMbqqmjrW5B9g6ZZ9LN1SxLKt+6iotrWwYb0SGd0vhS2FpaxyHaCqxh4f0jPhUI1gXFYavZO7+l1e3p4SHlucx5vf7KRLVASXnDCAX5yWTY+k2Db9Hut2FjN3pYt5q/LZe7CKtPguzBjVh24JXXjwvU1cdUoWd52T06YyVMejiUGpIKiqqWO16wBLtxSxdMs+VrsOkNU9gXGZqYzLSid3QGqDmkRrbSk8yGMf5jFv1U4iI4Qfj+vPL07PDijJFJZUMm9VPnNX5rO+oJjoSGHSsB6cPzaDCUN70CXK1hDufXMtz32+lYcuHMX5x+kS3EcTTQxKdUDbikp54sPNzF3pIkKEHx2fwbUTBtE3xXeCqKyp5YP1e5i7wsVHmwqprTOMzEjm/LEZTB/Vx2ffRXVtHZc98xXLt+3n5V+cyOh+AUziUx2aJgalOrAd+8p48uPNvLJ8BwAXHJfBdRMG0S8tDmMMq3YcYO5KF/O/KcBdXk3PpBh+OKYvF4zNYHDPxBbvv7+0ihmPf0pVTR3zbzilzU1XqmPQxKBUJ7DzQDlPfbyZOV/toNYYpo7oxYZdxWwuLCUmKoIzR/Ti/OMyOGVQNyID7O9ZX1DMeU98zrDeicy5enxAgxVa42BlDU99tJnzxvYlu3uCo2Up3zQxKNWJ7HJX8Pclm3l52Q5y+iRx/tgMpo3s3eZhp++sKeDa51dyUW4/7jv/WMcGVOwuruCKZ5exrqCYE7LSmHP1+JAN3lCH6VpJSnUivZJjuWf6CO6ZPiKo9z3r2N7cOGkQjy7OI6dPEpedlBnU+4MdmnvFs8twl1dz/tgM5q50sXjDHiYP1/0owpkOZlbqKHbTlCFMGd6T37+1js837w3qvT/L28uFT35BbZ3h5V+cyH3nH0t2t3jue2cDNZ61y1R40sSg1FEsIkL4fxeNIqtbPNc/v5Id+8qCct9XV7i47Jmv6J0Sy+vXn8wxfZOJjoxg9tShfLfnIHNXuoJSjnKGJgaljnKJsdH849JcausMP//3csqqalp9L2MMjyzaxC2vfMMJ2Wm8eu1JDYbbnjmiF2P7p/Dw+5so9yxHosKPJgalFFnd4vnbj8eyaXcJt7zyDa0ZlFJVU8etr67mkUXfcf7YDJ69fNwRHeQiwh3ThrO7uJJnPgvBSpaqVTQxKKUAOH1Id+44azgL1uzi8Q/zArq2uKKaK577ildXuLhpymAevHDkodnWjR2fmcYZOT158qPNFNVbg8xp763dFdLyOjJHE4OITBWRjSKSJyK3+3j/EhFZ7Xl8LiKjnIxHKdW8n52axblj+vLQ+5tYtG63X9fsPFDOhU9+wZdb9vHABSO5acqQFoejzp46jPLqWv62OLAE1FqvrXRx9X9WcMdra0JSXkfnWGIQkUjgceAsIAeYJSKNV+76HjjdGDMS+APwtFPxKKVaJiL8+bxjObZvMje9tIq8PSXNnr92p5tzn/iMnQfKee6KcVyY69/GR4N6JHDR8f3479JtbN1bGozQm7StqJS73viWhJgo3lu3m6+373e0vM7AyRrDOCDPGLPFGFMFzAFm1j/BGPO5Mcb7t7QU0FW9lGpnsdGR/P2nxxEbHcnP/70Cd1m1z/M+2riHHz31BREivHLtiZwyOLDtoG6aPJjoyAgeeG9jMML2qbq2jhvnrCIyQnjj+pNIj+/CAwudK6+zcDIx9AV21PvZ5TnWlKuAdxyMRynlp97JXXnqJ2Nx7S/jxjlfH7EnyItfbeeqfy1nQHo8b1x/MsN6JQVcRo+kWH5+WjZvry5g1Y4DwQq9gYff38Q3Ow5w3/kjGdQjkesnDuLzzUV8lhfcORudjZOJwVcjo8+hDiIyEZsYbmvi/atFZLmILC8sLAxiiEqppuRmpvH7mcfw8aZC7l+4AbDDUR9YuIE7XlvDKYO68fI1J9KzDYvwXX1aNt0SuvDnBetbNRKqOZ/n7eWpjzdz8fH9mHZsbwAuGd+fPsmx3L9wY9DL60ycTAwuoH6DYwaws/FJIjIS+Ccw0xhT5OtGxpinjTG5xpjc7t27OxKsUupIs8b156fjB/D3j7fw8vId3PTSKh7/cDOzxvXjn5flkhDTtlV1EmKi+OXkwXz5/T4+3LgnSFHbjbV+9fIqsrvFc/f0w12bMVGR3DRlCN/sOMB7fnauB0tFdceZt+FkYlgGDBaRLBHpAlwMvFn/BBHpD7wG/NQYs8nBWJRSrXT39BzGZaUx+9XVzFu1k1vPHMqfzj2W6CBtD3rxuP5keZbKaGob20AYY5j96mr2l1bz14vHHLHToV3hNZ4HF24MSnn+yD9Qzkn3LeYv724ISXlt5VhiMMbUADcAC4H1wMvGmLUico2IXOM57W4gHXhCRFaJiC6bqlSYiY6M4MlLxjJpWA/+evForp84KKiro0ZHRjD7zKFs2n2QuSvavlTGf7/czqL1u5k9dSjH9E0+4v2oyAh+/QO7NMe8VfltLq8lxhjueG0N+0qreHrJFr7b3fxIr3Cgy24rpdqdMYbznvycnQfK+eiWia3ex3vjrhJmPPYp47PTefby45vcl7yuzjDj8U9xl1fzwc0TmpyMFwwvL9vB7Lmr+dWUIfzfp1s4NiOZ/151QsiXHg9k2W2d+ayUanciwm/auFRGRXUtN774NYmxUTx44agmkwLYxQNvOWMoO/aV89Ky7a0Nu0UF7nL+8NY6xmen8T+TBvHrM4byWV4RC9fucqzMYNDEoJQKC8dnpvGDNiyV8ecF69m4u4QHLxxF98SYFs8/fUh3xmWl8ejivDYtHNgUbxNSTZ3h/vNtorrkhP4M7ZnIH95aH9ad0ZoYlFJh47apQymrqgl4qYxF63bzry+2cdUpWUwY2sOva0SE2WcOpbCkkn99vq014Tbr1RUuPtpYyG1Th9I/PQ6w/Rv3zhhB/oFy/v7xlqCXGSyaGJRSYWNQj0QuOr4/z3+5jW1F/i2Vsbu4gltf/Yac3knMnjo0oPJyM9OYNKwHT328GXe57xnerbHLXcHv31rHuMw0Lj0xs8F7Jw5M5+yRvXniozxc+4Oz/0WwaWJQSoWVX00ZTFREhF9LV9TVGX798jdUVNfx6KwxxEQF3mn96zOG4C6v5h9LgvMN3hjDb15fQ3VtHfdfMNJnX8dvpg1HBP60YH1Qygw2TQxKqbDSIymWn5+axVurC/imhaUynv5kC5/m7eWe6TkM6pHQqvJG9Elm+qg+PPPZ9xSWtH1Z7te/zmfxhj3MPnMYmd3ifZ7TN6Ur100YxII1u/g8DJfn0MSglAo7V58+kPT4LvypmaUyVrsO8ODCjZx1TC8uOt6/VV2bcvMPhlBZUxfwPhSN7Smu4N4313J8ZiqXn5TZ7LlXn5ZNv7Su3Dt/bdjtga2JQSkVdhJiovjllKaXyjhYWcONL35Nj8QY7jtvZJvnBGR1i+dHuRm88OX2Vrf7e5uQKmvquP+C5ofLgl3F9s6zc9i0+yD/XRr8zu+20MSglApLs5pZKuOeeWvZvq+MRy4eQ3JcdBN3CMyNkweDwF8Xfdeq6+et2smi9Xu49cyhZDXRhNTYGTk9OXVwNx5+f1NY7S6niUEpFZaiIyO41cdSGfNW5TN3pYsbJg5iXFZa0MrrndyVS8cPYO5KV4sbFDW2p6SCe+evZWz/FK44Ocvv60SEe6bnUFZVy4MO7ksRKE0MSqmwddYxvRjdL4WH399EeVUtO/aVcefr33LcgFT7DT/Irp0wkK7RkTz8vv9rehpjuPP1bymrquWBC0cR2UITUmODeiRy+UmZzFm2gzUud6AhO0ITg1IqbHmXythVXME/PtnCL+d8DcAjF40mKkiru9aXnhDDz07NZsGaXX5/SM9fXcB763ZzyxlDGNi9dSOjbpwymPT4Ltzz5rdhsU+EJgalVFgbl5XGlOE9efj9TazcfoA/nncs/dLiHCvvZ6dmkRIX7deWo4Ulldwz71vG9E/hqlOyW11mUmw0s6cOY+X2A7z+tfMrvrZEE4NSKuzdftZQoiOFC47LYMaoPo6WlRgbzXUTBrJkUyFLt/jcOwywTUh3vfEtpVW1PHDByICbkBq7YGwGo/ql8Od3NnCwMvhrNwVCE4NSKuwN6pHIp7dN4v7zR4akvEtPzKRnUgwPNLMF6NtrCnh37S5+NWUIg3oktrnMiAjhdzNGUFhSyd8Wt25kVLBoYlBKdQg9k2JbnBsQLLHRkdw4eTArtu1n8YYj51EUHazk7nlrGZWRzM9P9X8UUktG90vhwuMyeObT79lceDBo9w2UJgallPLhR7n9GJAexwMLN1LXaB7F3W+u5WBFDQ9cOCroneCzpw4jNiqS389f124d0ZoYlFLKh+jICG7+wRA27Cph/uqdh44vWFPA26sL+OWUwQzp2fYmpMa6J8bwyymD+XhToc/aSihoYlBKqSZMH9mHYb0Sefj9TVTX1rGvtIq73viWY/sm84vTWj8KqSWXnZTJoB4J/P6tdVTWhH5DH00MSinVBO8WoNuKynhluYt73lxLcUU1DzrQhFRfdGQE90zPYVtRGf/8pHVbnbaFJgallGrG5OE9GNs/hT++vY753+zkxkmDGdor+E1IjZ06uDtnjujJY4vzKHCXO15efZoYlFKqGSLCrWcOo7SqlhF9krhmwsCQlX3n2TnUGsN972wIWZmgiUEppVp04sB0HvvxGP5xaS7RDjYhNdYvLY5rTstm3qqdLNu6L2TlamJQSik/nDOyD31Suoa83GsnDKJPciz3zFt7xPLjTtHEoJRSYaxrl0h+e3YO6wqKefGr7SEpUxODUkqFuWnH9mLGqD6kxnUJSXlRISlFKaVUq4kIj84aE7LytMaglFKqAU0MSimlGnA0MYjIVBHZKCJ5InK7j/dFRB71vL9aRMY6GY9SSqmWOZYYRCQSeBw4C8gBZolITqPTzgIGex5XA086FY9SSin/OFljGAfkGWO2GGOqgDnAzEbnzAT+baylQIqI9HYwJqWUUi1wMjH0BXbU+9nlORboOYjI1SKyXESWFxYWBj1QpZRShzmZGHxttdR42p4/52CMedoYk2uMye3evXtQglNKKeWbk4nBBfSr93MGsLMV5yillAohcWrrOBGJAjYBk4F8YBnwY2PM2nrnnA3cAEwDTgAeNcaMa+G+hcA2R4J2Vjdgb3sH0Qoad+h11Ng17tAKNO4Bxhi/mlwcm/lsjKkRkRuAhUAk8IwxZq2IXON5/ylgATYp5AFlwBV+3LdDtiWJyHJjTG57xxEojTv0OmrsGndoORm3o0tiGGMWYD/86x97qt5rA1zvZAxKKaUCozOflVJKNaCJIXSebu8AWknjDr2OGrvGHVqOxe1Y57NSSqmOSWsMSimlGtDEoJRSqgFNDK0kIv1E5EMRWS8ia0Xkl57jaSLyvoh853lOrXfNHZ6VZDeKyJn1jh8nIms87z0qIr5mhAc7/kgR+VpE3uoocYtIioi8KiIbPH/uJ3aQuH/l+TfyrYi8KCKx4Rq3iDwjIntE5Nt6x4IWq4jEiMhLnuNfikimg3E/4Pm3slpEXheRlHCLu6nY6713i4gYEekW0tiNMfpoxQPoDYz1vE7ETubLAe4Hbvccvx34i+d1DvANEANkAZuBSM97XwEnYpcIeQc4KwTx3wy8ALzl+Tns4wb+BfzM87oLkBLucWPX/voe6Or5+WXg8nCNGzgNGAt8W+9Y0GIFrgOe8ry+GHjJwbjPAKI8r/8SjnE3FbvneD/sPLBtQLdQxu7Yf+Kj7QHMA34AbAR6e471BjZ6Xt8B3FHv/IWev8TewIZ6x2cBf3c41gzgA2AShxNDWMcNJGE/YKXR8XCP27tQZBp23tBbng+ssI0byKThB2zQYvWe43kdhZ25K07E3ei9c4HnwzHupmIHXgVGAVs5nBhCErs2JQWBp2o2BvgS6GmMKQDwPPfwnNbUSrJ9Pa8bH3fSI8BsoK7esXCPOxsoBJ71NIH9U0Tiwz1uY0w+8CCwHSgA3MaY98I97kaCGeuha4wxNYAbSHcs8sOuxH6LbhBDo/jCJm4RmQHkG2O+afRWSGLXxNBGIpIAzAVuMsYUN3eqj2OmmeOOEJFzgD3GmBX+XuLjWMjjxn7TGQs8aYwZA5RimzWaEhZxe9rjZ2Kr/X2AeBH5SXOX+DjWHn/e/mhNrCH/PUTkt0AN8HwLMYRF3CISB/wWuNvX203EEdTYNTG0gYhEY5PC88aY1zyHd4tnsyHP8x7P8aZWknV5Xjc+7pSTgRkishW7edIkEflvB4jbBbiMMV96fn4VmyjCPe4pwPfGmEJjTDXwGnBSB4i7vmDGeugasQttJgP7nApcRC4DzgEuMZ62lA4Q90DsF4lvPP9PM4CVItIrVLFrYmglT4///wHrjTEP13vrTeAyz+vLsH0P3uMXe0YIZGG3M/3KUzUvEZHxnnteWu+aoDPG3GGMyTDGZGI7ohYbY37SAeLeBewQkaGeQ5OBdeEeN7YJabyIxHnKmwys7wBx1xfMWOvf6wLsvz+nvnlPBW4DZhhjyhr9PmEbtzFmjTGmhzEm0/P/1IUd6LIrZLEHq/PkaHsAp2CrY6uBVZ7HNGzb3QfAd57ntHrX/BY7imAj9UaUALnAt573HiOInVot/A4TONz5HPZxA6OB5Z4/8zeA1A4S9++ADZ4y/4MdURKWcQMvYvtCqrEfSFcFM1YgFngFu6LyV0C2g3HnYdvWvf8/nwq3uJuKvdH7W/F0Pocqdl0SQymlVAPalKSUUqoBTQxKKaUa0MSglFKqAU0MSimlGtDEoJRSqgFNDKrTEZGeIvKCiGwRkRUi8oWInOt5b4J4VpRt5vp7ReSWAMs82MTx34pdWXW1iKwSkRM8x2/yzHBVKuxoYlCdimdyzxvAEmNMtjHmOOxEvozmr3QklhOxs27HGmNGYmdBe9e5uQnQxKDCkiYG1dlMAqqMMU95Dxhjthlj/tb4RLH7DLzh+Ta/VERG1nt7lIgsFrsHwc895yeIyAcistKz7v3MFmLpDew1xlR64thrjNkpIjdi1036UEQ+9Nz7DE/NZqWIvOJZgwsR2SoifxGRrzyPQZ7jF4rd3+EbEVnS+j8upY6kiUF1NiOAlX6e+zvga8+3+d8A/6733kjgbOySxneLSB+gAjjXGDMWmAg85KmhNOU9oJ+IbBKRJ0TkdABjzKPYdWwmGmMmit2E5U5giufey7H7ZXgVG2PGYWezPuI5djdwpjFmFDDDz99XKb9oYlCdmog87vlWvczH26dgl6jAGLMYSBeRZM9784wx5caYvcCHwDjsKpV/EpHVwCLscsY9myrbGHMQOA64Grtk+EsicrmPU8djN2D5TERWYde1GVDv/RfrPZ/oef0Z8JynNhPZzB+BUgGLau8AlAqytcD53h+MMdd7vpEv93Fuc8sRN14rxgCXAN2B44wx1Z6VL2ObC8YYUwt8BHwkImuwH/rP+YjjfWPMrKZu0/i1MeYaT0f22cAqERltjClqLhal/KU1BtXZLAZiReTaesea6uRdgv2wR0QmYPsDvHtqzBS7N3M6drHBZdjlivd4ksJEGn6rP4KIDBWRwfUOjcZu0whQgt0SFmApcHK9/oM4ERlS77qL6j1/4TlnoDHmS2PM3dgdueovxaxUm2iNQXUqxhgjIj8E/p+IzMY24ZRil19u7F7sjnCrgTIOL00MdhXKt4H+wB88ncbPA/NFZDl2tc4NLYSTAPxN7Cb0NdjVLa/2vPc08I6IFHj6GS4HXhSRGM/7d2L3EQeIEZEvsV/kvLWKBzxJR7Arnjbe6UupVtPVVZUKY57mqlxPX4dSIaFNSUoppRrQGoNSSqkGtMaglFKqAU0MSimlGtDEoJRSqgFNDEoppRrQxKCUUqqB/w/l4yuxhJNcgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (comment,attack ), _ in test_loader:\n",
    "\n",
    "                comment = comment.type(torch.LongTensor)           \n",
    "                comment = comment.to(device)\n",
    "                attack = attack.type(torch.LongTensor)  \n",
    "                attack = attack.to(device)\n",
    "                output = model(comment, attack)\n",
    "\n",
    "                _, output = output\n",
    "                y_prob.extend(output.tolist())\n",
    "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "                y_true.extend(attack.tolist())\n",
    "    return y_true, y_pred,y_prob    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model = BERT().to(device)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "\n",
    "y_true, y_pred,y_prob = evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_true = []\n",
    "for i in y_true:\n",
    "    if i == 1:\n",
    "        label_true.append([1,0])\n",
    "    else:\n",
    "        label_true.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_final = []\n",
    "for i in range(len(y_prob)):\n",
    "    tempA = abs(y_prob[i][0])\n",
    "    tempB = abs(y_prob[i][1])\n",
    "    y_prob_final.append(tempB/(tempA+tempB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000       231\n",
      "           0     0.9165    1.0000    0.9564      2535\n",
      "\n",
      "    accuracy                         0.9165      2766\n",
      "   macro avg     0.4582    0.5000    0.4782      2766\n",
      "weighted avg     0.8399    0.9165    0.8765      2766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\conda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2400 = pd.read_csv('Data/test5.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fucking piece of shit your whole community is...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im not being funny but coronavirus in china ir...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>got on the victoria line today to seven sister...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s appalling that the media amp libtards bit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dude fuck the chinese man fuck em and if you t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  attack\n",
       "0   fucking piece of shit your whole community is...     1.0\n",
       "1  im not being funny but coronavirus in china ir...     0.0\n",
       "2  got on the victoria line today to seven sister...     1.0\n",
       "3  it s appalling that the media amp libtards bit...     1.0\n",
       "4  dude fuck the chinese man fuck em and if you t...     1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2400.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2400['attack'] = [int(i) for i in test_data2400['attack']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 75\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('text', text_field),('hate', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='GABtrain.csv', validation='GABvalid.csv',\n",
    "                                           test='test5.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred,y_prob = evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_true = []\n",
    "for i in y_true:\n",
    "    if i == 1:\n",
    "        label_true.append([1,0])\n",
    "    else:\n",
    "        label_true.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_final = []\n",
    "for i in range(len(y_prob)):\n",
    "    tempA = abs(y_prob[i][0])\n",
    "    tempB = abs(y_prob[i][1])\n",
    "    y_prob_final.append(tempB/(tempA+tempB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000       678\n",
      "           0     0.7076    1.0000    0.8288      1641\n",
      "\n",
      "    accuracy                         0.7076      2319\n",
      "   macro avg     0.3538    0.5000    0.4144      2319\n",
      "weighted avg     0.5007    0.7076    0.5865      2319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\conda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAHwCAYAAAAW3v7qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5hV1dn38e89Q+8dFLAiFjR27IodY0GiKJZoLOExGjXRvJZo4pNiEpNHY0xs2FCjFCsae8PeMBZELChSBAug0svMrPePc3AGHKXtM2eY+X5ynYu91957rXs4GA6/s/bakVJCkiRJkiRJ+j4lxS5AkiRJkiRJtZ8hkiRJkiRJkpbLEEmSJEmSJEnLZYgkSZIkSZKk5TJEkiRJkiRJ0nIZIkmSJEmSJGm5DJGkAoiIphFxf0R8HRF3rEY/x0TEo1nWVgwR8VBEHF/sOiRJkiRJq84QSfVaRBwdEaMjYk5ETMuHHbtm0PXhQGegfUppwKp2klK6LaW0Xwb1LCUi+kREioi7l2nfMt8+agX7+d+I+PfyzkspHZBSunkVy5UkSarzIuLjiJif/1z6aUQMiYgWy5yzc0Q8GRGz819W3h8Rmy1zTquIuDwiJuX7Gp/f71CzP5GkusgQSfVWRJwFXA78iVzgsw5wFdAvg+7XBd5PKZVl0FehfAHsHBHtq7QdD7yf1QCR4//PSJIkrZiDU0otgK2ArYHzlxyIiJ2AR4GRwNrA+sCbwPMRsUH+nEbAE0AvoC/QCtgZmAH0LlTREdGgUH1Lql38x53qpYhoDfweOC2ldHdKaW5KaXFK6f6U0v/Ln9M4/63N1Pzr8ohonD/WJyKmRMTZEfF5fhbTCfljvwN+CxyZ//bnpGVn7ETEevkZPw3y+z+JiI/y3ypNiIhjqrQ/V+W6nSPi1fw3T69GxM5Vjo2KiD9ExPP5fh5dzjdOi4B7gYH560uBI4Dblvm9+kdETI6IWRHxWkTslm/vC/y6ys/5ZpU6Lo6I54F5wAb5tpPzx6+OiDur9H9JRDwREbHCb6AkSVIdllL6FHiEXJi0xF+BW1JK/0gpzU4pzUwpXQi8BPxv/pzjyH0x2j+l9E5KqSKl9HlK6Q8ppQerGysiekXEYxExMyI+i4hf59uHRMQfq5zXJyKmVNn/OCLOjYi3gLkRcWHVz3j5c/4REVfkt1tHxA35z82fRMQf858/Ja1BDJFUX+0ENAHu+Z5zLgB2JPeX95bkvr25sMrxLkBroCtwEnBlRLRNKV1EbnbT8JRSi5TSDd9XSEQ0B64ADkgptST3bdEb1ZzXDnggf2574DLggWVmEh0NnAB0AhoBv/q+sYFbyH3YANgfGAtMXeacV8n9HrQDbgfuiIgmKaWHl/k5t6xyzY+BQUBLYOIy/Z0N/CAfkO1G7vfu+JRSWk6tkiRJ9UJEdAMOAMbn95uR+4xY3VqbI4B989v7AA+nlOas4DgtgceBh8nNbupBbibTijoKOBBoA9wK/DAiWuX7XvIF5e35c28GyvJjbA3sB5y8EmNJqgUMkVRftQemL+d2s2OA3+e/vfkC+B25cGSJxfnji/Pf7MwBNl7FeiqAzSOiaUppWkppbDXnHAh8kFK6NaVUllIaCrwLHFzlnJtSSu+nlOaT+0CxVTX9fCOl9ALQLiI2Jhcm3VLNOf9OKc3Ij3kp0Jjl/5xDUkpj89csXqa/ecCx5EKwfwOnp5SmVNeJJElSPXNvRMwGJgOfAxfl29uR+7fbtGqumQYsmX3e/jvO+S4HAZ+mlC5NKS3Iz3B6eSWuvyKlNDmlND+lNBH4L3Bo/thewLyU0ksR0ZlcKPaL/B0AnwN/Jz8jXtKawxBJ9dUMoMNy7t9em6Vn0UzMt33TxzIh1DxgqcUPV0RKaS5wJHAKMC0iHoiITVagniU1da2y/+kq1HMr8HNgT6qZmZW/ZW9c/ha6r8jNvlrewoyTv+9gSukV4CMgyIVdkiRJgkPzM9P7AJtQ+ZnrS3JfOq5VzTVrAdPz2zO+45zv0h34cJUqzVn2M9/t5GYnQW6G/JJZSOsCDcl91v0q/5nyWnKz5yWtQQyRVF+9CCyg8puS6kwl9xfeEuvw7Vu9VtRcoFmV/S5VD6aUHkkp7UvuL/13getWoJ4lNX2yijUtcStwKvBgfpbQN/K3m51Lbipy25RSG+BrcuEPwHfdgva9t6ZFxGnkZjRNBc5Z9dIlSZLqnpTS08AQ4P/y+3PJfX6t7qm/R1B5C9rjwP755RJWxGRgw+849r2fX5eUusz+HUCf/O14/akMkSYDC4EOKaU2+VerlFKvFaxTUi1hiKR6KaX0NbnFr6+MiEMjollENIyIAyLir/nThgIXRkTH/ALVvyV3+9WqeAPYPSLWyS/qXfVJG50j4pD8X/YLyd0WV15NHw8CPSPi6IhoEBFHApsB/1nFmgBIKU0A9iC3BtSyWpK7d/0LoEFE/JbcUz6W+AxYL1biCWwR0RP4I7lb2n4MnBMR33vbnSRJUj10ObBvlc9J5wHHR8QZEdEyItrmF77eidyyC5D7cnAycFdEbBIRJRHRPiJ+HRE/rGaM/wBdIuIXkXuoTMuI2CF/7A1yaxy1i4guwC+WV3B+CYhRwE3AhJTSuHz7NHJPlrs0Ilrl69owIvZYhd8XSUVkiKR6K6V0GXAWucWyvyD3F+7PyT2xDHJBx2jgLWAMuXu8//jtnlZorMeA4fm+XmPp4KeE3GLTU4GZ5AKdU6vpYwa5+9bPJjdV+RzgoJTS9GXPXYX6nkspVTfL6hHgIeB9crfOLWDpactLFnecERH/Xd44+dsH/w1cklJ6M6X0AbknvN0a+SffSZIk6ZtA5hbgN/n958g9COVH5NY9mkhugepd85+pSCktJLe49rvAY8As4BVyt8V9a62jlNJscotyH0xuWYQPyC1xALlA6k3gY3IB0PAVLP32fA23L9N+HLkHv7xD7va8O1m5W+8k1QLhA5EkSZIkSZK0PM5EkiRJkiRJ0nIZIkmSJEmSJGm5DJEkSZIkSZK0XIZIkiRJkiRJWi5DJEmSJEmSJC1Xg2IX8F0WlOFj46QVcM2LE4pdgrRG+MVu60ehx2i69c8L8nfX/Nf/VfDapSXatGmTevToUewyVMXcuXNp3rx5scvQMnxfah/fk9rJ96X2ee2116anlDquyrW1NkSSJElSzevcuTOjR48udhmqYtSoUfTp06fYZWgZvi+1j+9J7eT7UvtExMRVvdYQSZKkrIR3iUuSJKnu8tOuJEmSJEmSlsuZSJIkZSVcukiSJEl1lzORJEmSJEmStFzORJIkKSuuiSRJkqQ6zE+7kiRlJaIwr+UOGzdGxOcR8fYy7adHxHsRMTYi/lql/fyIGJ8/tn+V9m0jYkz+2BUR3p8nSZKkSoZIkiSt+YYAfas2RMSeQD/gBymlXsD/5ds3AwYCvfLXXBURpfnLrgYGARvlX0v1KUmSpPrNEEmSpKxESWFey5FSegaYuUzzz4C/pJQW5s/5PN/eDxiWUlqYUpoAjAd6R8RaQKuU0osppQTcAhya0e+MJEmS6gBDJEmS6qaewG4R8XJEPB0R2+fbuwKTq5w3Jd/WNb+9bLskSZIEuLC2JEnZKdASQhExiNxtZksMTikNXs5lDYC2wI7A9sCIiNgAqK7I9D3tkiRJEmCIJElSdgr0dLZ8YLS80GhZU4C787emvRIRFUCHfHv3Kud1A6bm27tV0y5JkiQB3s4mSVJddS+wF0BE9AQaAdOB+4CBEdE4ItYnt4D2KymlacDsiNgx/1S244CRxSldkiRJtZEzkSRJykqBbmdb/rAxFOgDdIiIKcBFwI3AjRHxNrAIOD4/K2lsRIwA3gHKgNNSSuX5rn5G7klvTYGH8i9JkiQJMESSJGmNl1I66jsOHfsd518MXFxN+2hg8wxLkyRJUh1iiCRJUlYKtCaSJEmSVBv4aVeSpKxEFOYlVSMiboyIz/O3LFZ3PCLiiogYHxFvRcQ2NV2jJEmqWwyRJEmS1kxDgL7fc/wAcgunbwQMAq6ugZokSVId5u1skiRlxdvZVINSSs9ExHrfc0o/4Jb8guovRUSbiFgr/yQ+SZJUH306erUuN0SSJEmqm7oCk6vsT8m3GSJJklSfPH8RTHqCR//bhK2av7RaXRkiSZKUFdcvUu1S3R/IVO2JEYPI3fJGx44dGTVqVAHL0sqaM2eO70kt5PtS+/ie1E6+L8XVaeaTbPrRH/jbqF0478Fd2XW9dYCbVrk/QyRJkqS6aQrQvcp+N2BqdSemlAYDgwE23njj1KdPn4IXpxU3atQofE9qH9+X2sf3pHbyfSmiijLm/3Vfjh3xI25//QcA7PujPjx76aqHSC7eIElSVqKkMC9p1dwHHJd/StuOwNeuhyRJUj2QKuCDe5jy997sfuUJ3P76D2jRvIR77jmS3/zfcavVtTORJEnKioGPalBEDAX6AB0iYgpwEdAQIKV0DfAg8ENgPDAPOKE4lUqSpBqTElzehBc/6kz/mwfy2ewWrN9xLvc9+Ss237zTandviCRJkrQGSikdtZzjCTithsqRJEnFMu8LuPcQmDsNZk0E4OH3evDZ7Bbs1bsxI+46mfbdVj9AAkMkSZKyU+LC2pIkSaohsz+B538D7w2DsvlLHbrosI/pfsTBHH/8ljRsWJrZkIZIkiRJkiRJa4qKcnhvODx4zDdNM+c15czHTuCSwaex9totKGnZnZML8ORgQyRJkrLimkiSJEkqhIpy+OJNSOXw3IUw8dFvDo1t8T/0u2JdPpy0iFnnvsXIkQMLVoYhkiRJWSnAtz2SJEkST5wGb137reb7Wg/jmF9+xJw5i9hmm7X4178OKGgZhkiSJEmSJEm10aI5cO3asGh2ZVvn7UiU8KcPLuI3/+9VUoKBAzfnhhsOoVmzhgUtxxBJkqSseDubJEmSsvTq3yoDpCbt4Sdvk5p15uij72bYsFeJgD//eW/OPXcXogZmxRsiSZIkSZIk1Ubzp+d+7bgl/Ph1iCCAzTfvSMuWjbj99sM46KCeNVaOIZIkSVlxTSRJkiRloaIM7tgHpr2Y29/ip8xfUEbTprnb1X7969348Y+3ZJ11WtdoWc67lyQpK1FSmJckSZLql9mTYcrTUL4IGjTh2kc70rPnv5g06WsAIqLGAyQwRJIkSZIkSaqVFjdbh1PHDeOU88YxZcos7rlnXFHr8XY2SZKy4u1skiRJWl0pwbtD+WJOMwYM/iFPv/8GjRqVMnjwQRx//FZFLc0QSZIkSZIkqTYoWwAfP8pbI67gkJsGMfHLNnTp0oJ77jmSHXfsVuzqDJEkScqM6xdJkiRpVT1+Grx5FTPmNmW3q85k1oImbL91W+65/yd07dqq2NUBhkiSJGXH29kkSZK0st65FZ74OSyaBUD75vP57QGv8kbZgQwe/rNvnshWGxgiSZIkSZIk1bSJj8N7I2DMdcxZ2IgPpndh666fwqkzOOustkDuKWy1iSGSJElZ8XY2SZIkrYgpz8Kd+wIwYUYb+g05ik/md+PVV3/KBk3bUbuio0qGSJIkSZIkSTXhuQtg0lMw7UUAnhq/HgOGnsCMrxMbb9yK8lS7v5Q0RJIkKSu1bLqxJEmSapGvP4aX/wRASnDVC9tz5n0HUl6eOOCAHgwdehitWzcpbo3LYYgkSZIkSZJUaFNzs48WlZXy87eu4bp7PgHgnHN25k9/2pvS0to9CwkMkSRJyo5rIkmSJKk6qQIePBqAN8r6cuPwqTRp0oDrrz+YY475QZGLW3GGSJIkZcUQSZIkSctaPA+uaP7Nbu+B/8MN3brTq1cntttu7SIWtvIMkSRJkiRJkrK0aDZMexk+uAvevIbhb/SidZOF9O3bAzY8mOM3LHaBq8YQSZKkrLiwtiRJklIF3HUATH2eiorgN4/sxZ+e2J3WzRYz7vfnslax61sNhkiSJEmSJEmra/4MeG84PHEaALMWNObY4Udx/5j1KC0Nfv/ng+nSpUWRi1w9hkiSJGXFNZEkSZLqn3nT4emz4J1bv2kaP70dh9xyAuOmtqRt2yaMGDGAffbZoIhFZsMQSZKkrHg7myRJUv1SthBu2hgWzPym6ckv9uHwa/bgy6/L2WyzjowcOZAePdoVscjs+JWpJEmSJEnSylr4NYwdUhkgddsDBk2m8cE3MGde4uCDe/LiiyfVmQAJnIkkSVJ2vJ1NkiSpfvh0NNy2/Te7Fc3WpuTIUQDssgu88MJJbLPNWpSU1K2Z6n7alSRJkiRJWlHzZy4VIH2aNmX3G37JyJHvftO23XZr17kACZyJJElSdlwTSZIkqW5bOAuuav/N7uiuV3Hor+bzySez+eqCJznooJ6Ultbd+TqGSJIkZSQMkSRJkuqed4fD2zfkli74+JFvmm/7+AhOvnAmCxaUsdtu63DnnUfU6QAJDJEkSZIkSZKqN34kPDBwqabyiuDXz53IX+/rDpQxaNA2/POfP6RRo9Li1FiDDJEkScqIM5EkSZLqkOcuhJcvrtzvdy+UNuZ/fvMpN9w3kQYNSrjiir787Gfbf3cfdYwhkiRJkiRJUlXT3146QDrmFeiSC4v+58xPeOTpEdx6a3/69FmvOPUViSGSJElZKdJEpIi4ETgI+DyltPkyx34F/A3omFKanm87HzgJKAfOSCk9km/fFhgCNAUeBM5MKaWa+jkkSZKKatorMPX53PbMyietceoMxn8CPfK722/flfHjT6dx4/oXqdS/n1iSpLpnCPAv4JaqjRHRHdgXmFSlbTNgINALWBt4PCJ6ppTKgauBQcBL5EKkvsBDNVC/JElSzRt9Kbx5NZQ2zu3PeOdbp6QND+XSK9/l3HMfZ+jQwzjiiF4A9TJAAkMkSZIyU6w1kVJKz0TEetUc+jtwDjCySls/YFhKaSEwISLGA70j4mOgVUrpRYCIuAU4FEMkSZJU18yaBA8eA588V/3xbc4EgvmLGzDo+s349x2PATBhwpc1V2MtZYgkSVJGatPC2hFxCPBJSunNZerqSm6m0RJT8m2L89vLtkuSJNUdX46HGzdauu2oF6BRq9x26/WhYTM++WQW/fsP59VXJ9G8eUNuvbU//ftvWvP11jKGSJIk1XIRMYjcbWZLDE4pDf6e85sBFwD7VXe4mrb0Pe2SJEl1w6zJSwdIW54Cu/wRmrZf6rSXXppC//7D+fTTOay/fhtGjhzIFlt0ruFiaydDJEmSMlKomUj5wOg7Q6NqbAisDyyZhdQN+G9E9CY3w6h7lXO7AVPz7d2qaZckSVrzTXgY7j6gcv+Ht8GmR3/rtLKyCo4//l4+/XQOe+65HiNGDKBDh2Y1V2ctZ4gkSVIdk1IaA3Rasp9f72i7lNL0iLgPuD0iLiO3sPZGwCsppfKImB0ROwIvA8cB/6z56iVJklZD+SKY+DhMHwPPngeNWubaF82uPGebM6sNkAAaNChhxIjDufnmN7nkkn1o2LC0BopecxgiSZKUkWKtiRQRQ4E+QIeImAJclFK6obpzU0pjI2IE8A5QBpyWfzIbwM/IPemtKbkFtV1UW5IkrTle/D28cNHSbVXDI4Af3g6bDFyq6csv53P33eM46aRtANhyyy5cdlmXQla6xjJEkiQpK0VaVzuldNRyjq+3zP7FwMXVnDca2DzT4iRJkgpt6ovw/p3w2mVLt298JGx1KnTcKrffqAVEyVKnvPPOF/TrN4zx42fStGlDjj56ixoqes1kiCRJkiRJktZML/8Fnjt/6bajXoC1dvhWYLSs++9/j2OOuZvZsxex1VZd2HXXdQpYaN1giCRJUkaKdTubJElSvTR7ytIB0g6/hvV/CGvv9L2XpZT485+f48ILnyQlOOKIXtx44yE0b96owAWv+QyRJEmSJEnSmmXWJLhu3cr9QVOgZdflXjZv3mJOPHEkw4ePBeDii/fi/PN39cvAFWSIJElSRvzwIUmSVANmTYTbdqjc3+PSFQqQABYsKGP06Km0aNGI2277EYccsnGBiqybDJEkScqIIZIkSVKBpATzPoPH/gc+vK+yfduzYbuzVribdu2act99R5FSolevTgUotG4zRJIkSZIkSbXXh/+Bew/+dvtGh8HOFy338uuv/y/vvPMFl122PwCbbdYx6wrrDUMkSZIy4kwkSZKkjKQE/zkSZk+CaS8vfWyTo6DP36F55+/tYvHicn75y0e48spXARgwYDN22ql7oSquFwyRJEmSJElS7TKyP3w4cum2496Ejj9YocunT5/HEUfcwVNPfUyjRqVcc82BBkgZMESSJCkrTkSSJElaPdNegZnjlg6QjnoR2vaEpu1WqIsxYz6jX79hTJjwFV26tODuu48wQMqIIZIkSZIkSap5KcHM9+CZX0H5Ivh6Anw1fulzTvsSmrRZ4S6fe24Sffv+m7lzF7Pddmtzzz1H0q1bq4wLr78MkSRJyohrIkmSJK2g8SNh5KHffbznEbDhQSsVIAFssUUnundvzbbbrsV11x1M06YNV7NQVWWIJElSRgyRJEmSlqN8Ebz+L3j67KXbe/0kt2B2lMDaO0HD5ivc5dy5i2jQoITGjRvQunUTnnvuBNq1a+pnswIwRJIkSZIkSTXjli1h5ruV+wffAT0PX+XuPv74K/r1G8b226/NddcdTETQvn2zDApVdQyRJEnKiN92SZIkLS1SObxzK8z9LNdQNUA6djR03naV+x416mMOP3wEM2bMZ8GCMr76agFt2zZdzYr1fQyRJEmSJElSdlIFDN8D5k9nj6qhUVVnzFmpW9aWdfXVr3LGGQ9TVlZB3749GDr0MNq0abLK/WnFGCJJkpQVJyJJkqT6avSlMH1sbnvsTdWfs21+HaS1dljlAGnRonLOOOMhrr32NQD+3//bmT//eW9KS0tWqT+tHEMkSZIy4u1skiSpXrp9R5j2cjUHglc2u4neO+0KrTeADD4r/elPz3Ltta/RuHEp119/CMce+4PV7lMrzhBJkiRJkiStnIqy3PpGi+ctHSDtd0Pu1wZNocchzHv+VWizYWbD/upXO/Pyy5/w+9/3Yfvtu2bWr1aMIZIkSRlxJpIkSarzZrwDY26A1y779rGzyiGyv63s4YfHs8ce69K0aUNatGjEQw8dk/kYWjHeNChJkiRJklbMCxctHSA17QAdNodd/pB5gFRRkfjtb5/igANu46c/vZ+UUqb9a+U5E0mSpIw4E0mSJNVJKcHsKfDGlfD+nbm2LU6GngNgvf0KMuTs2Qv58Y/vYeTI9ygpCbbbbu2CjKOVY4gkSVJGDJEkSVKddN16MHvS0m1bnQadtirIcB9+OJN+/YYxduwXtG3bhOHDD2fffbNbV0mrzhBJkiRJkiRVb/7MpQOk1hvAofdBh14FGe6JJz7iiCPuZObM+Wy6aQfuu+8oevRoV5CxtPIMkSRJyooTkSRJUl3z4cjK7bMLvybRkCFvMnPmfA4+uCf//vePaNWqccHH1IozRJIkSZIkSdWb+V7u12adamS4a689iB126Mqpp25PSYnf0NU2hkiSJGXENZEkSdIa78sP4KHjYMGX+f18iLTe/gUZ7rPP5nDhhU9y+eV9ad68Ec2aNeTnP+9dkLG0+gyRJEmSJElSzoSHYdpL327f4uTMh3rttakceuhwpkyZRaNGpVx55YGZj6FsGSJJkpQRZyJJkqQ1WkU5fHBXbnvTY2CHC3PbzTpC0/aZDjV06BhOPPE+FiwoY5dduvPb3+6Raf8qDEMkSZIyYogkSZLWaM+eB1Oezm03Xxvab5L5EOXlFVxwwZNccsnzAJx88tZceeWBNGpUmvlYyp4hkiRJkiRJgq/GV25vfVrm3S9cWMZhh43ggQc+oLQ0+Mc/+nLqqdv7RdwaxBBJkqSs+PlHkiStqRbPgw/vz20fOAxarZv5EI0aldKxY3Pat2/KHXcMYM891898DBVWSbELkCRJ0qqJiL4R8V5EjI+I86o53joi7o+INyNibEScUIw6JUm13OJ58PgpkMpz+w2aZNr9okW5fiOCa645kNdeG2SAtIYyRJIkKSMRUZCXVJ2IKAWuBA4ANgOOiojNljntNOCdlNKWQB/g0ohoVKOFSpJqt3FD4Yrm8M6tuf0GTWHdfTLpOqXEiBGT2W67wcyatRCAxo0bsO66bTLpXzXPEEmSpIwYIqmG9QbGp5Q+SiktAoYB/ZY5JwEtI/cHqQUwEyir2TIlSbVSSjD2Fnjw6Mq2TlvDcW9Bw+ar3f2CBWUcf/y9XH31R4wZ8zkPPvjBavep4nNNJEmSpDVTV2Bylf0pwA7LnPMv4D5gKtASODKlVLFsRxExCBgE0LFjR0aNGlWIerWK5syZ43tSC/m+1D6+JyuuQdkc1pr+ABt+cs03ba9ueh1zm/WAN6eQ+ytl1U2fvpDf/GYs7747m8aNSzj//E3o0mW6708dYIikFfL8s89wyV8upqK8gv6HDeCknw4qdklSUS2cN4dRN1/OzE8+BoI9T/glbz12L199lvsLd9G8OTRq1oIjLrqK8rIyRt18OdMnjaeivJyNd96bbX44sKj1qzCcNaQaVt0fuLTM/v7AG8BewIbAYxHxbEpp1lIXpTQYGAyw8cYbpz59+mRfrVbZqFGj8D2pfXxfah/fkxUwazLc1BPKFizdfuxotu+8bSZDvPzyFM44YzjTps1h3XVbc+GFPTj55IMy6VvFZ4ik5SovL+dPF/+ea6+7ic6dO3P0kYfTZ8+92LBHj2KXJhXNc0OvoXuvbdn/ZxdSXraYskUL2e+UX39z/IXhg2nULDcN+MPXnqWibDFH/u4aFi9cwPDfDqJH7z606tClWOVLqhumAN2r7HcjN+OoqhOAv6SUEjA+IiYAmwCv1EyJkqRa5bGfLh0glTSEY16BTltl0v0HH8xgjz2GsHBhOXvssS533DGAsWNfzaRv1Q6GSFqut8e8Rffu69Kte+5zat8fHsiop54wRFK9tWj+XKZ9MIa9TjwbgNIGDSlt0PCb4yklxo9+hkN+dQmQmyqweOECKsrLKV+8iJIGDWnUZPXvM1ft40wk1bBXgY0iYn3gE2AgcPQy50wC9gaejYjOwMbARzVapSSpdqgog48fyW1vcDAcOhIy/uyy0UbtOeGErYgI/rxhx1EAACAASURBVPGPvjRsWJpp/yq+goVIEXFASumhZdpOSSld813XqHb6/LPP6LJW5YyJTp07M+att4pYkVRcs774lKYtWvPUTZcyY/IEOqzbg12P+hkNG+cehTrtg7dp1qotbTp3BWCDbXfj4zde4uazj6Zs0QJ2OfJ/aNKiZTF/BBWKGZJqUEqpLCJ+DjwClAI3ppTGRsQp+ePXAH8AhkTEGHJ/Qs9NKU0vWtGSpJpVvgjG3wtjboCJj1a2H3h7ZgHSl1/OZ+bM+Wy4YTsArrzyQEpK/FBUVxVyJtJvImJhSulJgIg4l9yjZQ2R1jDpW8sr+G276reKinK+mDSeXY8+lc4bbMJzQ6/m9YeG0/vQ4wH44OVR9Ojd55vzP5/wHlFSwnH/dxsL583h3kvOpttmW9Oq41pF+gkk1RUppQeBB5dpu6bK9lRgv5quS5JUC0x8Au7c59vtG/0IGrXIZIhx476gX79hpASvvHIybds2NUCq40oK2PchwJ8iYreIuJjcY2gP+b4LImJQRIyOiNE3XDe4gKVpZXTu3IVPp336zf7nn31Gp06diliRVFwt2nagRdsOdN5gEyA30+iLieMBqCgvZ8J/n6fH9rt/c/4HrzxF9823pbRBA5q1asNaPXrx+cc+4rQuioiCvCRJklbYW9fBE6d/O0Da/lw4dQYcclcmwzzwwPvssMP1fPDBTJo3b8icOYsy6Ve1W8FmIqWUpkfEIcDjwGvA4flFHb/vmm+eDLKgrJrpLyqKXptvwaRJHzNlymQ6d+rMww8+wJ//dmmxy5KKplnrdjRv15EvP51M2y7d+WTc67Rdex0Apox7nTZrdadFu47fnN+yXSc+GfcmPXfcm7JFC/nso3f5wT6HFqt8SZIk1VWzP4HHlnmS9h7/B9ueldntayklLrnkeX796ydICQYM2IybbupH8+aNMulftVvmIVJEzCb3eNnI/9oI2AA4PCJSSqlV1mOqsBo0aMD5F/yWnw06mYqKcg7tfxg9emxU7LKkotrtqFN54rq/Ul62mFYd12KvE84CYPwro9ioyq1sAJvveTBP3nQpwy/6H0iw8S770r77BkWoWoXmrCFJklQ0C7+Gwd0q9/e8App1gp6HZxYgzZu3mJNPvo+hQ98G4A9/2JMLLtjNz0D1SOYhUkrJ1WLroN1234Pddt+j2GVItUaHdTbk8N/881vte534q2+1NWzSlP1/dmFNlCVJkqT6aOzN8PBPKvfX2Qe2OT3zYR55ZDxDh75NixaN+Pe/+9Ov3yaZj6HarWBrIkVE/4hoXWW/TUR4/4Ykqc6KKMxr+ePGjRHxeUS8XaXtbxHxbkS8FRH3RESbKsfOj4jxEfFeROxfpX3biBiTP3ZF+LWiJEm138t/XiZA2hsOf6QgQ/Xvvyl/+cvevPjiSQZI9VQhF9a+KKX09ZKdlNJXwEUFHE+SpKIq4sLaQ4C+y7Q9BmyeUvoB8D5wfr7GzYCBQK/8NVdFRGn+mquBQcBG+deyfUqSpNpm7JDK7RM/gAGPQ2T3T/2bbnqdt9767Jv9c8/dlc0390FL9VUhQ6Tq+i7YQt6SJNVXKaVngJnLtD2aUirL774ELFkkoR8wLKW0MKU0ARgP9I6ItYBWKaUX8w/CuAVwBrEkSbXZ/Bnw5fu57R+/Dm17ZNb14sXlnHHGQ5x44n306zeMuXN9+poKG+qMjojLgCvJLbB9OrmntEmSVCfV4pu/TgSG57e7kguVlpiSb1uc3162XZIk1Ubli+GqDpX77TfLrOsZM+ZxxBF38uSTE2jYsIQLL9zNp68JKOxMpNOBReQ+tN4BLABOK+B4kiTVSRExKCJGV3kNWv5V31x7AVAG3LakqZrT0ve0S5Kk2mbxXLi8Sqiz5SlQmk3I8/bbn9O79/U8+eQEOnduzqhRP+Gkk7bJpG+t+Qo2EymlNBc4r1D9S5JU2xRqHeqU0mBg8MpeFxHHAwcBe+dvUYPcDKPuVU7rBkzNt3erpl2SJNUmT/0C/vuPyv1uu8M+V2fS9ciR73LssfcwZ84itt12Le6550i6d2+9/AtVbxQsRIqIjsA55BbubLKkPaW0V6HGlCSpmGrT7WwR0Rc4F9gjpTSvyqH7gNvzt5yvTW4B7VdSSuURMTsidgReBo4D/lnTdUuSpO+QEgzbFaa+UNm20Y/g4DszG2LevMXMmbOIo47anOuvP4RmzRpm1rfqhkKuiXQbuVvZDgJOAY4HvijgeJIk1UsRMRToA3SIiCnknoZ6PtAYeCw/Q+qllNIpKaWxETECeIfcbW6npZTK8139jNyT3poCD+VfkiSp2D56AB4bBHOqTBI+dTo0bb/aXaeUvplNfdRRW7D22i3Zffd1CzbDWmu2QoZI7VNKN0TEmSmlp4GnI+LpAo4nSVJRlZQU58NWSumoappv+J7zLwYurqZ9NLB5hqVJkqTVNe1luOegpdt+sTCTNZAmTvyKgQPv4p//PIDttlsbgD32WG+1+1XdVciFtRfnf50WEQdGxNYsvdaCJEmSJEla1pTn4M794caecPuOle17Xg6nfZlJgPTMMxPZbrvreOmlKZxzzmOr3Z/qh0LORPpjRLQGzia3pkIr4BcFHE+SpKJy1rckScrEW9fAxEeXbtvrX7B1Ng88v+aa0Zx++kOUlVWw334bMmzYYZn0q7qvkCHSlymlr4GvgT0BImKXAo4nSVJRuXaAJElabfO+gHG35bZ3uADW3Qe69IaGzVa768WLyznzzIe5+urRAJx99k785S/70KBBIW9SUl1SyBDpn8A2K9AmSZIkSVL9U74YFn6d2x79fzBzHHx4X+XxjfpD520zGSqlRP/+w3nggQ9o3LiUwYMP5rjjtsykb9UfmYdIEbETsDPQMSLOqnKoFVCa9XiSJNUWTkSSJEkrbPE8uHFjmDOl+uPbn5NZgAS5GdMnnrg1b7zxKXfffSS9e3fNrG/VH4WYidQIaJHvu2WV9lnA4QUYT5IkSZKk2m/xXPjsNUgpFx7NmQIENGkHJGjSFva4FNptCu16ZjLk5Mlf0717awB+9KNN6du3B82aNcykb9U/mYdIKaWngacjYn5K6a9Vj0XEAOCDrMeUJKk2cE0kSZL0nVKCK1p8u71tTzjx3cyHq6hI/O53o7jkkud56qnj2Wmn7gAGSFothVw9a2A1becXcDxJkiRJkmqXVAHvjYDLqvzzu/1m0G136LYH7JD9P5Nnz17I4YeP4Pe/f4bFiysYM+bzzMdQ/VSINZEOAH4IdI2IK6ocagkszno8SZJqC2ciSZKkpSyeB9euXbl4NsB6feGwhwo25EcffUm/fsN4++3PadOmCcOGHcb++/co2HiqXwqxJtJU4DXgkPyvS6wLzCvAeJIk1QpmSJIkaSmfvrJ0gHTYw7De/gUb7sknJzBgwB3MnDmfTTbpwMiRA+nZs33BxlP9U4g1kd4E3oyI24BewNHAEcAE4K6sx5MkSZIkqdYpXwTj781tN2kPp34OUbgVZWbPXvhNgHTggRtx220/onXrJgUbT/VTIW5n60luPaSjgBnAcCBSSntmPZYkSbWJt7NJkiQAZn8Cg7tV7nfZrqABEkDLlo255ZZDee65Sfzxj3tRWlrY8VQ/FeJP1bvA3sDBKaVdU0r/BMoLMI4kSZIkSbXLxMeXDpAAdvxtQYb67LM53Htv5ZPdDjywJ3/+8z4GSCqYQvzJOgz4FHgqIq6LiL0Bv5qVJNV5EYV5SZKkNUT5Irhz38r9jY+Esyqg686ZD/Xf/05ju+2uY8CAO3jmmYmZ9y9VJ/MQKaV0T0rpSGATYBTwS6BzRFwdEftlPZ4kSbVFRBTkJUmS1gDv3wWXN67c3/c6OHBoQb4RGjbsbXbd9UamTJlF795dXTxbNaZgc9xSSnNTSrellA4CugFvAOcVajxJkiRJkmpUqoCxN8PzF8H9h1e2d90NfnBy5gFSeXkF55//OEcddRfz55dx4olb8eSTx9GlS4tMx5G+S+YLa1cnpTQTuDb/kiSpTnLSkCRJ9cjiuXBvP5j0xNLthz8O62T/XKlZsxZy9NF38cADH1BaGvz97/vz85/3dtayalSNhEiSJEmSJNUZcz+Da7os3db7PNiwH6y9Y0GG/OKLubzwwmTatm3CHXcMYO+9NyjIONL3MUSSJCkjfhMoSVI9cec+ldsdNofDH4PmXb77/AxsuGE77r13IF27tmTDDdsVdCzpuxgiSZKUETMkSZLqgVQB09/ObW9wIPT/T2GGSYnLL3+J0tISzjhjBwB2333dgowlrShDJEmSJEmSlidVwKt/g2erPC9q76sLMtSCBWWccsp/uPnmNyktDQ46qCcbbNC2IGNJK8MQSZKkjHg7myRJddj4+5YOkNbaCVp1z3yYadNm07//cF5++ROaNWvIkCH9DJBUaxgiSZIkSZK0PE+cWrl9yN3Qo1/mQ7zyyif07z+cqVNns846rRk5ciBbbVXYtZaklWGIJElSRpyIJElSHTZ3Wu7X/W+Cjfpn3v3997/HgAF3sHBhObvttg533nkEnTo1z3wcaXUYIkmSJEmS9H1mTazcXr9vQYb4wQ8607JlY37yk0254ooDaNSotCDjSKvDEEmSpIy4JpIkSXXMgq+gfOHSIVLz7G4vmzNnEc2bNyQiWHfdNrz11imstVbLzPqXsmaIJElSRsyQJEmqI2a8C/cfBjPeWbq93aaZDfHuu9Pp128YJ5ywFeedtyuAAZJqvZJiFyBJkiRJUq1RtgCGbLp0gNSsEzTrDJsdm8kQDz74ATvscD3vvz+D4cPHsmhReSb9SoXmTCRJkjLi7WySJK3ByhdDRRlc0ayybftzYccLoFE2M4RSSvz1r89z/vlPkBIcdtimDBlyqOsfaY1hiCRJkiRJqn9SgvH3wJypMOFBmPDQ0sd7nQC7/yWz4ebPX8zJJ9/P7bePAeB3v+vDhRfuTkmJX0JpzWGIJElSRpyIJEnSGmLqSzB0p+qPlTaGNj2g742ZDnnGGQ9x++1jaN68Ibfe2p/+/bNbX0mqKYZIkiRlxNvZJElaA3z66rcDpK1Og0atYNtfQrOOBRn2f/+3D+PGTefqqw9kiy06F2QMqdAMkSRJkiRJdducqXBtVyhpkFv3aIm9r4QtT4EozDOnHnvsQ/beewNKSoKuXVvx7LMn+KWT1mg+nU2SpIxEREFekiRpJc2ZCjPfh4eOh2G75wIkWDpAOvgO2OrUggRIZWUV/OIXD7Pffv/m979/+pt2/17Xms6ZSJIkSZKkuuPZX8Mrf67+2FanQZ+/Q0lpwWYfzZw5nyOPvJPHH/+Ihg1L6NatVUHGkYrBEEmSpIz45aIkSTVswZdwbTdo0IRdFpfB2AawYGbl8TY9oFln2O1P0LgNdNiioH9hjx37Of36DePDD7+kU6fm3HXXEey66zoFG0+qaYZIkiRlxCnqkiTVkLE3w/t3wEcP5PbL5tEQoLzKOT+dBK2611hJ9933Hsccczdz5ixim23W4p57jmSddVrX2PhSTTBEkiRJkiStOVKCh3+ydNu2Z/Fc2e7susuuuf1GLaG0UY2VVFGR+NvfXmDOnEUMHLg5N9xwCM2aNayx8aWaYogkSVJGnIgkSVKBLZ4Pjw2q3P/hbdC+F3TakrJRo6Bp+6KUVVIS3HnnAIYPH8vpp/d2drLqLJ/OJkmSJEmq/cbcAFc0g3H/rmzb9GjotGVRypk06Wt+9atHKS+vAKBz5xacccYOBkiq05yJJElSRvzQKElSgcyfAY+eXLnfal044qmilfPcc5P40Y+G88UX8+jUqTnnnLNL0WqRapIhkiRJGTFDkiSpABbPh6s6VO4PfA66Fi+0ue661zjttAdZvLiCfffdgJ/+dJui1SLVNEMkSZIkSVLt8fFj8MFdlfufPFu5ve1ZRQuQFi8u55e/fIQrr3wVgF/+ckf++td9adDAVWJUfxgiSZKUkRKnIkmStHrGDYUHj67+WOdtoc+lNVtP3tdfL6B//+E89dTHNGpUyuDBB3H88VsVpRapmAyRJElaw0XEjcBBwOcppc3zbe2A4cB6wMfAESmlL/PHzgdOAsqBM1JKj+TbtwWGAE2BB4EzU0qpJn8WSVI99+hJldt7XgGlDXPbUQobHFicmoBmzXJ1dOnSgnvuOZIdd+xWtFqkYjJEkiQpI0WciDQE+BdwS5W284AnUkp/iYjz8vvnRsRmwECgF7A28HhE9EwplQNXA4OAl8iFSH2Bh2rsp5Ak1V9ffQRvXAll83P7h94PGx5U3JqA8vIKSktLaNiwlBEjBrBwYRldu7YqdllS0XjzpiRJa7iU0jPAzGWa+wE357dvBg6t0j4spbQwpTQBGA/0joi1gFYppRfzs49uqXKNJEmF9dIf4bXLKvfX2694tQAVFYn//d9RHHDAbZSVVQDQoUMzAyTVe85EkiQpI1G71kTqnFKaBpBSmhYRnfLtXcnNNFpiSr5tcX572XZJkgpv9uTcr5scDb3PhdJGRStlzpxFHH/8vdx99zhKSoJnnpnIXnutX7R6pNrEEEmSpIyUFChDiohB5G4zW2JwSmnwqnZXTVv6nnZJkgpnwVcwfHeYPia3v+Eh0PEHRStnwoQv6ddvGGPGfE7r1o0ZNuxwAySpCkMkSZJquXxgtLKh0WcRsVZ+FtJawOf59ilA9yrndQOm5tu7VdMuSVL2yhfB1BdhRJ+l27vuWpRyAEaN+pjDDx/BjBnz2Xjj9owcOZCNN+5QtHqk2sg1kSRJykhEFOS1iu4Djs9vHw+MrNI+MCIaR8T6wEbAK/lb32ZHxI6RG/S4KtdIkrT6UgUsngtvD4HLGy8dILVaF86YAy2Lcyf1Cy9MZt99b2XGjPkccEAPXnrpZAMkqRrORJIkaQ0XEUOBPkCHiJgCXAT8BRgREScBk4ABACmlsRExAngHKANOyz+ZDeBn5J701pTcU9l8MpskKRtlC+AfTb/d3qAJ9BwAfYdAFG+Oww47dGXvvddnyy0786c/7U1pqfMtpOoYIkmSlJFiraudUjrqOw7t/R3nXwxcXE37aGDzDEvTSoiI5imlucWuQ5IK4pGTKrcbNIOyeTDgCei+Z9H+Av3887lEQMeOzSktLeH++4+iYcPSotQirSmMVyVJykgU6H+q2yJi54h4BxiX398yIq4qclmSlK2FX+V+bdIezpwLZydYZ6+iBUivvz6N7bYbzGGHjWDRotyEXAMkafkMkSRJkorr78D+wAyAlNKbwO5FrUiSsnTjJjDhwdz2/jcUtxZg+PC32WWXG5k8eRaLF1cwe/bCYpckrTG8nU2SpIyUOGlIqyilNHmZRdTLv+tcSVqjjBsKX76X227YHDptXbRSKioSv/3tU1x88bMA/OQnW3HNNQfSuLH/LJZWlP+1SJIkFdfkiNgZSBHRCDiD/K1tkrRG++x1ePDoyv3TZxft9rVZsxZy7LF3c//971NSElx66X6ceeYOq/MUVKleMkSSJCkjfhDVKjoF+AfQFZgCPAqcWtSKJGl1pQr49zaV+3tfVbwnUAA33fQ699//Pm3bNmHEiAHss88GRatFWpMZIkmSlBEzJK2ijVNKx1RtiIhdgOeLVI8krb6qT2Pb7S/Q6/ji1QKcfvoOTJz4Naeeuj09erQrai3SmsyFtSVJkorrnyvYJkm137ih8M/WMHZIbr/DFtD7XGjYrEbLSClxzTWjmTp1NgAlJcFll+1vgCStJmciSZKUkRKnImklRMROwM5Ax4g4q8qhVoDPmZa05vniraXXQAI44qkaL2PhwjJOOeUBhgx5g5tvfpPnnjuB0lLnT0hZ8L8kSZKk4mgEtCD3pV7LKq9ZwOEr0kFE9I2I9yJifESc9x3n9ImINyJibEQ8nVHtkvRtt2xZub3nFXDmAmjavkZLmDZtNn363MyQIW/QtGkDfvGLHQyQpAw5E0mSpIw4EUkrI6X0NPB0RAxJKU1c2esjohS4EtiX3ILcr0bEfSmld6qc0wa4CuibUpoUEZ0yKl+SlnZb78rtPS+HbU6v8RLefXcWxx57HZ98Mpvu3VsxcuRAtt56rRqvQ6rLDJEkSZKKa15E/A3oBTRZ0phS2ms51/UGxqeUPgKIiGFAP+CdKuccDdydUpqU7/PzLAuXJABmTYZPX81tN24D25xZ4yXcfvsYzjzzTRYtqmDXXdfhzjsH0LlzixqvQ6rrDJEkScpIOBVJq+Y2YDhwEHAKcDzwxQpc1xWYXGV/CrDDMuf0BBpGxChyt8r9I6V0y7IdRcQgYBBAx44dGTVq1Mr9BCqoOXPm+J7UQr4vlXp9+Fs65rdH9boTivD78uSTE1m0qIKDDlqLM85Yl3HjRjNuXI2XoWr430rdYogkSVJGzJC0itqnlG6IiDOr3OK2ImsXVfcnLi2z3wDYFtgbaAq8GBEvpZTeX+qilAYDgwE23njj1KdPn5X9GVRAo0aNwvek9vF9yXv0p/DVs7ntXj+hz557F6WMPfZIbLTR3Zxzzo/8UqeW8b+VusUVxiRJkoprcf7XaRFxYERsDXRbgeumAN2r7HcDplZzzsMppbkppenAM8CWSNLqSgkeOwXGXF/ZtselNTb8++/PYI89hjBp0tdAbjbwDju0N0CSCswQSZKkjJREFOSlOu+PEdEaOBv4FXA98IsVuO5VYKOIWD8iGgEDgfuWOWcksFtENIiIZuRud/MGD0mr74u34K1rc9sNW8AZc6FpuxoZ+uGHx9O793U888xELrjgyRoZU1KOt7NJkiQVUUrpP/nNr4E9ASJilxW4riwifg48ApQCN6aUxkbEKfnj16SUxkXEw8BbQAVwfUrp7UL8HJLqiRF7wmf/hUWzKttOGg8NmxV86JQSl176Iuee+zgVFYn+/Tfh6qsPLPi4kiotN0SKiDOBm4DZ5L4Z2xo4L6X0aIFrkyRpjeKcIa2M+P/s3Xd4VGX6xvHvk0IJvYMUBUEQrCigYkEFVEBAuh1FsWD7qWtfu7vuWnbtigVxXaVFimJHEUUEpYhig5UuTXpNSPL8/piRJBAg4Jw5k+T+cM0157znzDz3ZSQhz7znPWbJQB8iC2S/7+7fm1kX4A4i6xcdvbf3cPd3gXd3Gnt+p/1HgEdilVtESrC182DxxPxjx98D5WoFXnrbtiwuv/xtXn99NgD33nsKf/3rKSQl6aevSDwVZibSpe7+hJmdAdQALiHSVFITSUREJA+twyD76GUiaxpNA540s4XA8UQ+rBsTajIRkYKM65G7PWgtJKdCarnAy2Zl5XDaaUOZMmUJ5cql8tpr59Cjx6GB1xWRXRWmifTHv4g7AUPc/VvTv5JFRERE/qxjgSPcPcfMygC/A43dfXnIuUREIjI3QvpZsHFRZH/j4sjz0ddBmcpxi5GSkkSPHoeybNkmxo7txxFHBD/zSUQKVpgm0nQz+xBoCNxuZhWIXFMvIiIieWhGveyjTHfPAXD3bWb2ixpIIpIQtq6GyXfB3NGwZUX+Y2Wrw4kPxiXGypWbqVkzMtPpppuOZ+DAY6hYsXRcaotIwQrTRBoAHAX86u5bzKwakUvaRERERGT/NTOz2dFtAw6O7hvg7n5EeNFEpESb+TR8m2d5tTptoMuIyHbZGpBaNtDyWVk53HLLR7z22rdMm3Y5jRpVwczUQBJJALttIplZy52GGukqNhERkd3Tz0nZR1rQQ0QSy/Yt8HZvmB9drz8pFTq/AQ1OhzJV4hJh7dqt9O07io8++pWUlCSmT/+NRo3iU1tE9m5PM5Ee28MxB06LcRYREZEiTT0k2RfuvjDsDCIiO7jDkzstkt3zfWgQv1/7fvxxFV27DmPevDXUqJFGenofTjrpwLjVF5G9220Tyd1PjWcQERERERERCcman3K367eDLsMhrWbcyr/zzi+cd146GzdmcvTRtRkzph8NGlSKW30RKZy9rolkZmnAjUADdx9oZk2Apu7+TuDpREREihBdziYiIkVW1rbc7T6fxrX0kiUb6NlzBJmZ2fTp04IhQ7qRlpYa1wwiUjiFWVh7CDAdOCG6vwQYCaiJJCIiIhIDZlaWyAd2P4edRURKuBpHxb1kvXoVefzxjqxfn8Htt5+oD2VEElhhmkgHu3tfMzsXwN23mv5Wi4iI7CJJPx1lP5jZ2cCjQCmgoZkdBdzv7l3DTSYiJcq0h+NabvHi9cyfv46TT46seTRoUOu41heR/ZNUiHMyo5+OOYCZHQxkBJpKREREpOS4F2gNrANw91nAQSHmEZGSxh1+GRHZLlst8HKTJy/i2GNfpGvXN/nll9WB1xOR2CnMTKR7gPeB+mb2X6At0D/IUCIiIkWRJurKfspy9/X6/0dEQvNSw9zts14LttRLM7j66vFs355D+/aNqF49LdB6IhJbe20iuftHZjYDOA4w4Hp3/z3wZCIiIkWMWgCyn743s/OA5OgNTK4Dvgw5k4iUFFkZsGFhZLvqoVD+gEDKbN+ezY03fsDTT38NwA03tOGRRzqSklKYi2NEJFEUZiYSwCnAiUQuaUsFRgeWSERERKRkuRa4k8hyAW8AHwAPhppIRIo/z4HPbob183PH+s8JpNTq1Vvo3Xskn366gFKlknn++c5ccsnRgdQSkWDttYlkZs8CjYE3o0NXmFl7dx8UaDIREZEiJkmXI8n+aerudxJpJImIBG/7VvjP0bA2zw0hKzWCgH6O/fLLar74YhG1apVj9Oi+HH98/UDqiEjwCjMT6RTgMHf/Y2HtocB3gaYSERERKTkeN7M6wEhgmLsHMxVAREq2Tctg5Uz45FpY/2v+Y11GQJ02gZU+/vj6DB/ei1at6lKvXsXA6ohI8ArTRPoZaABEL5SlPjA7sEQiIiJFlCYiyf5w91PNrDbQBxhsZhWB4e6uS9pE5M/L2BC5bO27F3c9Vr4uXDQbylaNacmcHOfBBydx5JG16NatGQDnnHNoTGuISDh220Qys7eJIlKwEAAAIABJREFUrIFUCfjRzKZF99ugxR5FRER2obtryf5y9+XAk2b2KXALcDdaF0lE/qyM9fB2H1j4Ye5YrWOg2bnQpAdUarj71+6nTZsy6d9/DOnpP1KpUmnmz7+eKlXKxryOiIRjTzORHo1bChEREZESyswOBfoCvYDVwDDgplBDiUjRtnEpbFkOrx+bO5ZSFi75CSo2CKzsggXr6NZtGLNnr6BixdK88UZPNZBEipndNpHc/bN4BhERESnqNBFJ9tMQIjcw6ejuv4UdRkSKuIUTYFT7/GPVD4eu6YE2kD77bAG9eo3k99+3cMgh1Rg7th/NmlUPrJ6IhKMwd2c7DngKOBQoBSQDm91dK6KJiIiI/EnuflzYGUSkCFv5LYzpChlrI/uZG3OPVWkCzc6HE+4JNMJrr33LgAHjyMrK4cwzG/Pmmz2pXLlMoDVFJByFWVj7aaAfkTuGHAtcBDQJMpSIiEhRlKSpSLIPzGyEu/cxs++IrDu54xDg7n5ESNFEJNHlZMOEQZG7rC38aNfjlgxnvAItLopLnGbNqpOcbNxww/E8/HB7kpOT4lJXROKvME0k3H2emSW7ezYwxMy0sLaIiMhO1EOSfXR99LlLqClEpOhZ/CnMfiH/WNfR0OC0yHZSCqSmBRph69btlC2bCkDr1nX56adrOOigyoHWFJHwFaZFvMXMSgGzzOyfZvZ/QLmAc4mIiIgUa+6+LLp5tbsvzPsArg4zm4gkMHeYfHfufs8P4MKZ0KQ7lK4YeQTcQJo1azmHHvoMI0fO2TGmBpJIyVCYJtKF0fOuATYD9YEeQYYSEREpiswskIcUex0KGDsr7ilEJLH9+AaM6wWPJ8GyKZGxFhfDQR2h5lFxizFy5Bzatn2FhQvX8/zz03H3vb9IRIqNvV7OFv00DGAbcB+AmQ0ncitaEQnZ7dc9FnYEkSLhhplPhx1BJB8zu4rIjKNGZjY7z6EKwORwUolIQlr9E7x7/q7jx98btwg5Oc4993zKgw9+DsDFFx/J88930YcdIiVModZEKsDxMU0hIiJSDGgZUdlHbwDvAX8HbsszvtHd14QTSUQSUnrH3O0zXoG0WnBge0guFZfyGzdmcOGFoxk79meSkoxHH+3ADTccpwaSSAm0v00kERERSSDRNQsvI3KXr++AS4A0YDhwELAA6OPua6Pn3w4MALKB69z9g/inLvHc3ReY2aCdD5hZVTWSRAR3+HU8bFwc2W/7ABx2SdxjnHtuOuPHz6Vy5TIMH96Ljh0PjnsGEUkMu20imVnL3R0CUoOJIyIiUnSF9YmsmdUFrgOau/tWMxsB9AOaAxPc/WEzu43IbJdbzax59HgL4ADgYzM7JHoXVomfN4jcmW06keZf3v+BHGgURigRSRDukH4GLPwod6zNnaFEefDB01ixYjNvvNGDJk2qhZJBRBLDnmYi7WmhlZ9iHURERKSoSwp3Vn8KUNbMthOZgfQbcDvQLnp8KDARuBXoBgxz9wxgvpnNA1oDU+KcuURz9y7R54ZhZxGRBOE5sOYXmPUM/DYZVs7MPXbRbIjThxXuzuTJiznxxAYAHHVUbaZNu0yXr4nI7ptI7n5qPIOIiIjI/nH3pWb2KLAI2Ap86O4fmlmtP24j7+7LzKxm9CV1ga/yvMWS6JiEwMzaArPcfbOZXQC0BP7t7otCjiYi8bJ4IswbAzOe2PVY5cYwYG7comRkZHH11eN55ZVZvPZady688EggvNm2IpJYtCaSiIhIjAQ1E8nMBgID8wwNdvfBeY5XITK7qCGwDhgZbUbs9i0LGNM9msPzHHCkmR0J3AK8DPwHOCXUVCISvOxMeK4mZKzf9ViNI6H9c1CtRdziLF++iZ49R/Dll4spWzaFUqWS41ZbRIoGNZFEREQSXLRhNHgPp7QH5rv7KgAzews4AVhhZnWis5DqACuj5y8B6ud5fT0il79JOLLc3c2sG/CEu79sZheHHUpEArJ1DWzfFNl+8cD8x076B1RqCIf0itula3+YPv03uncfzpIlG6hXryJjx/ajZcs6cc0gIolPTSQREZEYCXGq/yLgODNLI3I52+nAN8Bm4GLg4ejz2Oj544A3zOxxIgtrNwGmxTu07LAxere8C4GTzCwZ3cREpHia8SR8ev2u47WOgfOmQlI4M3/efPM7Lr10HNu2ZdG2bX3S0/tQq1b5ULKISGLbaxPJIv8iPh9o5O73m1kDoLa76x+bIiIieYS1sLa7TzWzUcAMIAuYSWTmUnlghJkNINJo6h09f070Dm4/RM8fpDuzhaovcB5wqbsvj/5b65GQM4lILGVnwuvHwO/f545VqB+5A1ulhtDn09AaSNu2ZXH33RPZti2LAQOO5plnOlG6tOYaiEjBCvPd4VkgBzgNuB/YCKQDrQLMJSIiIvvA3e8B7tlpOIPIrKSCzn8IeCjoXLJ30cbRf4FWZtYFmObur4WdS0RiJGMDDD0MNi7OHbv4O6h+WHiZ8ihTJoUxY/oyceICrr66lRbQFpE9SirEOW3cfRCwDcDd1wKlAk0lIiJSBJkF85Dizcz6ELmcsDfQB5hqZr3CTSUiMTHxZni6Um4DqeKBcN3m0BtIc+eu5h//+GLHfosWNRk0qLUaSCKyV4WZibQ9em2+A5hZDSIzk0RERETkz7sTaOXuK2HHv7U+BkaFmkpE/rzvX448p5aDMlVhwP9Cu2ztDx9++D/69h3FunXbOOigyvTtmxgzokSkaChME+lJYDRQ08weAnoBdwWaSkREpAhK0ie4sn+S/mggRa2mcLPFRSRRrf4BRneBjHWR/d4ToE6bUCO5O//611f85S8fkZPjdO/ejE6dmoSaSUSKnr02kdz9v2Y2nciaCgZ0d/cfA08mIiJSxOi3ftlP75vZB8Cb0f2+wLsh5hGRP2PZNHhjp4ZRzZbhZInati2LK654h9de+xaAu+8+mXvuaUdSWHeEEJEiqzB3Z2sAbAHezjvm7ouCDCYiIiJSErj7X8ysB3AikQ/sBrv76JBjici+yNoG7/SDzb/B8q9zx4+7G1rdDMmpoUVbvnwT3bsPY+rUpaSlpTJ0aHd69WoeWh4RKdoKcznbeCLrIRlQBmgI/Ay0CDCXiIhIkaOr2WRfmFkT4FHgYOA74GZ3XxpuKhEprJprPoFPov3emU/uekLvT6DBqfENVYCyZVNYt24bBx5YibFj+3HkkbXDjiQiRVhhLmc7PO++mbUErggskYiIiEjJ8ArwGjAJOBt4CugRaiIRKZztW2g+/4Fdxw/sAG0fhEoNIa1G/HPlkZPjJCUZlSqV4d13z6dChVLUqFEu1EwiUvQVZiZSPu4+w8xaBRFGRESkKNPC2rKPKrj7i9Htn81sRqhpRKTwptyXu33qvyPPtVpB3RPCyZNHVlYOt932MRs2ZPDCC10wMxo1qhJ2LBEpJgqzJtKNeXaTgJbAqsASiYiIiJQMZczsaCJLBgCUzbvv7moqiSSan4bB6jnw9T9zx1peH16enaxdu5Vzz03ngw/+R0pKEtdf34YWLWqGHUtEipHCzESqkGc7i8gaSenBxBERESm6NBFJ9tEy4PE8+8vz7DtwWtwTiUjBsrbBiNNg2ZT8493fLvj8EPz00+907fomc+euoXr1NNLT+6iBJCIxt8cmkpklA+Xd/S9xyiMiIlJk6U7Jsi/cPfwVd0WkcCb+X/4G0gn38/OSdTRteGZ4mfJ49925nHtuOhs2ZHDkkbUYO7YfBx5YOexYIlIM7baJZGYp7p4VXUhbRERERESk5PEc+Pb5yHZKGbhmAySnsmziRJom7fMSszE3ZsxP9OgxHHfo3bs5Q4Z0o1y5UmHHEpFiak/f9aYRWf9olpmNA0YCm/846O5vBZxNRESkSNHC2iIixcz2LTC2e+7+gP9Bcmp4eQrQvn0jjjiiFr16NefOO0/C9LNIRAJUmNZ5VWA1kevynchijw6oiSQiIiIiIsXL1jXw9T9gywqYMzT/sfIHhJNpJ0uXbqBatTTKlEmhfPlSTJ16GaVLhz8rSkSKvz19p6kZvTPb9+Q2j/7ggaYSEREpgvThr+wPi0wbOB9o5O73m1kDoLa7Tws5mkjJs+gTGHn6ruPVmkPvCfHPU4Avv1xMjx7D6djxYIYO7Y6ZqYEkInGzp+82yUB58jeP/qAmkoiIyE60sLbsp2eBHCKzvu8HNhK5E26rMEOJlDjrF+RvIJ38TyhbA6o2gwOOCy1WXq+8MpMrr3yH7dtzWLp0I1u3ZpGWlliX14lI8banJtIyd78/bklERERESqY27t7SzGYCuPtaM9OquCLx9lLD3O0zhsBh/UOLsrOsrBxuuukDnnwyMkHx2mtb89hjHUlNTQ45mYiUNHtqIunzVBERkX1g+tEp+2e7mSUTneltZjWIzEwSkXgYejj8/n3ufqtbE6qBtHr1Fvr2HcWECfNJTU3iuec6M2CAbqAtIuHYUxOpgIuBRURERCTGngRGE1mP8iGgF3BXuJFESoiV3+ZvIFVoACc/HF6eAvztb58zYcJ8atYsx1tv9aFt2wZhRxKREmy3TSR3XxPPICIiIkWd1kSS/eHu/zWz6UQ+wDOgu7v/GHIskeJv6xr4z1G5+9dthtS08PLsxgMPnMbatdu477521K9fKew4IlLCaRl/ERGRGFETSfZH9G5sW4C38465+6LwUokUQznZsPRz2L4ZZg+G/43LPXZwt4RpILk7gwdP54ILjqBcuVKkpaXyyivdwo4lIgKoiSQiIiIStvFE1kMyoAzQEPgZaBFmKJFiZ+aTMPHGXccPOhM6vxH/PAXYvDmTSy4Zy8iRPzBx4kLefLNn2JFERPJRE0lERCRGzDQVSfadux+ed9/MWgJXhBRHpHjaviV/A6lhJ0gtD6c/A2nVw8uVx8KF6+jWbRjffruCChVKcf75h+/9RSIicaYmkoiIiEgCcfcZZtYq7BwixcaGRfD6Mbn750+D2on1V2zSpIX06jWCVau20LhxVcaN68ehh9YIO5aIyC7URBIREYkRrYkk+8PM8l5fkwS0BFaFFEek+PnmMdj6e2T74G4J10B64YVvuOaa98jKyqFjx4MZNqwnVaqUDTuWiEiBksIOICIiIlLCVcjzKE1kjSStoisSK6vnRJ6rNYeu6eFm2Ym789VXS8nKyuHGG49j/Pjz1EASkYSmmUgiIiIxoiWRZF+ZWTJQ3t3/EnYWkWJr0YTIc+vbISk53Cw7MTOee64z55zTjK5dm4YdR0RkrzQTSUREJEaSzAJ5SPFkZinunk3k8jURCcLXj+Zu1zgivBx5fPvtcjp3foONGzMAKFMmRQ0kESky1EQSERERCce06PMsMxtnZheaWY8/HqEmEykO0s+ESXkm+SVAE2nUqB844YRXePfdufztb5+HHUdEZJ/pcjYREZEY0cLasp+qAquB0wAHLPr8VpihRIq036bAgg9y969YGl4WICfHue++idx//yQALrzwCO65p12omURE9oeaSCIiIiLhqBm9M9v35DaP/uDhRBIpJhZ+nLt9YzZYeBdgbNyYwcUXj2H06J9ISjL++c/23Hjj8ZguVxaRIkhNJBERkRjR7wOyj5KB8uRvHv1BTSSRfbX6R1j3v8j2l3dHno++NtQG0oYNGbRt+wrff7+SSpVKM3x4L844o3FoeURE/iw1kURERGIkqcBegMhuLXP3+8MOIVIsLPggsgbSztJqxj9LHhUrlqZt2/ps357NuHHncsgh1ULNIyLyZ6mJJCIiIhIOdR1F/qzs7TCkKayfnzvWqHPkuVwdaH173CO5O+vWbaNKlbIAPPnkWWzdup1KlcrEPYuISKypiSQiIhIjupxN9tHpYQcQKdI8B4afkr+B1P1tOLhLaJEyM7MZNGg8kyYtYurUy6hcuQylSiVTqlRyaJlERGJJTSQRERGRELj7mrAziBQ5ngM5WfDNY/DFHfmPhbyA9ooVm+jZcwSTJy+mTJkUpk//jdNPbxRaHhGRIKiJJCIiEiNJmokkIhJ7WRkwNx1Wz4Gpfyv4nKtXh9pAmjFjGd27D2Px4g3UrVuBMWP6ceyxB4SWR0QkKGoiiYiIxEiSrmeTODOzM4EniNzp7SV3f3g357UCvgL6uvuoOEYU+fPSz4Aln+Ufs2RIqwEdXoSGZ0JSeL/WDB/+PZdcMpatW7M4/vh6vPVWX2rXLh9aHhGRIKmJJCIiIlIEmVky8AzQAVgCfG1m49z9hwLO+wfwQfxTivxJ29bmbyAddQ0ccTnUOCK8THnMmLGMfv3SAbj00qN49tnOlC6tX7FEpPjSdzgREZEY0UQkibPWwDx3/xXAzIYB3YAfdjrvWiAdaBXfeCJ/kjs8Xyd3//qtkJJYdzhr2bION910PA0aVOLaa1tj+kEgIsWcmkgiIiIiRVNdYHGe/SVAm7wnmFld4BzgNNREkkS2eTls35x/7N0LITsjsn1A24RpIM2du5rMzOwd+48+2jHENCIi8aUmkoiISIxoTSSJs4L+h/Od9v8N3Oru2XuaIWFmA4GBADVq1GDixImxyigxsGnTpmL9NWn+6/3UXPvpHs+ZVOtuchLgv8E336zhvvt+pEKFFB55pGmx/roURcX970pRpa9L8aImkoiIiEjRtASon2e/HvDbTuccCwyLNpCqA53MLMvdx+Q9yd0HA4MBmjZt6u3atQsqs+yHiRMnUiy/JitmwCfXwtovc8cqNcp/Tmo5uGA6JyenxjfbTtydf//7K2699XtycpzTTz+YKlXKF8+vSxFWbP+uFHH6uhQvaiKJiIjEiCYiSZx9DTQxs4bAUqAfcF7eE9y94R/bZvYq8M7ODSSRuFvzM4zuAuvm5R+/Zh2UrhROpj3IyMjiyivH8+qrswC4666TuO++U5k06bO9vFJEpPhRE0lERCRGkkKsbWaVgZeAw4hc0nQp8DMwHDgIWAD0cfe10fNvBwYA2cB17q47dxUx7p5lZtcQuetaMvCKu88xsyujx58PNaBIQbK2wZBm+ceanQdnvAIppcPJtAfLlm2kR48RfPXVEtLSUnn11W707t0i7FgiIqFRE0lERKR4eAJ43917mVkpIA24A5jg7g+b2W3AbcCtZtacyKyVFsABwMdmdoi7Z+/uzSUxufu7wLs7jRXYPHL3/vHIJLJHm5flbh93N7T6C5QqH16evfjii0V89dUSGjSoxNix/TjqqNphRxIRCZWaSCIiIjES1q2dzawicDLQH8DdM4FMM+sGtIueNhSYCNxK5Dbww9w9A5hvZvOI3C5+SlyDi0jJ4g4znohsVzwI2t4XapzC6N27BS++mEHXrk2pWbNc2HFEREIX5sx7ERERiY1GwCpgiJnNNLOXzKwcUMvdlwFEn2tGzy/o1vB14xlYREoYd/hXSm4TKa1GuHl2Izs7hzvumMCMGbkzpi67rKUaSCIiUWoiiYiIxIgF9TAbaGbf5HkM3Kl0CtASeM7djwY2E7l0bU9Rd7bzreFFRGJn2VfgOZHt1PJwdnq4eQqwbt02unR5k7///Qt69RpBZqau8BUR2ZkuZxMREYmRpIAuZ8t7+/XdWAIscfep0f1RRJpIK8ysjrsvM7M6wMo85+/t1vAiIrGzYnru9nUbw8uxGz///Dtduw7jl19WU61aWV55pRulSiWHHUtEJOFoJpKIiEgR5+7LgcVm1jQ6dDrwAzAOuDg6djEwNro9DuhnZqWjt4dvAkyLY2QRKSm2rIL3+8Mn10b2ax0bapyCvPfeXFq3folfflnN4YfX5OuvL6ddu4PCjiUikpA0E0lERCRGwllWe4drgf9G78z2K3AJkQ+LRpjZAGAR0Bsgehv4EUQaTVnAIN2ZTURibs7QSAMpr7YPhBJld558cio33PA+7tCjx6EMHdqd8uVLhR1LRCRhqYkkIiJSDLj7LKCgj/hP3835DwEPBRpKREqmZdNgRDvI2po7dtAZ0PFlqJBYa/jXq1cRgHvvPYW//vUUkpJC/jhARCTBqYkkIiISIwEtiSQiUnRM/xdMvDH/2HlToU7rcPIUIDMze8d6Rz16HMoPPwyiWbPqIacSESkatCaSiIhIjJhZIA8RkSIhJzt/A+nUJ+D6bQnVQJoyZTGHHPIUX321ZMeYGkgiIoWnJpKIiIiIiPw5m36Df+W5yOHSX6DldZBSOrxMOxkyZCbt2g1l4cL1PPnk1L2/QEREdqHL2URERGJEn8yISIk15b7c7UZnQ5Um4WXZSVZWDjff/CFPPBFpHF1zTSsef/yMkFOJiBRNaiKJiIiIiMj+274ZZg+ObDe/CM4aGm6ePNas2UrfvqP4+ONfSU1N4plnOnH55ceEHUtEpMhSE0lERCRGtH6RiJQoWRkwpClsWJg7duh54eXZSU6O06HDf5gxYxk1aqTx1lt9OfHEBmHHEhEp0jTzXkRERERE9t3TFfM3kBqeBQd2CC/PTpKSjPvvb8cxx9Thm28GqoEkIhIDmokkIiISI5qHJCIlxpLPITszsl25cWQh7QSYjenuzJy5nJYt6wDQufMhnHlmY5KT9dm5iEgs6LupiIhIjJhZIA8RkYSzcUnudoI0kDZvzqRfv3TatHmJSZNyZ0ipgSQiEjuaiSQiIiIiIvunab+EaCAtWrSebt2GMWvWcipUKMWmTZlhRxIRKZbURBIREYkRfdYtIiXGwg/DTrDD558vpGfPEaxatYWDD67CuHHn0rx5jbBjiYgUS/r3roiIiIiIFN5nt8CcVyPbqeVCjTJ48HROP/01Vq3aQvv2jZg27XI1kEREAqSZSCIiIjGi9YtEpET46b+5223vDy3GqlWbue22j9m+PYcbbmjDI490JCVFn5GLiARJTSQREZEYUQtJRIq9eWNh02+R7csXQvkDQotSo0Y5hg/vxdKlG+nf/6jQcoiIlCRqIomIiIiIyJ4t+AjSO+YZMEgtH/cY3323gunTl+1oGnXocHDcM4iIlGRqIomIiMSIrmYTkWJp+Tc7NZCALsOhbNW4xhg9+kcuvHA027Zl0aRJVdq2bRDX+iIioiaSiIiIiIgUZMMi+GggLPggd6zTG9CsL1j81h7KyXEeeOAz7r33MwAuuOAIWrasE7f6IiKSS00kERGRGEnSqkgiUlxkbIAXD8w/1v45OPTcuMbYtCmT/v3HkJ7+I0lJxj/+0Z6bbjpeNzIQEQmJmkgiIiIxot9pRKRYcIeX86w11OJiOOUxKFstrjEWLFhHt27DmD17BZUqlWbYsF6ceWbjuGYQEZH81EQSEREREZGIDYvhh6Gw9ffIfqtb4eSHQ4vz228badq0GmPH9qNp0+qh5RARkQg1kURERGLEdDmbiBRlc16D9y/OPxbnBpK7A2BmHHRQZT788AIaNqxC5cpl4ppDREQKpiaSiIiIiEhJ5Tkw8xlYNw9mPpk73qQnHHdXXKNkZmZz7bXvcvDBVbnllrYAHH20FtAWEUkkaiKJiIjEiNZEEpEiZ/nX8Ol1+ccG/A8qN4prjJUrN9Oz5wi++GIRaWmpXHzxkdSqVT6uGUREZO/URBIREYkR3Z1NRIqUrAwY1yuyXakRHHEFHNQx7g2kmTOX0b37cBYtWk/duhUYM6afGkgiIglKTSQRERERkZImKwOeyLPO0AHHQ+tb4h5jxIg59O8/hq1bszjuuHq89VYf6tSpEPccIiJSOGoiiYiIxIguZxORImH9AnipYe5+g/bQ/vm4xxg8eDpXXPEOAJdcchTPPdeZ0qX164mISCLTd2kRERERkeJu8wr44k7Yvhl+HpY73qgznPNOKJE6dWpCvXoVufnm47nuujaYOvEiIglPTSQREZEY0e8/IpJwtq6BX0bAx1fteuzwy6FDfGcgLV26gTp1KpCUZNSrV5GffhpEuXKl4ppBRET2X1LYAUREREREJCBf3pO/gdT8Quj0BvSeAB1eAIvfrwMff/wrhx/+HA8+OGnHmBpIIiJFi2YiiYiIxIjp7mwikkg2LoVZT0e2k0tBtzHQ8Ky4x3B3nnxyKjfe+CE5Oc706cvIyXGSkvQ9U0SkqFETSUREJEb0+5CIJIzMjTC4Xu7+Rd9B1UPiHiMjI4urrhrPkCGzALjjjhN54IHT1EASESmi1EQSERERESluvrwnd/ukf4TSQFq+fBM9egxnypQllC2bwpAh3ejb97C45xARkdhRE0lERCRGdDmbiCSESbfB9H9Ftmu2hNa3hBLj2mvfY8qUJdSvX5ExY/rRsmWdUHKIiEjsqIkkIiIiIlJceA58/Y/c/fbPhhblqafOIinJePLJM6lVq3xoOUREJHZ0dzYREZEYMQvmISJSaJ6Tu33VSqjTJm6ls7NzGDJkJtnZkQy1a5dn+PBeaiCJiBQjmokkIiISI7qcTURCl35m7nZajbiVXbduG+edl857781j3rw1PPTQ6XGrLSIi8aMmkoiIiIhIcbDkc1g0IbLdpGfcyv7yy2q6dn2Tn39eTbVqZWnfvlHcaouISHypiSQiIhIjumO1iITm1/EwukvufpdhcSn7/vvz6NdvFOvXZ3D44TUZO7YfDRtWiUttERGJP62JJCIiIiJSlE39e/4GUseXICnYz4rdnUcf/ZLOnd9g/foMzjmnGV9+OUANJBGRYk4zkURERGJEayKJSNxlboJvHs3dP3skNO4eeNnsbGf8+Lnk5Dj33HMKd999CkmajikiUuxpJpIUyuTPJ9G18xl0ObMDL784OOw4InH3/D3ns3DC3/lm5B35xq/qdwrfjv4r00fdyUPXd8t3rH7tKqya/Bg3XJi7uOi9g85m7nsPsGryY3HJLfGlu7OJSNy9eQJsWxPZ7vk+HNIr8FlIACkpSYwc2ZuxY/tx773t1EASESkh1ESSvcrOzuZvD93Ps8+/xOhx43n/3Xf437x5YccSiav/vP0V3QY9k2/s5GOb0KXd4bTq83eO6fUQ/35tQr7j/7y5Jx9OnpNv7N1J33HShY8Enlfa2gOpAAAgAElEQVRERIqxld/C90Mij7U/R8YatIcD2gZadurUJVx00WiysnIAqF49ja5dmwZaU0REEktgH1OY2THuPn2nsbPd/e2gakowvv9uNvXrH0i9+vUBOLNTZyZ+OoGDGzcOOZlI/Eye8T8a1Kmab2xg75N4dMhHZG7PAmDV2k07jp3d7gjmL/mdzVsz871m2ncLAs8q4dHn8CISqG3r4If/wKfX5R+3JOg+DlLLBlZ66NBZDBz4DpmZ2bRpU5dBg1oHVktERBJXkDORXjSzw//YMbNzgbsCrCcBWbliBbXr1N6xX7NWLVasWBFiIpHE0PjAmrQ9+mAmvXYzH750Pcc0bwBAWplS3HRJBx564d2QE4qISHGQnL0JPrkOnqmSv4HUon/k0WFwYA2krKwcbrzxA/r3H0tmZjZXX30sAwceE0gtERFJfEFeMN0LGGVm5wMnAhcBHQOsJwFxfJcx0yIdIqQkJ1GlYhonX/Qox7Y4kNf/eSmHdrmXv17Vmade/2SXWUhS/CWF/L3RzJKBb4Cl7t7FzKoCw4GDgAVAH3dfGz33dmAAkA1c5+4fhBJaRPbMnZNmnZ1/rEID6P0xVGkSaOm1a7fSt+8oPvroV1JSknjmmU5qIImIlHCBNZHc/Vcz6weMARYDHd19655eY2YDgYEATz/7AgMuHxhUPNkHtWrVZvmy5Tv2V65YQc2aNUNMJJIYlq5Yx5gJ3wLwzZyF5OQ41auUp9VhB3JO+6N46IbuVKpQlpwcZ1vmdp4fPinkxFICXA/8CFSM7t8GTHD3h83stuj+rWbWHOgHtAAOAD42s0PcPTuM0CKyGytmwGc35e7Xbg2d/gtVgl9SYOnSDbRrN5R589ZQo0Ya6el9OOmkAwOvKyIiiS3mTSQz+w7yTV2pCiQDU80Mdz9id69198HAYIBtWQVMf5FQtDjscBYtWsCSJYupVbMW7787nr8/ojtLibw9cTbtWh/C59Pn0rhBTUqlpvD72k20H/DvHefceUUnNm/JUAOphAhzHpKZ1QM6Aw8BN0aHuwHtottDgYnArdHxYe6eAcw3s3lAa2BKHCOLyJ6s/BZezzPrp+KBcP7UuJWvVas8jRpVoXz5UowZ05cDD6wct9oiIpK4gpiJ1CWA95QQpaSkcPudd3PVwMvIycmm+zk9adw42OnTIolm6N/7c9IxTaheuTzz3n+AB55/l6FjpvDCvefzzcg7yNyezWV3/2ev7/PQ9d3oe9axpJVJZd77DzBk9BStnVSchHs127+BW4AKecZqufsyAHdfZmZ/TCOtC3yV57wl0TERCZs7rPgG/pu7cPXyqh2p3ff1OJR2Nm/eTvnypUhJSWL48F6kpiZRrlypwGuLiEjREPMmkrsvBDCz44A57r4xul8BaA4sjHVNCd5JJ5/CSSefEnYMkdBcfPurBY5fetdre3zdzg2iO58Yy51PjI1VLCkh8l7uHTU4Onv3j+NdgJXuPt3M2hXmLQsY0wxgkTDlZMHnt8PswZC5IXe87YP8suVYaqfVCLT8li3bGTBgHL/9tpGPPrqQUqWSqVy5TKA1RUSk6AlyYe3ngJZ59jcXMCYiIlJsWEBTkfJe7r0bbYGuZtYJKANUNLPXgRVmVic6C6kOsDJ6/hKgfp7X1wN+CyC6iOxJTjb8PAK2rIDfpsAvI/If7/gyHH4pORMnBhpj8eL1dO8+nBkzllG+fCnmzFnJ0UfXCbSmiIgUTUE2kczdd3yq6e45ZhZkPRERkRLJ3W8HbgeIzkS62d0vMLNHgIuBh6PPf0yDGwe8YWaPE1lYuwkwLd65RUq84afAb5N3He82Bg7sAKlpgUeYPHkRPXqMYOXKzRx8cBXGju1Hixa6gYqIiBQsyKbOr2Z2HZHZRwBXA78GWE9ERCRUFu6aSAV5GBhhZgOARUBvAHefY2YjgB+ALGCQ7swmEoKV03O3W14PlgzNL4KaR8al/EsvzeDqq8ezfXsO7ds3YvjwXlStWjYutUVEpGgKsol0JfAkcBeRdRYmkH89BxERkWIlEXpI7j6RyF3YcPfVwOm7Oe8hIndyE5GwWHLk+doNUKrCns+NsXfe+YXLL38bgOuvb8Ojj3YkJSUprhlERKToCayJ5O4rgX5Bvb+IiIiISPEQ/xZ0p05N6NWrOZ06NeaSS46Oe30RESmaAmsimVkZYADQgsginwC4+6VB1RQREQlVIkxFEhHZje+/X0m1amWpU6cCSUnGiBG9sAS8DldERBJXkHNW/wPUBs4APiNy55eNAdYTEREREZECjB37E8cf/zI9eowgIyMLQA0kERHZZ0E2kRq7+1+Bze4+FOgMHB5gPRERkVBZQH9EpBjJ2AAbl8L892H75sDLuTsPPPAZ3bsPZ9OmTBo1qkJOju/9hSIiIgUIcmHt7dHndWZ2GLAcOCjAeiIiIqHSh/oisls/vA5z02HemF2PJZcOpOTmzZn07z+WUaN+wAwefrg9f/nLCZqBJCIi+y3IJtJgM6tC5O5s44DywF8DrCciIiIiknhW/wTvXZh/rPwBkVlJXd+C5NSYl1ywYB3dug1j9uwVVKxYmjff7EmnTk1iXkdEREqWIJtIE9x9LTAJaARgZg0DrCciIhIqfbYvIgX6/bvIc7k6cNQgaHExVKgXaMmRI+cwe/YKDjmkGmPH9qNZs+qB1hMRkZIhyCZSOtByp7FRwDEB1hQRERERSSx/zELyHDjuzriUvPnmE3CHgQOPoXLlMnt/gYiISCHEvIlkZs2AFkAlM+uR51BFQD/BRESk+NJUJBHZmedAdkZk+9ibAiuTmZnNX//6Cddc05r69SthZtxyS9vA6omISMkUxEykpkAXoDJwdp7xjcDlAdQTEREREUlMk/MsCXr4ZYGUWLVqM716jWTSpIVMnryYzz+/RItni4hIIGLeRHL3scBYMzvZ3SflPWZm+jhERESKLdNUJBHJa/k3MPVvke0qTaBMlZiX+Pbb5XTrNoyFC9dzwAEVePzxM9RAEhGRwCQF+N7/LmDsqQDriYiIhMosmIeIFFGr5+Ru9/ww5m8/atQPnHDCKyxcuJ42bery9deX07p13ZjXERER+UMQayIdD5wA1DCzG/Mcqggkx7qeiIiIiEhCWvNz5Ln5RVDpoJi+9X33TeTeez8D4OKLj+T557tQpkyQ98wREREJZk2kUkD56HtXyDO+AegVQD0REZGEoElDIpLPsq8izxnrYv7WaWmpJCUZjz7agRtuOE6XsImISFwEsSbSZ8BnZvaquy+M9fuLiIiISISZnQk8QWS290vu/vBOx88Hbo3ubgKucvdv45uyhMjJhs3L4Nfx8MtIsGT4/fvIsUadY1IiOzuH5OTIahQ333wCHTsezJFH1o7Je4uIiBRGkHNet5jZI0ALoMwfg+5+WoA1RUREwqOJABJHZpYMPAN0AJYAX5vZOHf/Ic9p84FT3H2tmZ0FDAbaxD9tMTZ3DKyaBVPu2/05NY7602U++WQ+V101nvffP5+GDatgZmogiYhI3AXZRPovMBzoAlwJXAysCrCeiIhIqHR3Nomz1sA8d/8VwMyGAd2AHU0kd/8yz/lfAfXimrC427YWxp2Tf6xMlcispDNfhdQ0SKsNNY/c7xLuzltvLeXZZyeRne089dQ0Hn/8jD+XW0REZD8F2USq5u4vm9n1eS5x+yzAeiIiIiIlSV1gcZ79Jex5ltEA4L1AE5U0n9+Wu338PVC7VcwuXQPIyMhi0KB3efnleQDcfvuJPPDAqTF7fxERkX0VZBNpe/R5mZl1Bn5Dn36JiEgxpnVtJc4K+j/OCzzR7FQiTaQTd3N8IDAQoEaNGkycODFGEYuf5Owt1FrzEUk5GTReMhiAHEtlUmY7WAQsmhiTOmvWZHL33XOYM2cDpUoZt9zSjNNPT+bzzyfF5P3lz9u0aZP+riQYfU0Sk74uxUuQTaQHzawScBPwFFAR+L8A64mIiIiUJEuA+nn26xH50C4fMzsCeAk4y91XF/RG7j6YyHpJNG3a1Nu1axfzsEXKx1fB4okU2Kdb8+MuQ0k9xtPuoHYxK79163aaN3+WBQs2UK9eRe66qzFXXHF2zN5fYmPixImU+L8rCUZfk8Skr0vxElgTyd3fiW6uBzTvVkREij1NRJI4+xpoYmYNgaVAP+C8vCeYWQPgLeBCd/8l/hGLoA2L4Nvn937eIX2gQl0oXxcaxPa+MWXLpnLjjccxbNgc3nqrDz/++E1M319ERGR/BTkTSUREpGRRF0niyN2zzOwa4AMgGXjF3eeY2ZXR488DdwPVgGctcr1llrsfG1bmIuH7Ibnb/ecUfE7ZGpBWI6Zls7NzmDt3Dc2aVQfgmmtac+WVx5KamsyPu05+EhERCYWaSCIiIiJFlLu/C7y709jzebYvAy6Ld64ia+08mHJvZLtqM6jWPC5l16/fxvnnv8XkyYuZNu0ymjSphpmRmpocl/oiIiKFlRTUG0enVu91TEREpLiwgP6ISBys/hFeaZK7f8L9cSk7d+5qjjvuZcaPn0tSkrF8+aa41BUREdkfgTWRgPQCxkYFWE9EREREZN998xi8mmfWUfOLoGnvwMt+8ME8Wrd+iZ9++p0WLWrw9deXc9JJBwZeV0REZH/F/HI2M2sGtAAqmVmPPIcqAmViXU9ERCRRmCYNiRQ92Znw2c25+23ugFa3BlrS3fnXv77iL3/5iJwcp3v3Zrz2WncqVCgdaF0REZE/K4g1kZoCXYDKQN57kW4ELg+gnoiIiIjI/kk/M3f7iqVQ/oDAS/7yy2puu+1jcnKcu+8+mXvuaUdSkrrQIiKS+GLeRHL3scBYMzve3afE+v1FREQSlX4FFCmCFn8aeS5fNy4NJICmTavzwgtdqFChNL16xWfxbhERkVgI8u5si81sNNAWcOAL4Hp3XxJgTRERkfCoiyRSNGRnwvYtsGJ67livjwItOW3aUlav3sJZZ0UW777kkqMDrSciIhKEIJtIQ4A3gD9WJbwgOtYhwJoiIiIiIru34ENIP2PX8arNAiv5n/98y+WXv02pUslMnz6QJk2qBVZLREQkSEHena2muw9x96zo41WgRoD1REREQmUB/RGRGHqnb+526UpgSdD1rUBWxs/OzuHmmz/koovGkJGRzQUXHMFBB1WOeR0REZF4CXIm0iozuwB4M7p/LrA6wHoiIiIiIrvnDqnlIWMd9HgXGp4VWKm1a7dy7rnpfPDB/0hJSeKpp87iyiuPDayeiIhIPATZRLoUeBr4F5E1kb6MjomIiBRLAUxkEJFYWTYNRnWAzA2R/WotAiv100+/07Xrm8ydu4bq1dNIT+/DyScfGFg9ERGReAmsieTui4CuQb2/iIhIolEPSSRBbVkJb7TJ3a/ZMtA7sa1fv41Fi9Zz5JG1GDOmny5hExGRYiPmTSQzu3sPh93dH4h1TRERERGR3fr0htztdv+CY27Y/bkx0KZNPd5773xat65LuXKlAq0lIiIST0EsrL25gAfAAODWAOqJiIgkBgvoISL7Z/MKmPk0/BRdovOAttDy+piX2bp1Oxdc8BYjR87ZMXbqqQ3VQBIRkWIn5jOR3P2xP7bNrAJwPXAJMAx4bHevExERERGJmQ2L4MWd1iHqNjrmi5ctWbKB7t2HMX36Mj7++Fc6dz6EtLTUmNYQERFJFEHMRMLMqprZg8BsIo2qlu5+q7uvDKKeiIhIIrCA/ojIPlr4cf4GUoPT4dwvIa1GTMt8+eVijj12MNOnL6NRoyp8/PFFaiCJiEixFsSaSI8APYDBwOHuvinWNURERBKR7s4mkgCm/RM+z7OCwjE3QrvYT4Z/5ZWZXHXVeDIzsznttIaMGNGLatXSYl5HREQkkQRxd7abgAzgLuBOy/0XtRFZWLtiADVFREREpKTauho+uwlWfgurZuWOn/kqNL8o5uUeemgSd931KQDXXdeaxx47g5SUQCb4i4iIJJQg1kTST1ARESmRNBFJJARbVsJztXYdv3I5lCtgPAY6dWrCo49O4dFHOzBgQMtAaoiIiCSiIGYiiYiIiIgExx2GnxJpIK39OXe8cmM4+Z9Q/1QoUzmmJVeu3EzNmuUAOProOixYcD2VKpWJaQ0REZFEp1lDIiIisWIBPUQkv0+ug6Wf528gHdIHLvkRmpwT8wbS2LE/0bjxk7z++uwdY2ogiYhISaSZSCIiIiJSNGRuhGeqQk5W7lj/H6FMlUAuXXN3/va3z3esfzRhwnwuuOCImNcREREpKjQTSUREJEYsoD97rWtW38w+NbMfzWyOmV0fHa9qZh+Z2dzoc5U8r7ndzOaZ2c9mdkaA/1lEYiMnC56qmL+BNGAeVGsWSANp8+ZM+vYdxV13fYoZ/P3vp/PKK11jXkdERKQo0UwkERGRGLHwLj3LAm5y9xlmVgGYbmYfAf2BCe7+sJndBtwG3GpmzYF+QAvgAOBjMzvE3bNDyi+ydxNvzN1u2g+6vBlYqYUL19G9+3BmzVpOhQqleOONnnTpckhg9URERIoKzUQSEREp4tx9mbvPiG5vBH4E6gLdgKHR04YC3aPb3YBh7p7h7vOBeUDr+KYW2Qc/DYOZT0W2K9QPtIHk7vTrl86sWctp3LgqU6depgaSiIhIlJpIIiIiMZII62qb2UHA0cBUoJa7L4NIowmoGT2tLrA4z8uWRMdEEtPnt+du95kYaCkz48UXz6ZHj0OZNu0yDj20RqD1REREihI1kURERBKcmQ00s2/yPAbu5rzyQDpwg7tv2NNbFjDmscgqEnM52bBhQWS7wwtQuVHMS2zfns2oUT/s2D/ssJqkp/ehSpWyMa8lIiJSlKmJJCIiEisBTUVy98Hufmyex+BdSpulEmkg/dfd34oOrzCzOtHjdYCV0fElQP08L68H/BaT/wYisTb/vdztZufG/O1XrdpMhw7/oXfvkbz88oyYv7+IiEhxoiaSiIhIjIR4dzYDXgZ+dPfH8xwaB1wc3b4YGJtnvJ+ZlTazhkATYFrM/kOIxErWNhhzdmS75fVQqkJM33727BW0avUin322kNq1y9OiRc29v0hERKQE093ZREREir62wIXAd2Y2Kzp2B/AwMMLMBgCLgN4A7j7HzEYAPxC5s9sg3ZlNElLeO7K1uSumb52e/gMXXTSGLVu206rVAYwe3Ze6dSvGtIaIiEhxoyaSiIhIjNi+roIdI+7+Bbtfg/v03bzmIeChwEKJ/Fnbt8CSzyLblRpCWvWYvG1OjnPffRO5//5JAFxwwREMHtyFsmVTY/L+IiIixZkuZxMRERGRxDOuB6yOLnbd+o6Yve3mzZkMHz6HpCTjkUc68Npr3dVAEhERKSTNRBIREYmRkCYiiRQ/S7+EBR9Etis3hkadYvbWFSqUZuzYfsyfv44zz2wcs/cVEREpCTQTSUREJEbMgnmIlCjz34NhbXP3L/4Oyh/wp97y00/nc8stH+HuADRtWl0NJBERkf2gmUgiIiIikjhmv5i7ffozkFJmv9/K3Xnmma+54Yb3yc522ratT7duzWIQUkREpGRSE0lERCRmNG1I5E/ZsBDmjY5st30Qjrxqv98qMzObQYPG89JLMwG45ZYT6NLlkFikFJH/b+/O46Oq7/2Pvz4kgbCEVcUFNFyLuLApi7ggARfAKkLRGxDrletPVFS8tlqX21qt1VsR63LdKkjdhSoioBW0ogRFEISAC8JFRUld2KXsIfn8/jgnZBiSzCTMTAJ5P3nkwVm/5zPznZPzzed8z3dEpNZSEklEREREqt/2jTA2u3T+kBOr/Dznjz9uZvDgv/HBB6vIzExn3LjzGTasY2LiFBERqcWURBIREUkQjV8kUkWfPQPTLyud7zQSWveuUlHLlq3l7LOfY9WqTRxxRBavvTaErl33bUwlERERCSiJJCIiIiLVJ+9mmD+6dP7UO+GU26tc3OGHZ9G4cT1OOaUVr76ay6GHNkpAkCIiIgJKIomIiCSMOiKJVNKPC/dMIF26BA7uUOliioudwsIi6tVLJyurHm+99UtatKhPvXpq6oqIiCSSrqwiIiIJosfZRCop8hG2UVsgo0Gli9i0aQfDhr1Ks2aZPPPMQMyMww/PSlyMIiIispuSSCIiIiKSWu6Q/yis/SSY7/mnKiWQVqxYz4ABL7F06VqaNcvkm29+Iju7aYKDFRERkRJKIomIiCSI6YE2kfg83xVWLyyd73xNpYt4++0vyc19hQ0btnP88QczdeoQJZBERESSrE51ByAiIiIitcjKt/ZMIA3/AurGP/i1u/Pgg3Pp1+8FNmzYzoAB7fjww8s5+ujmSQhWREREIqknkoiISKKoI5JIbBuWl05fvw3SMyu1+9ixC7nhhhkA/Pa3Pbnzzt7UqaOTT0REJBWURBIREUkQ/RkrEkPhNph5XTDdcUSlE0gAl1zSkeeeW8KoUd256KITEhygiIiIVERJJBERERFJjcWPl063OD7u3fLzf6Bt2+Y0bFiXBg0yyMu7DNPXIYqIiKScxkQSERFJELPk/IgcEFbnw6xfB9PpDeDEUXHt9sILS+jRYxzDh0/B3QGUQBIREakmSiKJiIiISPL946rS6SGzY2ZIi4qK+c1v3uaSSyazY0cRzZplUlTkSQ5SREREKqLH2URERBLENCqSSPnWfBL83+N30PKkCjfduHE7F188iTffXEF6eh0efrgfV1/dLQVBioiISEWURBIREUkU5ZBE9rZ9IxTvgnpNYNdWOO6SCjdftmwtAwZMYPnydbRoUZ9XXvl3cnKyUxOriIiIVEhJJBERERFJjvn3Qd5v9lyWXr/CXR5+eB7Ll6+jY8eWTJkyhOzspkkMUERERCpDSSQREZEEUUckkZA7/PgxfPV6MJ/RCNLqBY+xZR1R4a7339+X5s3rc/PNp9OoUd0UBCsiIiLxUhJJRERERBJr/DGwcUXpfP9noe2gMjfdtq2QP/4xj1tuOZ2srHpkZqZz1119UhSoiIiIVIaSSCIiIgmibx0XATZ9s2cC6Zh/h9a9y9z0n//cxMCBE1mw4Du+/nojL744OEVBioiISFUoiSQiIiIi+84d1i+FDf9Xuuy/dkBa2Y+kzZ1bwKBBE/nhh81kZzfl1ltPT1GgIiIiUlVKIomIiCSIaVQkqc1eOQu+nVk63+yYchNITz+dz5VXvs7OnUXk5GTz8ssXcdBBDVIUqIiI7A8KCwspKChg+/bt1R3KfiszM5NWrVqRkZGRsDKVRBIREUkQPc4mtVpBXun0ISdChyv22qS42Pn1r2fw4IPzALjmmm488EBfMjLSUhWliIjsJwoKCsjKyiI7OxtTI6vS3J1169ZRUFBAmzZtElaukkgiIiIism+8GIp3BdNXfQ8NDy1zMzPYsqWQjIw6PProuVxxRZcUBikiIvuT7du3K4G0D8yMFi1asGbNmoSWqySSiIiIiOybhyIeRauz9yNsxcVOnTqGmfHII+dy5ZVd6NLl8BQGKCIi+yMlkPZNMt6/OgkvUURERERqBy+GB+pC0Y5g/sgzIbPZHptMm7aMk08ex8aNwZgWdeumKYEkIiKyn1ISSUREJEHMkvMjUiMtehT+nAbFhcF8Vmu46B+7P7Tuzj33zOaCCyawYMF3jB37cTUGKyIiUnXuTnFxcbUce9euXdVy3PIoiSQiIpIglqR/IjXO5u9g5rWl8w0Pgyu+2T27dWshQ4dO4r//O/i2trvv7sONN56a6ihFRESqbOXKlRx33HGMHDmSk046iVWrVnHTTTfRvn17OnTowMSJE3dvO3r0aDp06ECnTp245ZZb9irrxx9/ZNCgQXTq1IlOnToxZ84cVq5cSfv27XdvM2bMGO644w4AcnJyuO222+jVqxd333032dnZu5NYW7dupXXr1hQWFvLll1/Sr18/unTpQs+ePfniiy+S+6agMZFEREREpLI+HV86PXgGtM7Z3QPp229/YuDACSxa9AONGtXlxRd/wfnnt6ueOEVE5MBwf5Juqv3aK1y9bNky/vrXv/LYY48xadIk8vPzWbx4MWvXrqVbt26cccYZ5Ofn89prrzFv3jwaNGjA+vXr9ypn1KhR9OrVi8mTJ1NUVMTmzZvZsGFDhcfeuHEjs2bNAmDhwoXMmjWL3r17M23aNPr27UtGRgYjRozgiSeeoG3btsybN4+RI0cyc+bMqr8fcVASSUREJEH06JnUCu7wwe+C6WZtIfuc3atWr95Ct25jWb16C0cf3YwpU4ZwwgmHVFOgIiIi++aoo46iR48eALz//vsMHTqUtLQ0WrZsSa9evZg/fz6zZs1i+PDhNGgQfMlE8+bN9ypn5syZPPvsswCkpaXRpEmTmEmk3NzcPaYnTpxI7969mTBhAiNHjmTz5s3MmTOHiy66aPd2O3bs2OfXHIuSSCIiIiISn6Kd8GC90vlTfr/H6kMOaUhu7gksXbqWiRMvpHnz+ikOUEREDkgxegwlS8OGDXdPu5cdg7tX6VvQ0tPT9xhnafv27eUee8CAAdx6662sX7+ejz/+mD59+rBlyxaaNm1Kfn5+pY+9LzQmkoiISIJYkn5Eaozpw0un6zWBdrkUFhbx7bc/7V785z/35c03hymBJCIiB5QzzjiDiRMnUlRUxJo1a8jLy6N79+6cc845jB8/nq1btwKU+TjbmWeeyeOPPw5AUVERmzZtomXLlqxevZp169axY8cOXn/99XKP3ahRI7p3787111/PeeedR1paGo0bN6ZNmza8/PLLQJDMWrx4cRJe+Z6URBIREUkUZZHkQFa4Db54sXT+mg2sXb+Tvn2fJyfnadauDRrP6el1SE9XE1NERA4sgwYNomPHjnTq1Ik+ffowevRoDj30UPr168eAAQPo2rUrnTt3ZsyYMXvt+9BDD/Huu+/SoUMHunTpwmeffUZGRga33347J598Mueddx7HHntshcfPzc3l+eef3+MxtxdeeIGnnnqKTp06ccIJJzBlyvQbC3wAABHISURBVJSEv+5oVl6XrOq2fRc1MzCRGqZZt2tjbyQibFv0SNLTMf/aUZyUa1dWvTpKJUnKtGvXzpctW7b3iv97DaYOCqaHzeeTNa0ZMGACK1dupGXLhkyffgmdOx+a2mBriffee4+cnJzqDkOiqF5qHtVJzVTVelm6dCnHHXdc4gOqZcp6H83sY3fvWpXyNCaSiIhIgpi6DcmBrCgcqyG9Pq9+0IBL/+MptmwppGvXw5k8OZdWrRpXb3wiIiKSdOprLCIiIiKxffYMxcXGnfOGM/jCl9mypZBhwzqQl3eZEkgiIiK1hHoiiYiIJEgVvphDZP+wdTUUzCLvq6O446VDMIN77z2LG288tUrfSCMiIiL7JyWRRERERKR8XgyPtwQg52crufO6g+nW/2z6929bzYGJiMiBzt11s2IfJGMMbCWRREREEkRNHDkQvTfyaBpnHsZJrb6HdkO4/dphUK9JdYclIiIHuMzMTNatW0eLFi2USKoCd2fdunVkZmYmtFwlkURERBJF7Rs5gPjWtTx+7ytc/+QvOTRrM4seWM5B571U3WGJiEgt0apVKwoKClizZk11h7LfyszMpFWrVgktU0kkERGRA4CZ9QMeAtKAce7+p2oOSVIgVr1bcOv2IeBcYCtwmbsvjFXuzp1FXNd/FE/mtQPSGHripzS7dHriX4CIiEg5MjIyaNOmTXWHIVGURBIREUkQq6auSGaWBjwKnA0UAPPNbKq7f14tAUlKxFnv/YG24c/JwOPh/+XbuYWzevyJ2YvaUS99F+OuXM4lVw2FNH2pr4iISG2nJJKIiMj+rzuwwt2/AjCzCcAFgJJIB7Z46v0C4FkPRtaca2ZNzewwd/++vEJXripk+cpdHN54E69dPoluY1ZAHTUZRUREBHRLSUREJEHMkvMThyOAVRHzBeEyObDFU++V/mzsLEqjx1GrWDC2kG5XjlYCSURERHarsa2CzHQNT1oTmdkId3+yuuOQUtsWPVLdIUgZdK7UTsm6dpnZCGBExKInoz5fZR038d/pKjVNPPUe12cj6jO2Y+43T316eC7A/wK5+xKjJMZBwNrqDkL2onqpeVQnNZPqpeZpV9Uda2wSSWqsEYD+MBaJTeeKJEyYMKro81QAtI6YbwV8l9SgpCaIp97j+mxEfsbMbIG7d01sqLIvVCc1k+ql5lGd1Eyql5rHzBZUdV89ziYiIrL/mw+0NbM2ZlYXGAJMreaYJPniqfepwKUW6AH8VNF4SCIiIiIVUU8kERGR/Zy77zKza4EZBF/1Pt7dP6vmsCTJyqt3M7sqXP8E8HfgXGAFsBUYXl3xioiIyP5PSSSpLD2eIxIfnSuSUu7+d4KEgdQiZdV7mDwqmXbgmkoWq99fNY/qpGZSvdQ8qpOaSfVS81S5TixoW4iIiIiIiIiIiJRPYyKJiIiIiIiIiEhMSiLVUmY2yMzczI4N5zub2bkR63PM7NR9KH9zIuIUSYbws39/xPyNZnZHjH0GmtnxlTzOHudRVcqI2DfbzD6tyr4iImUxs35mtszMVpjZLWWsNzN7OFy/xMxOqo44a5M46mRYWBdLzGyOmXWqjjhrk1h1ErFdNzMrMrMLUxlfbRVPvYTtsHwz+8zMZqU6xtomjt9fTcxsmpktDutEY/QlmZmNN7PV5f0NUdXrvJJItddQ4H2Cb3IB6Eww8GaJHKDKSSSRGm4H8AszO6gS+wwEKpsAymHP86gqZYiIJJyZpQGPAv0Jfi8NLSPJ3R9oG/6MAB5PaZC1TJx18jXQy907AnehcUaSKs46KdnuXoJB7iXJ4qkXM2sKPAYMcPcTgItSHmgtEue5cg3wubt3Imgj3x9+s6gkz9NAvwrWV+k6ryRSLWRmjYDTgMuBIeHJ+wcgN8zW3wxcBdwQzvc0s/PNbJ6ZLTKzf5hZy5KyzOyvZvZJmL0cHHWsg8zsQzP7eYpfpkhFdhE0vG+IXmFmR5nZO+Hn+R0zOzLsTTQAuC88J46O2mev88PMstnzPOoVXYaZXWFm88M7MpPMrEFYXkszmxwuX2xRvQLN7N/CY3VLxpsjIrVCd2CFu3/l7juBCcAFUdtcADzrgblAUzM7LNWB1iIx68Td57j7hnB2LtAqxTHWNvGcJwDXAZOA1akMrhaLp14uBl51928B3F11k1zx1IkDWWZmQCNgPUGbXJLE3fMI3ufyVOk6ryRS7TQQmO7uywk+VO2B24GJ7t7Z3e8FngAeCOdnE/Ra6uHuJxL8UvhNWNbvgJ/cvUN4V2xmyUHCRNMbwO3u/kaqXpxInB4FhplZk6jljxD8Mu0IvAA87O5zgKnATeE58WXUPnudH+6+kj3Po1lllPGqu3cL78gsJUjsAjwMzAqXnwTs/qp2M2tH0FAd7u7zE/ReiEjtcwSwKmK+IFxW2W0kcSr7fl8OvJnUiCRmnZjZEcAggmu+pEY858oxQDMze8/MPjazS1MWXe0UT508AhwHfAd8Alzv7sWpCU/KUaXrfHrSwpGabCjwYDg9IZz/rPzNgeBO18QwM1mXoDs1wFmUPhJHxN2xDOAd4Jrwj2eRGsXdN5nZs8AoYFvEqlOAX4TTzwGj4yiuvPMjlvZm9kegKcEdmZJu8H2AS8M4i4CfzKwZcDAwBRjs7rHOWRGRilgZy6K/sjeebSRx4n6/zaw3QRLp9KRGJPHUyYPAze5eFHSwkBSIp17SgS7AmUB94EMzmxveRJfEi6dO+gL5BO3co4G3zWy2u29KdnBSripd59UTqZYxsxYEJ+44M1sJ3ATkUvYHKNL/Ao+4ewfgSiCzpEjK/qDtAj4m+GUhUlM9SNAIb1jBNvH8wVTe+RHL08C14X53xrHfTwR3C06Ls3wRkfIUAK0j5lsR3B2u7DaSOHG932bWERgHXODu61IUW20VT510BSaE7eoLgcfMbGBqwqu14v39Nd3dt7j7WiAP0ED0yRNPnQwn6IXv7r6C4KbrsSmKT8pWpeu8kki1z4UEj+oc5e7Z7t6a4AQ+EsiK2O5fUfNNgH+G0/8Rsfwt4NqSmbC3BAR/eP8ncGxF32QhUp3cfT3wN0ofIwOYQ2nvumEEj6rB3udEpPLOj+h9ouezgO/NLCM8Vol3gKshGKjQzBqHy3cSPI56qZldXOGLExGp2HygrZm1CcdGHELwyG2kqQS/b8zMehA8vv59qgOtRWLWiZkdCbwK/FI9KlIiZp24e5uwTZ0NvAKMdPfXUh9qrRLP768pQE8zSw/HnDyZYOgASY546uRbgp5hJcOetAO+SmmUEq1K13klkWqfocDkqGWTgEOB48MBf3OBacCgkoG1gTuAl81sNrA2Yt8/Ejxv/KmZLQZ6l6wIH8MZAvQ2s5FJe0Ui++Z+IPJb2kYBw81sCfBL4Ppw+QTgpnBA66OjyriDss+P6PMouozfAfOAt4EvIva7nuC8+YSgR98JJSvcfQtwHsGA3WUN7ikiEpO77yK4CTSD4A+rv7n7Z2Z2lZldFW72d4IG/gpgLKBreRLFWSe3Ay0Iervkm9mCagq3VoizTiTF4qkXd18KTAeWAB8B49y9zK85l30X57lyF3Bq2L59h+Ax0LVllyiJYGYvAR8C7cyswMwuT8R13tz1aLuIiIiIiIiIiFRMPZFERERERERERCQmJZFERERERERERCQmJZFERERERERERCQmJZFERERERERERCQmJZFERERERERERCQmJZFEYjCzovBrdD81s5fNrME+lPW0mV0YTo8zs+Mr2DbHzE6twjFWmtlB8S4vp4zLzOyRRBxXREREpLaIaDeW/GRXsO3mBBzvaTP7OjzWQjM7pQpl7G6TmtltUevm7GuMYTmR7elpZtY0xvadzezcRBxbRBJLSSSR2La5e2d3bw/sBK6KXGlmaVUp1N3/n7t/XsEmOUClk0giIiIiUm1K2o0lPytTcMyb3L0zcAvwl8ruHNUmvS1qXaLaopHt6fXANTG27wwoiSRSAymJJFI5s4Gfhb2E3jWzF4FPzCzNzO4zs/lmtsTMrgSwwCNm9rmZvQEcUlKQmb1nZl3D6X7h3aPFZvZOeNfqKuCG8K5NTzM72MwmhceYb2anhfu2MLO3zGyRmf0FsHhfjJl1N7M54b5zzKxdxOrWZjbdzJaZ2e8j9rnEzD4K4/pLdBLNzBqa2Rvha/nUzHIr+R6LiIiIHBDMrFHYtltoZp+Y2QVlbHOYmeVF9NTpGS4/x8w+DPd92cwaxThcHvCzcN9fhWV9amb/FS4rs41W0iY1sz8B9cM4XgjXbQ7/nxjZMyjsATW4vDZwDB8CR4Tl7NUWNbO6wB+A3DCW3DD28eFxFpX1PopIaqRXdwAi+wszSwf6A9PDRd2B9u7+tZmNAH5y925mVg/4wMzeAk4E2gEdgJbA58D4qHIPBsYCZ4RlNXf39Wb2BLDZ3ceE270IPODu75vZkcAM4Djg98D77v4HM/s5MKISL+uL8Li7zOws4B5gcOTrA7YC88Mk2BYgFzjN3QvN7DFgGPBsRJn9gO/c/edh3E0qEY+IiIjI/qy+meWH018DFwGD3H2TBY/9zzWzqe7uEftcDMxw97vDm3MNwm1/C5zl7lvM7GbgVwTJlfKcT3BzswswHDiZ4ObiPDObBfwbFbTR3P0WM7s27NUUbQJBG/DvYZLnTOBq4HLKaAO7+9dlBRi+vjOBp8JFe7VF3X2wmd0OdHX3a8P97gFmuvt/WvAo3Edm9g9331LB+yEiSaAkkkhskY2B2QQXvVOBjyIukOcAHS0c7whoArQFzgBecvci4Dszm1lG+T2AvJKy3H19OXGcBRxvtrujUWMzywqP8Ytw3zfMbEMlXlsT4Bkzaws4kBGx7m13XwdgZq8CpwO7gC4ESSWA+sDqqDI/AcaY2b3A6+4+uxLxiIiIiOzPtkUmYcwsA7jHzM4Aigl64LQEfojYZz4wPtz2NXfPN7NewPEESRmAugQ9eMpyn5n9FlhDkNQ5E5hckmAJ23E9CW6EVrWN9ibwcJgo6kfQdt1mZuW1gaOTSCXt6WzgY+DtiO3La4tGOgcYYGY3hvOZwJHA0kq8BhFJACWRRGLbFn1HJryYR975MOA6d58Rtd25BBfEilgc20Dw+Okp7r6tjFji2b8sdwHvuvsgCx6hey9iXXSZHsb6jLvfWl6B7r48vAN2LvA/4d2oiu6aiYiIiByohgEHA13CXtwrCRIgu7l7Xphk+jnwnJndB2wguKE3NI5j3OTur5TMhD169rIvbTR3325m7wF9CXokvVRyOMpoA5dhm7t3Dns/vU4wJtLDVNwWjWTAYHdfFk+8IpI8GhNJJDFmAFeHd5Aws2PMrCHBs+lDwufFDwN6l7Hvh0AvM2sT7ts8XP4vICtiu7eAa0tmzKwksZVH0EDBzPoDzSoRdxPgn+H0ZVHrzjaz5mZWHxgIfAC8A1xoZoeUxGpmR0XuZGaHA1vd/XlgDHBSJeIREREROZA0AVaHCaTewFHRG4RtqdXuPpagx/tJwFzgNDMrGeOogZkdE+cx84CB4T4NgUHA7DjbaIUl7dkyTCB4TK4nQdsXym8Dl8ndfwJGATeG+5TXFo1uB88ArrPw7qmZnVjeMUQkudQTSSQxxhF0z10YXtzWECReJgN9CB7xWg7Mit7R3deEYyq9amZ1CB4POxuYBrwSDhx4HcEF91EzW0Jw7uYRDL59J/CSmS0My/+2gjiXmFlxOP03YDRBF+JfAdGP2r0PPEcwQOOL7r4AIOwu/VYYayHBnaRvIvbrQNCtujhcf3UF8YiIiIgcyF4AppnZAiCfYAygaDnATWZWCGwGLg3bh5cRtPHqhdv9lqA9WSF3X2hmTwMfhYvGufsiM+tL7DbakwTtxYXuPixq3VsE42BOdfedJWVTdhu4ovgWmdliYAjlt0XfBW4JH4H7H4IeSw+GsRmwEjiv4ndCRJLB9hzTTUREREREREREZG96nE1ERERERERERGJSEklERERERERERGJSEklERERERERERGJSEklERERERERERGJSEklERERERERERGJSEklERERERERERGJSEklERERERERERGJSEklERERERERERGL6/weJ3DkRfcp4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
