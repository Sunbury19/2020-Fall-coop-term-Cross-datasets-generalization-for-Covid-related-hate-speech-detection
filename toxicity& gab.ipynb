{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model construction with 'Toxicity' and 'Gab' data\n",
    "\n",
    "\n",
    "2020/10/22\n",
    "\n",
    "In this version, we augmented the toxicity and gab data together based on the manually created 'racism_word.csv', then we test the model on different data sets.\n",
    "\n",
    "Student: Xuanyu Su                                                                 \n",
    "Supervisor: Isar Nejadgholi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYHMqcSSPwsb"
   },
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "DkD7cFwvQLqa",
    "outputId": "c729e5d2-727b-46c8-edff-f764ade87a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
      "\r",
      "\u001b[K     |██████▍                         | 10kB 25.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 20kB 15.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 30kB 13.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 40kB 13.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=a3d2c30201634c7d78317c98fd3a8447730f0e3c5794cb70510ea4b1345d52fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "us6M0vmTzZxz",
    "outputId": "8abbfc45-fa67-4e81-aac9-33d9ca0ec00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "O7l7a6xlQPF-",
    "outputId": "92849849-e91c-4cb7-df71-94722abdca3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20kB 5.3MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40kB 6.5MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 51kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 61kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 71kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 81kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 92kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 102kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 112kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 122kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 133kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 143kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 153kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 163kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 174kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 184kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 194kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 204kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 215kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 225kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 235kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 245kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 256kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 266kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 276kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 286kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 296kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 307kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 317kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 327kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 337kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 348kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 358kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 368kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 378kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 389kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 399kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 409kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 419kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 430kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 440kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 450kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 460kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 471kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 481kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 491kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 501kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 512kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 522kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 532kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 542kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 552kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 563kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 573kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 583kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 593kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 604kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 614kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 624kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 634kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 645kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 655kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 665kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 675kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 686kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 696kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 706kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 716kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 727kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 737kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 747kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 757kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 768kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 778kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 788kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 798kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 808kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 819kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 829kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 839kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 849kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 860kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 870kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 880kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 890kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 901kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 911kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 921kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 931kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 942kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 952kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 962kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 972kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 983kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 993kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 1.0MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 1.0MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.0MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 1.0MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 1.0MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.1MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.2MB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.3MB 8.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Collecting tokenizers==0.9.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 34.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 50.5MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 48.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=41958f84d062a98e57c291332bc4fe2d2995abb65ae520c46c64c67dfb059b5e\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.2 transformers-3.4.0\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OsZ_iCMZOh0B"
   },
   "outputs": [],
   "source": [
    "source_folder = 'Data'\n",
    "destination_folder = 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "NeB48npCOonn",
    "outputId": "ebf43451-91ad-4775-909d-e34bfe611fc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_dga3ZKqANy9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "racism_word = pd.read_csv('/content/drive/My Drive/racism_word.csv', sep=',',encoding = \"ISO-8859-1\")\n",
    "GAB = pd.read_csv('Data/GabHateCorpus_annotations.tsv', sep='\\t',encoding = \"ISO-8859-1\")\n",
    "tox_train = pd.read_csv('Data/Toxicity_Train_comment.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "tox_val = pd.read_csv('Data/Toxicity_Val_comment.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "tox_test = pd.read_csv('Data/Toxicity_Test_comment.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LMsNib5P1wb"
   },
   "source": [
    "# Prepreocess Racism_word table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "sUW7t3MUPOEd",
    "outputId": "08d9935c-8401-4f31-e2c0-6a5e9b89dbbd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Content  Unnamed: 1\n",
       "0   Abbie         NaN\n",
       "1     Abe         NaN\n",
       "2    Abie         NaN\n",
       "3     ABC         NaN\n",
       "4    ABCD         NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "racism_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "8WuiQhX9PVH3",
    "outputId": "e34d72a7-3415-4699-f63e-1e2f30ef72f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abcd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Content\n",
       "0   abbie\n",
       "1     abe\n",
       "2    abie\n",
       "3     abc\n",
       "4    abcd"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process racism word table\n",
    "#import the racism keywords list\n",
    "import re\n",
    "racist_word_list = racism_word['Content']\n",
    "\n",
    "# drop unretaltes column in racism word table and convert all to lower\n",
    "new_racism_word = racism_word.drop(columns = ['Unnamed: 1'])\n",
    "new_racism_word['Content'] = new_racism_word['Content'].apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', ' ', x).lower())\n",
    "new_racism_word.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrxyCGY0P8OZ"
   },
   "source": [
    "# Preprocess Gab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "G9UVjHaxTk9B",
    "outputId": "52c8bbee-21d0-4629-a51d-2773697a44c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate</th>\n",
       "      <th>HD</th>\n",
       "      <th>CV</th>\n",
       "      <th>VO</th>\n",
       "      <th>REL</th>\n",
       "      <th>RAE</th>\n",
       "      <th>SXO</th>\n",
       "      <th>GEN</th>\n",
       "      <th>IDL</th>\n",
       "      <th>NAT</th>\n",
       "      <th>POL</th>\n",
       "      <th>MPH</th>\n",
       "      <th>EX</th>\n",
       "      <th>IM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27044</td>\n",
       "      <td>4</td>\n",
       "      <td>Ah the PSYOPS antifa crew is back. Thatâs ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27044</td>\n",
       "      <td>15</td>\n",
       "      <td>Ah the PSYOPS antifa crew is back. Thatâs ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27044</td>\n",
       "      <td>10</td>\n",
       "      <td>Ah the PSYOPS antifa crew is back. Thatâs ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27044</td>\n",
       "      <td>8</td>\n",
       "      <td>Ah the PSYOPS antifa crew is back. Thatâs ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27045</td>\n",
       "      <td>4</td>\n",
       "      <td>Get the new Android app update released today ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Annotator  ...  EX  IM\n",
       "0  27044          4  ... NaN NaN\n",
       "1  27044         15  ... NaN NaN\n",
       "2  27044         10  ... NaN NaN\n",
       "3  27044          8  ... NaN NaN\n",
       "4  27045          4  ... NaN NaN\n",
       "\n",
       "[5 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "dhr6yoUJUHNm",
    "outputId": "3dc1b8db-5bdb-4927-d8db-2e37f82193e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# voting the duplicates\n",
    "new_label = GAB.groupby('ID')['Hate'].mean() > 0.5\n",
    "\n",
    "#  tmp label list\n",
    "temp = []\n",
    "for i in new_label:\n",
    "    if i is True:\n",
    "        temp.append(1.0)\n",
    "    else:\n",
    "        temp.append(0.0)\n",
    "new_label = temp\n",
    "\n",
    "# drop previous duplicated list\n",
    "new_GAB = GAB.drop_duplicates(subset=['ID'])\n",
    "\n",
    "# assign new label list to table\n",
    "new_GAB['Hate'] = temp\n",
    "\n",
    "new_GAB = new_GAB[['Text','Hate']]\n",
    "new_GAB = new_GAB.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "_8vCf1JNUZQP",
    "outputId": "7edac8f8-0298-43fc-cf1e-35f3dc67a88a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27665 entries, 0 to 27664\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Text    27665 non-null  object \n",
      " 1   Hate    27665 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 432.4+ KB\n"
     ]
    }
   ],
   "source": [
    "new_GAB.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oRfZjlOpAN0W"
   },
   "outputs": [],
   "source": [
    "# class of data preprocessing\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "\n",
    "class Word_Preprocessing():\n",
    "    def eliminate_url(self,df,target):\n",
    "        print('Start eliminate url: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        text = df_temp[target_column_name]\n",
    "        for i in tqdm(text):\n",
    "            urls = re.findall(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', i)\n",
    "            for i in urls:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_username(self,df,target):\n",
    "        print('Start eliminate username: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'@\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    \n",
    "    def eliminate_hashtag(self, df, target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'#\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_symbol(self,df,target):\n",
    "        print('Start eliminate symbol: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        symbol_list = ['!','@','#','$','%','^','&','*','(',')','-','+','?','>','<','=','/','.',':',';','  ','   ','    ','      ','      ','  ']\n",
    "        for i in tqdm(symbol_list):\n",
    "            df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, ' '))\n",
    "        return df_temp\n",
    "    \n",
    "    def to_Lower(self,df,target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        print('Start eliminate lower: : )')\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', ' ', x).lower())\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_emoji(self,df,target):\n",
    "        print('Start transfer emoji: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        preprocessed_text = []\n",
    "        df_temp[target_column_name] = df_temp.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "        return df_temp\n",
    "    \n",
    "    def process_all(self, df,target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        df_remove_url = self.eliminate_url(df_temp,target_column_name)\n",
    "        df_transfer_emoji = self.eliminate_emoji(df_remove_url, target_column_name)\n",
    "        df_eliminate_hashtag = self.eliminate_hashtag(df_transfer_emoji, target_column_name)\n",
    "        df_remove_username = self.eliminate_username(df_eliminate_hashtag, target_column_name)\n",
    "        df_remove_symbol = self.eliminate_symbol(df_remove_username, target_column_name)\n",
    "        df_to_lower = self.to_Lower(df_remove_symbol, target_column_name)\n",
    "        print(\"finished!!\")\n",
    "        return df_to_lower   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "AERo8uReU3DS",
    "outputId": "55f6abd9-7f18-47d6-cd48-ff106b1bb5e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/27665 [00:00<03:32, 129.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate url: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27665/27665 [00:57<00:00, 484.39it/s]\n",
      "  0%|          | 0/27665 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start transfer emoji: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27665/27665 [00:00<00:00, 34280.21it/s]\n",
      "  1%|          | 306/27665 [00:00<00:22, 1237.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate username: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27665/27665 [00:00<00:00, 31065.27it/s]\n",
      " 46%|████▌     | 12/26 [00:00<00:00, 115.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate symbol: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 97.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate lower: : )\n",
      "finished!!\n"
     ]
    }
   ],
   "source": [
    "word_preprocesser = Word_Preprocessing()\n",
    "new_GAB1 = word_preprocesser.process_all(new_GAB, 'Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ULXAaaRR93"
   },
   "source": [
    "# start extrcting racism word from both gab and toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UGsm2cmEVtNQ"
   },
   "outputs": [],
   "source": [
    "# define find hate sentence function\n",
    "import numpy as np\n",
    "def find_hate_sentence(df, keyword, label, checklist):\n",
    "    hate_sentence = []\n",
    "    hate_label = []\n",
    "    for i in tqdm(range(len(df))):  \n",
    "      temp_df = df.iloc[i]\n",
    "      for j in checklist:\n",
    "        if j in temp_df[keyword]:\n",
    "          hate_sentence.append(temp_df[keyword])\n",
    "          hate_label.append(temp_df[label])\n",
    "    hate = {'Text':hate_sentence, 'Label':hate_label}\n",
    "    hate = pd.DataFrame(hate)\n",
    "    hate = hate.drop_duplicates(subset=[keyword])\n",
    "    return hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "T8I4clolXPJA",
    "outputId": "f40bb0cd-1269-4dc0-ab62-ba4ad6165a93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27665/27665 [00:49<00:00, 561.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply find hate function in gab data\n",
    "hate_keyword = new_racism_word['Content']\n",
    "hate_gab = find_hate_sentence(new_GAB1, 'Text', 'Hate', hate_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "wHvGleKWtMiX",
    "outputId": "95b685d9-a7aa-4cc3-f5f4-bcf66da53258"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these  companies  are run by sociopaths  he sa...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnn mocked over self important facts first ad ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joseph kleitsch  selfportrait hungarian ameri...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teachable moment this week patriots anons as y...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the world is a better safer place now that mcc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxicity\n",
       "0  these  companies  are run by sociopaths  he sa...       0.0\n",
       "1  cnn mocked over self important facts first ad ...       1.0\n",
       "2   joseph kleitsch  selfportrait hungarian ameri...       0.0\n",
       "3  teachable moment this week patriots anons as y...       0.0\n",
       "4  the world is a better safer place now that mcc...       0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the column name of hate in gab into toxicity\n",
    "hate_gab.rename(index=str, columns={\"Label\": \"toxicity\", 'Text':'comment'},inplace=True)\n",
    "hate_gab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "kEHz8rOsXo3s",
    "outputId": "b04eb1d7-d791-45b5-fc7b-79606f78836b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5860 entries, 0 to 7011\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comment   5860 non-null   object \n",
      " 1   toxicity  5860 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 137.3+ KB\n"
     ]
    }
   ],
   "source": [
    "hate_gab.info()\n",
    "# 5860 in 27665\n",
    "hate_gab.to_csv('Data/data_after_extraction/hate_gab.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nfJT87PPY-Bo"
   },
   "outputs": [],
   "source": [
    "# define find hate sentence function\n",
    "import numpy as np\n",
    "def find_hate_sentence(df, keyword, label, checklist):\n",
    "    hate_sentence = []\n",
    "    hate_label = []\n",
    "    for i in tqdm(range(len(df))):  \n",
    "      temp_df = df.iloc[i]\n",
    "      for j in checklist:\n",
    "        if j in temp_df[keyword]:\n",
    "          hate_sentence.append(temp_df[keyword])\n",
    "          hate_label.append(temp_df[label])\n",
    "    hate = {'comment':hate_sentence, 'toxicity':hate_label}\n",
    "    hate = pd.DataFrame(hate)\n",
    "    hate = hate.drop_duplicates(subset=[keyword])\n",
    "    return hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "vOfWar0NZmrZ",
    "outputId": "e3b08737-e1af-411a-d30d-1bd9f2009912"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this one can make an analogy in mathematical t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clarification for you and zundark s right  i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is such a fun entry devotchka i once had...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i fixed the link i also removed  homeopathy ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if they are  indisputable  then why does the ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxicity\n",
       "0  this one can make an analogy in mathematical t...       0.0\n",
       "1    clarification for you and zundark s right  i...       0.0\n",
       "2   this is such a fun entry devotchka i once had...       0.0\n",
       "3    i fixed the link i also removed  homeopathy ...       0.0\n",
       "4   if they are  indisputable  then why does the ...       0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tox_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EOutSkpIVZgU",
    "outputId": "fc7ae8f1-dea2-420d-ac6c-2aeff8990ff4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/36617 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 50/36617 [00:00<01:14, 491.89it/s]\u001b[A\n",
      "  0%|          | 102/36617 [00:00<01:13, 498.45it/s]\u001b[A\n",
      "  0%|          | 154/36617 [00:00<01:12, 504.41it/s]\u001b[A\n",
      "  1%|          | 208/36617 [00:00<01:10, 513.82it/s]\u001b[A\n",
      "  1%|          | 259/36617 [00:00<01:10, 512.54it/s]\u001b[A\n",
      "  1%|          | 311/36617 [00:00<01:11, 510.83it/s]\u001b[A\n",
      "  1%|          | 360/36617 [00:00<01:12, 503.25it/s]\u001b[A\n",
      "  1%|          | 410/36617 [00:00<01:12, 499.73it/s]\u001b[A\n",
      "  1%|▏         | 462/36617 [00:00<01:11, 503.17it/s]\u001b[A\n",
      "  1%|▏         | 511/36617 [00:01<01:12, 495.15it/s]\u001b[A\n",
      "  2%|▏         | 561/36617 [00:01<01:12, 495.23it/s]\u001b[A\n",
      "  2%|▏         | 614/36617 [00:01<01:11, 504.60it/s]\u001b[A\n",
      "  2%|▏         | 666/36617 [00:01<01:10, 508.01it/s]\u001b[A\n",
      "  2%|▏         | 721/36617 [00:01<01:09, 516.77it/s]\u001b[A\n",
      "  2%|▏         | 773/36617 [00:01<01:09, 512.40it/s]\u001b[A\n",
      "  2%|▏         | 825/36617 [00:01<01:11, 502.14it/s]\u001b[A\n",
      "  2%|▏         | 880/36617 [00:01<01:09, 514.03it/s]\u001b[A\n",
      "  3%|▎         | 932/36617 [00:01<01:10, 505.25it/s]\u001b[A\n",
      "  3%|▎         | 983/36617 [00:01<01:10, 505.32it/s]\u001b[A\n",
      "  3%|▎         | 1034/36617 [00:02<01:11, 499.94it/s]\u001b[A\n",
      "  3%|▎         | 1085/36617 [00:02<01:10, 502.30it/s]\u001b[A\n",
      "  3%|▎         | 1136/36617 [00:02<01:10, 503.06it/s]\u001b[A\n",
      "  3%|▎         | 1188/36617 [00:02<01:09, 506.22it/s]\u001b[A\n",
      "  3%|▎         | 1239/36617 [00:02<01:09, 506.88it/s]\u001b[A\n",
      "  4%|▎         | 1290/36617 [00:02<01:10, 500.27it/s]\u001b[A\n",
      "  4%|▎         | 1341/36617 [00:02<01:10, 497.81it/s]\u001b[A\n",
      "  4%|▍         | 1394/36617 [00:02<01:09, 506.46it/s]\u001b[A\n",
      "  4%|▍         | 1448/36617 [00:02<01:08, 514.53it/s]\u001b[A\n",
      "  4%|▍         | 1500/36617 [00:02<01:08, 513.95it/s]\u001b[A\n",
      "  4%|▍         | 1553/36617 [00:03<01:07, 517.86it/s]\u001b[A\n",
      "  4%|▍         | 1605/36617 [00:03<01:07, 515.78it/s]\u001b[A\n",
      "  5%|▍         | 1657/36617 [00:03<01:08, 508.80it/s]\u001b[A\n",
      "  5%|▍         | 1710/36617 [00:03<01:07, 514.85it/s]\u001b[A\n",
      "  5%|▍         | 1765/36617 [00:03<01:06, 523.02it/s]\u001b[A\n",
      "  5%|▍         | 1819/36617 [00:03<01:06, 526.35it/s]\u001b[A\n",
      "  5%|▌         | 1872/36617 [00:03<01:06, 520.58it/s]\u001b[A\n",
      "  5%|▌         | 1925/36617 [00:03<01:07, 517.56it/s]\u001b[A\n",
      "  5%|▌         | 1977/36617 [00:03<01:08, 504.07it/s]\u001b[A\n",
      "  6%|▌         | 2030/36617 [00:03<01:07, 509.63it/s]\u001b[A\n",
      "  6%|▌         | 2082/36617 [00:04<01:07, 511.42it/s]\u001b[A\n",
      "  6%|▌         | 2134/36617 [00:04<01:07, 510.71it/s]\u001b[A\n",
      "  6%|▌         | 2186/36617 [00:04<01:09, 498.97it/s]\u001b[A\n",
      "  6%|▌         | 2236/36617 [00:04<01:08, 498.58it/s]\u001b[A\n",
      "  6%|▌         | 2288/36617 [00:04<01:08, 503.41it/s]\u001b[A\n",
      "  6%|▋         | 2339/36617 [00:04<01:08, 502.88it/s]\u001b[A\n",
      "  7%|▋         | 2390/36617 [00:04<01:09, 492.29it/s]\u001b[A\n",
      "  7%|▋         | 2440/36617 [00:04<01:09, 490.87it/s]\u001b[A\n",
      "  7%|▋         | 2491/36617 [00:04<01:08, 495.20it/s]\u001b[A\n",
      "  7%|▋         | 2541/36617 [00:05<01:10, 484.53it/s]\u001b[A\n",
      "  7%|▋         | 2591/36617 [00:05<01:09, 488.86it/s]\u001b[A\n",
      "  7%|▋         | 2644/36617 [00:05<01:08, 498.59it/s]\u001b[A\n",
      "  7%|▋         | 2694/36617 [00:05<01:08, 494.04it/s]\u001b[A\n",
      "  8%|▊         | 2748/36617 [00:05<01:07, 505.16it/s]\u001b[A\n",
      "  8%|▊         | 2800/36617 [00:05<01:06, 509.19it/s]\u001b[A\n",
      "  8%|▊         | 2852/36617 [00:05<01:07, 501.00it/s]\u001b[A\n",
      "  8%|▊         | 2903/36617 [00:05<01:07, 497.74it/s]\u001b[A\n",
      "  8%|▊         | 2953/36617 [00:05<01:08, 488.99it/s]\u001b[A\n",
      "  8%|▊         | 3005/36617 [00:05<01:07, 495.68it/s]\u001b[A\n",
      "  8%|▊         | 3058/36617 [00:06<01:06, 504.30it/s]\u001b[A\n",
      "  8%|▊         | 3109/36617 [00:06<01:06, 502.12it/s]\u001b[A\n",
      "  9%|▊         | 3161/36617 [00:06<01:06, 506.23it/s]\u001b[A\n",
      "  9%|▉         | 3212/36617 [00:06<01:05, 506.51it/s]\u001b[A\n",
      "  9%|▉         | 3263/36617 [00:06<01:06, 500.85it/s]\u001b[A\n",
      "  9%|▉         | 3314/36617 [00:06<01:07, 496.42it/s]\u001b[A\n",
      "  9%|▉         | 3364/36617 [00:06<01:07, 492.57it/s]\u001b[A\n",
      "  9%|▉         | 3414/36617 [00:06<01:07, 493.47it/s]\u001b[A\n",
      "  9%|▉         | 3466/36617 [00:06<01:06, 498.81it/s]\u001b[A\n",
      " 10%|▉         | 3516/36617 [00:06<01:08, 480.93it/s]\u001b[A\n",
      " 10%|▉         | 3565/36617 [00:07<01:09, 477.04it/s]\u001b[A\n",
      " 10%|▉         | 3616/36617 [00:07<01:08, 485.10it/s]\u001b[A\n",
      " 10%|█         | 3666/36617 [00:07<01:07, 487.90it/s]\u001b[A\n",
      " 10%|█         | 3718/36617 [00:07<01:06, 495.66it/s]\u001b[A\n",
      " 10%|█         | 3769/36617 [00:07<01:05, 498.26it/s]\u001b[A\n",
      " 10%|█         | 3819/36617 [00:07<01:06, 496.19it/s]\u001b[A\n",
      " 11%|█         | 3869/36617 [00:07<01:06, 495.23it/s]\u001b[A\n",
      " 11%|█         | 3921/36617 [00:07<01:05, 501.26it/s]\u001b[A\n",
      " 11%|█         | 3974/36617 [00:07<01:04, 509.02it/s]\u001b[A\n",
      " 11%|█         | 4025/36617 [00:08<01:05, 494.96it/s]\u001b[A\n",
      " 11%|█         | 4075/36617 [00:08<01:06, 492.94it/s]\u001b[A\n",
      " 11%|█▏        | 4125/36617 [00:08<01:08, 471.15it/s]\u001b[A\n",
      " 11%|█▏        | 4180/36617 [00:08<01:05, 491.50it/s]\u001b[A\n",
      " 12%|█▏        | 4233/36617 [00:08<01:04, 502.32it/s]\u001b[A\n",
      " 12%|█▏        | 4287/36617 [00:08<01:03, 511.07it/s]\u001b[A\n",
      " 12%|█▏        | 4340/36617 [00:08<01:02, 514.10it/s]\u001b[A\n",
      " 12%|█▏        | 4392/36617 [00:08<01:04, 500.65it/s]\u001b[A\n",
      " 21%|██        | 7707/36617 [00:30<00:52, 552.02it/s]\n",
      " 12%|█▏        | 4493/36617 [00:08<01:04, 498.72it/s]\u001b[A\n",
      " 12%|█▏        | 4543/36617 [00:09<01:05, 486.12it/s]\u001b[A\n",
      " 13%|█▎        | 4592/36617 [00:09<01:05, 486.10it/s]\u001b[A\n",
      " 13%|█▎        | 4641/36617 [00:09<01:05, 485.20it/s]\u001b[A\n",
      " 13%|█▎        | 4690/36617 [00:09<01:06, 483.39it/s]\u001b[A\n",
      " 13%|█▎        | 4742/36617 [00:09<01:04, 491.66it/s]\u001b[A\n",
      " 13%|█▎        | 4793/36617 [00:09<01:04, 495.80it/s]\u001b[A\n",
      " 13%|█▎        | 4844/36617 [00:09<01:03, 498.77it/s]\u001b[A\n",
      " 13%|█▎        | 4895/36617 [00:09<01:03, 501.55it/s]\u001b[A\n",
      " 14%|█▎        | 4946/36617 [00:09<01:03, 501.88it/s]\u001b[A\n",
      " 14%|█▎        | 4999/36617 [00:09<01:02, 507.83it/s]\u001b[A\n",
      " 14%|█▍        | 5050/36617 [00:10<01:03, 496.26it/s]\u001b[A\n",
      " 14%|█▍        | 5102/36617 [00:10<01:02, 502.12it/s]\u001b[A\n",
      " 14%|█▍        | 5154/36617 [00:10<01:02, 504.62it/s]\u001b[A\n",
      " 14%|█▍        | 5205/36617 [00:10<01:03, 495.24it/s]\u001b[A\n",
      " 14%|█▍        | 5256/36617 [00:10<01:02, 498.93it/s]\u001b[A\n",
      " 14%|█▍        | 5307/36617 [00:10<01:02, 499.40it/s]\u001b[A\n",
      " 15%|█▍        | 5358/36617 [00:10<01:02, 501.45it/s]\u001b[A\n",
      " 15%|█▍        | 5409/36617 [00:10<01:02, 498.31it/s]\u001b[A\n",
      " 15%|█▍        | 5459/36617 [00:10<01:03, 494.00it/s]\u001b[A\n",
      " 15%|█▌        | 5509/36617 [00:10<01:03, 493.50it/s]\u001b[A\n",
      " 15%|█▌        | 5561/36617 [00:11<01:02, 500.54it/s]\u001b[A\n",
      " 15%|█▌        | 5614/36617 [00:11<01:01, 506.63it/s]\u001b[A\n",
      " 15%|█▌        | 5668/36617 [00:11<01:00, 514.44it/s]\u001b[A\n",
      " 16%|█▌        | 5720/36617 [00:11<01:00, 510.56it/s]\u001b[A\n",
      " 16%|█▌        | 5772/36617 [00:11<01:00, 512.41it/s]\u001b[A\n",
      " 16%|█▌        | 5827/36617 [00:11<00:58, 522.49it/s]\u001b[A\n",
      " 16%|█▌        | 5881/36617 [00:11<00:58, 526.40it/s]\u001b[A\n",
      " 16%|█▌        | 5934/36617 [00:11<00:59, 515.16it/s]\u001b[A\n",
      " 16%|█▋        | 5986/36617 [00:11<01:00, 504.35it/s]\u001b[A\n",
      " 16%|█▋        | 6037/36617 [00:12<01:01, 501.28it/s]\u001b[A\n",
      " 17%|█▋        | 6088/36617 [00:12<01:02, 490.22it/s]\u001b[A\n",
      " 17%|█▋        | 6138/36617 [00:12<01:02, 491.32it/s]\u001b[A\n",
      " 17%|█▋        | 6190/36617 [00:12<01:01, 498.27it/s]\u001b[A\n",
      " 17%|█▋        | 6241/36617 [00:12<01:00, 501.22it/s]\u001b[A\n",
      " 17%|█▋        | 6293/36617 [00:12<00:59, 506.07it/s]\u001b[A\n",
      " 17%|█▋        | 6344/36617 [00:12<01:00, 503.57it/s]\u001b[A\n",
      " 17%|█▋        | 6395/36617 [00:12<01:01, 495.18it/s]\u001b[A\n",
      " 18%|█▊        | 6445/36617 [00:12<01:02, 479.89it/s]\u001b[A\n",
      " 18%|█▊        | 6495/36617 [00:12<01:02, 484.76it/s]\u001b[A\n",
      " 18%|█▊        | 6549/36617 [00:13<01:00, 498.18it/s]\u001b[A\n",
      " 18%|█▊        | 6602/36617 [00:13<00:59, 504.99it/s]\u001b[A\n",
      " 18%|█▊        | 6654/36617 [00:13<00:58, 508.60it/s]\u001b[A\n",
      " 18%|█▊        | 6705/36617 [00:13<00:59, 504.38it/s]\u001b[A\n",
      " 18%|█▊        | 6756/36617 [00:13<00:59, 502.41it/s]\u001b[A\n",
      " 19%|█▊        | 6807/36617 [00:13<00:59, 503.98it/s]\u001b[A\n",
      " 19%|█▊        | 6858/36617 [00:13<00:58, 505.47it/s]\u001b[A\n",
      " 19%|█▉        | 6910/36617 [00:13<00:58, 505.90it/s]\u001b[A\n",
      " 19%|█▉        | 6962/36617 [00:13<00:58, 508.17it/s]\u001b[A\n",
      " 19%|█▉        | 7013/36617 [00:13<00:58, 505.77it/s]\u001b[A\n",
      " 19%|█▉        | 7066/36617 [00:14<00:57, 511.11it/s]\u001b[A\n",
      " 19%|█▉        | 7118/36617 [00:14<00:58, 505.45it/s]\u001b[A\n",
      " 20%|█▉        | 7171/36617 [00:14<00:57, 511.04it/s]\u001b[A\n",
      " 20%|█▉        | 7224/36617 [00:14<00:57, 515.11it/s]\u001b[A\n",
      " 20%|█▉        | 7276/36617 [00:14<00:57, 508.29it/s]\u001b[A\n",
      " 20%|██        | 7329/36617 [00:14<00:57, 513.73it/s]\u001b[A\n",
      " 20%|██        | 7381/36617 [00:14<00:58, 500.10it/s]\u001b[A\n",
      " 20%|██        | 7432/36617 [00:14<00:59, 486.74it/s]\u001b[A\n",
      " 20%|██        | 7484/36617 [00:14<00:58, 494.50it/s]\u001b[A\n",
      " 21%|██        | 7536/36617 [00:15<00:58, 501.05it/s]\u001b[A\n",
      " 21%|██        | 7587/36617 [00:15<00:57, 503.05it/s]\u001b[A\n",
      " 21%|██        | 7642/36617 [00:15<00:56, 514.22it/s]\u001b[A\n",
      " 21%|██        | 7694/36617 [00:15<00:58, 497.58it/s]\u001b[A\n",
      " 21%|██        | 7745/36617 [00:15<00:57, 500.48it/s]\u001b[A\n",
      " 21%|██▏       | 7796/36617 [00:15<00:57, 499.63it/s]\u001b[A\n",
      " 21%|██▏       | 7848/36617 [00:15<00:56, 505.32it/s]\u001b[A\n",
      " 22%|██▏       | 7901/36617 [00:15<00:56, 510.72it/s]\u001b[A\n",
      " 22%|██▏       | 7953/36617 [00:15<00:56, 507.23it/s]\u001b[A\n",
      " 22%|██▏       | 8004/36617 [00:15<00:57, 497.92it/s]\u001b[A\n",
      " 22%|██▏       | 8054/36617 [00:16<00:57, 494.21it/s]\u001b[A\n",
      " 22%|██▏       | 8105/36617 [00:16<00:57, 497.98it/s]\u001b[A\n",
      " 22%|██▏       | 8157/36617 [00:16<00:56, 502.56it/s]\u001b[A\n",
      " 22%|██▏       | 8210/36617 [00:16<00:55, 509.54it/s]\u001b[A\n",
      " 23%|██▎       | 8262/36617 [00:16<00:55, 512.61it/s]\u001b[A\n",
      " 23%|██▎       | 8314/36617 [00:16<00:56, 502.81it/s]\u001b[A\n",
      " 23%|██▎       | 8367/36617 [00:16<00:55, 508.89it/s]\u001b[A\n",
      " 23%|██▎       | 8418/36617 [00:16<00:55, 506.27it/s]\u001b[A\n",
      " 23%|██▎       | 8469/36617 [00:16<00:56, 499.25it/s]\u001b[A\n",
      " 23%|██▎       | 8519/36617 [00:16<00:57, 491.73it/s]\u001b[A\n",
      " 23%|██▎       | 8569/36617 [00:17<00:56, 493.78it/s]\u001b[A\n",
      " 24%|██▎       | 8619/36617 [00:17<01:00, 459.76it/s]\u001b[A\n",
      " 24%|██▎       | 8668/36617 [00:17<00:59, 467.95it/s]\u001b[A\n",
      " 24%|██▍       | 8717/36617 [00:17<00:59, 472.83it/s]\u001b[A\n",
      " 24%|██▍       | 8767/36617 [00:17<00:57, 480.18it/s]\u001b[A\n",
      " 24%|██▍       | 8818/36617 [00:17<00:57, 487.63it/s]\u001b[A\n",
      " 24%|██▍       | 8867/36617 [00:17<00:57, 485.59it/s]\u001b[A\n",
      " 24%|██▍       | 8916/36617 [00:17<00:59, 462.08it/s]\u001b[A\n",
      " 24%|██▍       | 8963/36617 [00:17<00:59, 463.85it/s]\u001b[A\n",
      " 25%|██▍       | 9010/36617 [00:18<00:59, 464.37it/s]\u001b[A\n",
      " 25%|██▍       | 9061/36617 [00:18<00:57, 476.30it/s]\u001b[A\n",
      " 25%|██▍       | 9109/36617 [00:18<00:57, 475.28it/s]\u001b[A\n",
      " 25%|██▌       | 9157/36617 [00:18<00:59, 461.81it/s]\u001b[A\n",
      " 25%|██▌       | 9207/36617 [00:18<00:58, 470.14it/s]\u001b[A\n",
      " 25%|██▌       | 9256/36617 [00:18<00:57, 475.81it/s]\u001b[A\n",
      " 25%|██▌       | 9306/36617 [00:18<00:56, 481.59it/s]\u001b[A\n",
      " 26%|██▌       | 9359/36617 [00:18<00:55, 493.78it/s]\u001b[A\n",
      " 26%|██▌       | 9412/36617 [00:18<00:54, 501.59it/s]\u001b[A\n",
      " 26%|██▌       | 9463/36617 [00:18<00:54, 497.55it/s]\u001b[A\n",
      " 26%|██▌       | 9513/36617 [00:19<00:55, 486.28it/s]\u001b[A\n",
      " 26%|██▌       | 9565/36617 [00:19<00:54, 494.91it/s]\u001b[A\n",
      " 26%|██▋       | 9615/36617 [00:19<00:54, 492.47it/s]\u001b[A\n",
      " 26%|██▋       | 9667/36617 [00:19<00:54, 498.22it/s]\u001b[A\n",
      " 27%|██▋       | 9721/36617 [00:19<00:53, 507.45it/s]\u001b[A\n",
      " 27%|██▋       | 9773/36617 [00:19<00:52, 509.33it/s]\u001b[A\n",
      " 27%|██▋       | 9825/36617 [00:19<00:52, 509.38it/s]\u001b[A\n",
      " 27%|██▋       | 9876/36617 [00:19<00:52, 505.04it/s]\u001b[A\n",
      " 27%|██▋       | 9927/36617 [00:19<00:53, 498.68it/s]\u001b[A\n",
      " 27%|██▋       | 9978/36617 [00:19<00:53, 500.21it/s]\u001b[A\n",
      " 27%|██▋       | 10029/36617 [00:20<00:54, 485.34it/s]\u001b[A\n",
      " 28%|██▊       | 10078/36617 [00:20<00:55, 474.39it/s]\u001b[A\n",
      " 28%|██▊       | 10132/36617 [00:20<00:53, 490.54it/s]\u001b[A\n",
      " 28%|██▊       | 10182/36617 [00:20<00:54, 482.06it/s]\u001b[A\n",
      " 28%|██▊       | 10233/36617 [00:20<00:53, 489.09it/s]\u001b[A\n",
      " 28%|██▊       | 10283/36617 [00:20<00:53, 489.26it/s]\u001b[A\n",
      " 28%|██▊       | 10333/36617 [00:20<00:54, 483.85it/s]\u001b[A\n",
      " 28%|██▊       | 10383/36617 [00:20<00:53, 486.76it/s]\u001b[A\n",
      " 28%|██▊       | 10434/36617 [00:20<00:53, 491.48it/s]\u001b[A\n",
      " 29%|██▊       | 10484/36617 [00:21<00:54, 476.39it/s]\u001b[A\n",
      " 29%|██▉       | 10533/36617 [00:21<00:54, 478.94it/s]\u001b[A\n",
      " 29%|██▉       | 10583/36617 [00:21<00:53, 483.34it/s]\u001b[A\n",
      " 29%|██▉       | 10635/36617 [00:21<00:52, 491.80it/s]\u001b[A\n",
      " 29%|██▉       | 10687/36617 [00:21<00:52, 498.31it/s]\u001b[A\n",
      " 29%|██▉       | 10739/36617 [00:21<00:51, 502.89it/s]\u001b[A\n",
      " 29%|██▉       | 10790/36617 [00:21<00:52, 496.09it/s]\u001b[A\n",
      " 30%|██▉       | 10843/36617 [00:21<00:51, 504.96it/s]\u001b[A\n",
      " 30%|██▉       | 10894/36617 [00:21<00:51, 501.03it/s]\u001b[A\n",
      " 30%|██▉       | 10945/36617 [00:21<00:52, 490.30it/s]\u001b[A\n",
      " 30%|███       | 10995/36617 [00:22<00:53, 474.92it/s]\u001b[A\n",
      " 30%|███       | 11045/36617 [00:22<00:53, 481.84it/s]\u001b[A\n",
      " 30%|███       | 11094/36617 [00:22<00:53, 479.04it/s]\u001b[A\n",
      " 30%|███       | 11146/36617 [00:22<00:52, 489.34it/s]\u001b[A\n",
      " 31%|███       | 11198/36617 [00:22<00:51, 496.98it/s]\u001b[A\n",
      " 31%|███       | 11250/36617 [00:22<00:50, 502.25it/s]\u001b[A\n",
      " 31%|███       | 11301/36617 [00:22<00:50, 499.76it/s]\u001b[A\n",
      " 31%|███       | 11352/36617 [00:22<00:51, 495.17it/s]\u001b[A\n",
      " 31%|███       | 11404/36617 [00:22<00:50, 496.77it/s]\u001b[A\n",
      " 31%|███▏      | 11454/36617 [00:22<00:51, 488.01it/s]\u001b[A\n",
      " 31%|███▏      | 11504/36617 [00:23<00:51, 490.23it/s]\u001b[A\n",
      " 32%|███▏      | 11554/36617 [00:23<00:50, 491.70it/s]\u001b[A\n",
      " 32%|███▏      | 11604/36617 [00:23<00:51, 489.19it/s]\u001b[A\n",
      " 32%|███▏      | 11655/36617 [00:23<00:50, 494.10it/s]\u001b[A\n",
      " 32%|███▏      | 11705/36617 [00:23<00:52, 477.80it/s]\u001b[A\n",
      " 32%|███▏      | 11755/36617 [00:23<00:51, 484.19it/s]\u001b[A\n",
      " 32%|███▏      | 11804/36617 [00:23<00:52, 472.93it/s]\u001b[A\n",
      " 32%|███▏      | 11852/36617 [00:23<00:52, 471.17it/s]\u001b[A\n",
      " 33%|███▎      | 11901/36617 [00:23<00:51, 475.48it/s]\u001b[A\n",
      " 33%|███▎      | 11949/36617 [00:24<00:51, 475.46it/s]\u001b[A\n",
      " 33%|███▎      | 12001/36617 [00:24<00:50, 486.81it/s]\u001b[A\n",
      " 33%|███▎      | 12050/36617 [00:24<00:51, 474.93it/s]\u001b[A\n",
      " 33%|███▎      | 12104/36617 [00:24<00:50, 489.81it/s]\u001b[A\n",
      " 33%|███▎      | 12156/36617 [00:24<00:49, 498.10it/s]\u001b[A\n",
      " 33%|███▎      | 12206/36617 [00:24<00:49, 497.09it/s]\u001b[A\n",
      " 33%|███▎      | 12258/36617 [00:24<00:48, 503.02it/s]\u001b[A\n",
      " 34%|███▎      | 12311/36617 [00:24<00:47, 509.94it/s]\u001b[A\n",
      " 34%|███▍      | 12363/36617 [00:24<00:47, 509.72it/s]\u001b[A\n",
      " 34%|███▍      | 12415/36617 [00:24<00:49, 488.89it/s]\u001b[A\n",
      " 34%|███▍      | 12465/36617 [00:25<00:49, 484.04it/s]\u001b[A\n",
      " 34%|███▍      | 12515/36617 [00:25<00:49, 487.06it/s]\u001b[A\n",
      " 34%|███▍      | 12565/36617 [00:25<00:49, 490.30it/s]\u001b[A\n",
      " 34%|███▍      | 12617/36617 [00:25<00:48, 497.56it/s]\u001b[A\n",
      " 35%|███▍      | 12671/36617 [00:25<00:47, 507.18it/s]\u001b[A\n",
      " 35%|███▍      | 12722/36617 [00:25<00:48, 496.32it/s]\u001b[A\n",
      " 35%|███▍      | 12773/36617 [00:25<00:47, 498.16it/s]\u001b[A\n",
      " 35%|███▌      | 12824/36617 [00:25<00:47, 500.34it/s]\u001b[A\n",
      " 35%|███▌      | 12875/36617 [00:25<00:48, 490.55it/s]\u001b[A\n",
      " 35%|███▌      | 12925/36617 [00:25<00:48, 483.65it/s]\u001b[A\n",
      " 35%|███▌      | 12975/36617 [00:26<00:48, 488.44it/s]\u001b[A\n",
      " 36%|███▌      | 13026/36617 [00:26<00:47, 494.35it/s]\u001b[A\n",
      " 36%|███▌      | 13078/36617 [00:26<00:46, 500.86it/s]\u001b[A\n",
      " 36%|███▌      | 13129/36617 [00:26<00:47, 495.71it/s]\u001b[A\n",
      " 36%|███▌      | 13182/36617 [00:26<00:46, 504.91it/s]\u001b[A\n",
      " 36%|███▌      | 13235/36617 [00:26<00:45, 511.98it/s]\u001b[A\n",
      " 36%|███▋      | 13287/36617 [00:26<00:45, 510.94it/s]\u001b[A\n",
      " 36%|███▋      | 13340/36617 [00:26<00:45, 515.36it/s]\u001b[A\n",
      " 37%|███▋      | 13392/36617 [00:26<00:45, 505.10it/s]\u001b[A\n",
      " 37%|███▋      | 13443/36617 [00:27<00:46, 495.88it/s]\u001b[A\n",
      " 37%|███▋      | 13494/36617 [00:27<00:46, 499.14it/s]\u001b[A\n",
      " 37%|███▋      | 13547/36617 [00:27<00:45, 506.83it/s]\u001b[A\n",
      " 37%|███▋      | 13600/36617 [00:27<00:44, 512.87it/s]\u001b[A\n",
      " 37%|███▋      | 13652/36617 [00:27<00:47, 486.63it/s]\u001b[A\n",
      " 37%|███▋      | 13701/36617 [00:27<00:47, 482.67it/s]\u001b[A\n",
      " 38%|███▊      | 13750/36617 [00:27<00:47, 481.40it/s]\u001b[A\n",
      " 38%|███▊      | 13803/36617 [00:27<00:46, 494.49it/s]\u001b[A\n",
      " 38%|███▊      | 13853/36617 [00:27<00:45, 494.94it/s]\u001b[A\n",
      " 38%|███▊      | 13903/36617 [00:27<00:45, 493.84it/s]\u001b[A\n",
      " 38%|███▊      | 13953/36617 [00:28<00:48, 464.74it/s]\u001b[A\n",
      " 38%|███▊      | 14002/36617 [00:28<00:48, 470.62it/s]\u001b[A\n",
      " 38%|███▊      | 14051/36617 [00:28<00:47, 474.61it/s]\u001b[A\n",
      " 39%|███▊      | 14103/36617 [00:28<00:46, 487.08it/s]\u001b[A\n",
      " 39%|███▊      | 14155/36617 [00:28<00:45, 494.55it/s]\u001b[A\n",
      " 39%|███▉      | 14205/36617 [00:28<00:46, 478.58it/s]\u001b[A\n",
      " 39%|███▉      | 14254/36617 [00:28<00:47, 468.76it/s]\u001b[A\n",
      " 39%|███▉      | 14302/36617 [00:28<00:47, 472.07it/s]\u001b[A\n",
      " 39%|███▉      | 14350/36617 [00:28<00:47, 471.58it/s]\u001b[A\n",
      " 39%|███▉      | 14398/36617 [00:28<00:47, 463.03it/s]\u001b[A\n",
      " 39%|███▉      | 14445/36617 [00:29<00:48, 456.35it/s]\u001b[A\n",
      " 40%|███▉      | 14497/36617 [00:29<00:46, 472.67it/s]\u001b[A\n",
      " 40%|███▉      | 14546/36617 [00:29<00:46, 477.03it/s]\u001b[A\n",
      " 40%|███▉      | 14596/36617 [00:29<00:45, 482.50it/s]\u001b[A\n",
      " 40%|████      | 14647/36617 [00:29<00:44, 489.70it/s]\u001b[A\n",
      " 40%|████      | 14697/36617 [00:29<00:45, 485.10it/s]\u001b[A\n",
      " 40%|████      | 14748/36617 [00:29<00:44, 491.02it/s]\u001b[A\n",
      " 40%|████      | 14799/36617 [00:29<00:44, 494.69it/s]\u001b[A\n",
      " 41%|████      | 14850/36617 [00:29<00:43, 498.87it/s]\u001b[A\n",
      " 41%|████      | 14900/36617 [00:30<00:44, 487.41it/s]\u001b[A\n",
      " 41%|████      | 14949/36617 [00:30<00:46, 470.74it/s]\u001b[A\n",
      " 41%|████      | 15000/36617 [00:30<00:45, 479.88it/s]\u001b[A\n",
      " 41%|████      | 15049/36617 [00:30<00:45, 478.63it/s]\u001b[A\n",
      " 41%|████      | 15100/36617 [00:30<00:44, 486.97it/s]\u001b[A\n",
      " 41%|████▏     | 15153/36617 [00:30<00:43, 498.28it/s]\u001b[A\n",
      " 42%|████▏     | 15203/36617 [00:30<00:43, 494.03it/s]\u001b[A\n",
      " 42%|████▏     | 15253/36617 [00:30<00:43, 493.01it/s]\u001b[A\n",
      " 42%|████▏     | 15303/36617 [00:30<00:43, 495.00it/s]\u001b[A\n",
      " 42%|████▏     | 15354/36617 [00:30<00:42, 496.97it/s]\u001b[A\n",
      " 42%|████▏     | 15404/36617 [00:31<00:44, 473.17it/s]\u001b[A\n",
      " 42%|████▏     | 15452/36617 [00:31<00:44, 470.46it/s]\u001b[A\n",
      " 42%|████▏     | 15505/36617 [00:31<00:43, 485.58it/s]\u001b[A\n",
      " 42%|████▏     | 15558/36617 [00:31<00:42, 496.45it/s]\u001b[A\n",
      " 43%|████▎     | 15610/36617 [00:31<00:41, 502.60it/s]\u001b[A\n",
      " 43%|████▎     | 15661/36617 [00:31<00:42, 495.16it/s]\u001b[A\n",
      " 43%|████▎     | 15713/36617 [00:31<00:41, 501.58it/s]\u001b[A\n",
      " 43%|████▎     | 15765/36617 [00:31<00:41, 505.94it/s]\u001b[A\n",
      " 43%|████▎     | 15818/36617 [00:31<00:40, 511.50it/s]\u001b[A\n",
      " 43%|████▎     | 15870/36617 [00:31<00:40, 511.74it/s]\u001b[A\n",
      " 43%|████▎     | 15922/36617 [00:32<00:41, 494.61it/s]\u001b[A\n",
      " 44%|████▎     | 15972/36617 [00:32<00:42, 489.19it/s]\u001b[A\n",
      " 44%|████▍     | 16022/36617 [00:32<00:42, 479.06it/s]\u001b[A\n",
      " 44%|████▍     | 16073/36617 [00:32<00:42, 486.03it/s]\u001b[A\n",
      " 44%|████▍     | 16128/36617 [00:32<00:40, 503.09it/s]\u001b[A\n",
      " 44%|████▍     | 16180/36617 [00:32<00:40, 507.41it/s]\u001b[A\n",
      " 44%|████▍     | 16231/36617 [00:32<00:40, 506.10it/s]\u001b[A\n",
      " 44%|████▍     | 16282/36617 [00:32<00:40, 503.13it/s]\u001b[A\n",
      " 45%|████▍     | 16333/36617 [00:32<00:42, 482.03it/s]\u001b[A\n",
      " 45%|████▍     | 16382/36617 [00:33<00:43, 468.87it/s]\u001b[A\n",
      " 45%|████▍     | 16431/36617 [00:33<00:42, 473.55it/s]\u001b[A\n",
      " 45%|████▌     | 16479/36617 [00:33<00:44, 457.41it/s]\u001b[A\n",
      " 45%|████▌     | 16529/36617 [00:33<00:42, 468.98it/s]\u001b[A\n",
      " 45%|████▌     | 16579/36617 [00:33<00:42, 476.91it/s]\u001b[A\n",
      " 45%|████▌     | 16627/36617 [00:33<00:42, 475.58it/s]\u001b[A\n",
      " 46%|████▌     | 16675/36617 [00:33<00:41, 476.53it/s]\u001b[A\n",
      " 46%|████▌     | 16723/36617 [00:33<00:41, 475.82it/s]\u001b[A\n",
      " 46%|████▌     | 16775/36617 [00:33<00:40, 486.50it/s]\u001b[A\n",
      " 46%|████▌     | 16825/36617 [00:33<00:40, 488.67it/s]\u001b[A\n",
      " 46%|████▌     | 16874/36617 [00:34<00:42, 463.47it/s]\u001b[A\n",
      " 46%|████▌     | 16921/36617 [00:34<00:42, 462.83it/s]\u001b[A\n",
      " 46%|████▋     | 16968/36617 [00:34<00:43, 451.55it/s]\u001b[A\n",
      " 46%|████▋     | 17019/36617 [00:34<00:41, 467.29it/s]\u001b[A\n",
      " 47%|████▋     | 17071/36617 [00:34<00:40, 479.91it/s]\u001b[A\n",
      " 47%|████▋     | 17123/36617 [00:34<00:39, 488.84it/s]\u001b[A\n",
      " 47%|████▋     | 17173/36617 [00:34<00:39, 491.07it/s]\u001b[A\n",
      " 47%|████▋     | 17223/36617 [00:34<00:40, 480.99it/s]\u001b[A\n",
      " 47%|████▋     | 17272/36617 [00:34<00:41, 468.02it/s]\u001b[A\n",
      " 47%|████▋     | 17320/36617 [00:35<00:41, 469.35it/s]\u001b[A\n",
      " 47%|████▋     | 17368/36617 [00:35<00:40, 470.05it/s]\u001b[A\n",
      " 48%|████▊     | 17417/36617 [00:35<00:40, 474.05it/s]\u001b[A\n",
      " 48%|████▊     | 17467/36617 [00:35<00:39, 480.42it/s]\u001b[A\n",
      " 48%|████▊     | 17517/36617 [00:35<00:39, 484.92it/s]\u001b[A\n",
      " 48%|████▊     | 17570/36617 [00:35<00:38, 495.13it/s]\u001b[A\n",
      " 48%|████▊     | 17622/36617 [00:35<00:37, 502.15it/s]\u001b[A\n",
      " 48%|████▊     | 17673/36617 [00:35<00:38, 497.23it/s]\u001b[A\n",
      " 48%|████▊     | 17725/36617 [00:35<00:37, 501.89it/s]\u001b[A\n",
      " 49%|████▊     | 17776/36617 [00:35<00:37, 501.18it/s]\u001b[A\n",
      " 49%|████▊     | 17827/36617 [00:36<00:39, 479.92it/s]\u001b[A\n",
      " 49%|████▉     | 17876/36617 [00:36<00:39, 468.70it/s]\u001b[A\n",
      " 49%|████▉     | 17924/36617 [00:36<00:39, 471.92it/s]\u001b[A\n",
      " 49%|████▉     | 17977/36617 [00:36<00:38, 487.28it/s]\u001b[A\n",
      " 49%|████▉     | 18027/36617 [00:36<00:37, 490.77it/s]\u001b[A\n",
      " 49%|████▉     | 18077/36617 [00:36<00:38, 486.32it/s]\u001b[A\n",
      " 50%|████▉     | 18126/36617 [00:36<00:37, 486.84it/s]\u001b[A\n",
      " 50%|████▉     | 18175/36617 [00:36<00:39, 472.26it/s]\u001b[A\n",
      " 50%|████▉     | 18226/36617 [00:36<00:38, 482.67it/s]\u001b[A\n",
      " 50%|████▉     | 18277/36617 [00:36<00:37, 490.37it/s]\u001b[A\n",
      " 50%|█████     | 18327/36617 [00:37<00:38, 476.45it/s]\u001b[A\n",
      " 50%|█████     | 18375/36617 [00:37<00:39, 464.25it/s]\u001b[A\n",
      " 50%|█████     | 18428/36617 [00:37<00:37, 482.06it/s]\u001b[A\n",
      " 50%|█████     | 18483/36617 [00:37<00:36, 499.83it/s]\u001b[A\n",
      " 51%|█████     | 18536/36617 [00:37<00:35, 507.00it/s]\u001b[A\n",
      " 51%|█████     | 18587/36617 [00:37<00:36, 495.75it/s]\u001b[A\n",
      " 51%|█████     | 18637/36617 [00:37<00:36, 487.49it/s]\u001b[A\n",
      " 51%|█████     | 18686/36617 [00:37<00:36, 487.80it/s]\u001b[A\n",
      " 51%|█████     | 18736/36617 [00:37<00:36, 490.46it/s]\u001b[A\n",
      " 51%|█████▏    | 18786/36617 [00:38<00:37, 480.35it/s]\u001b[A\n",
      " 51%|█████▏    | 18835/36617 [00:38<00:38, 463.95it/s]\u001b[A\n",
      " 52%|█████▏    | 18887/36617 [00:38<00:37, 477.96it/s]\u001b[A\n",
      " 52%|█████▏    | 18939/36617 [00:38<00:36, 489.15it/s]\u001b[A\n",
      " 52%|█████▏    | 18991/36617 [00:38<00:35, 496.20it/s]\u001b[A\n",
      " 52%|█████▏    | 19041/36617 [00:38<00:35, 493.93it/s]\u001b[A\n",
      " 52%|█████▏    | 19091/36617 [00:38<00:35, 492.86it/s]\u001b[A\n",
      " 52%|█████▏    | 19142/36617 [00:38<00:35, 496.15it/s]\u001b[A\n",
      " 52%|█████▏    | 19194/36617 [00:38<00:34, 501.51it/s]\u001b[A\n",
      " 53%|█████▎    | 19245/36617 [00:38<00:35, 490.60it/s]\u001b[A\n",
      " 53%|█████▎    | 19295/36617 [00:39<00:35, 486.61it/s]\u001b[A\n",
      " 53%|█████▎    | 19344/36617 [00:39<00:35, 484.76it/s]\u001b[A\n",
      " 53%|█████▎    | 19395/36617 [00:39<00:35, 491.80it/s]\u001b[A\n",
      " 53%|█████▎    | 19446/36617 [00:39<00:34, 495.10it/s]\u001b[A\n",
      " 53%|█████▎    | 19497/36617 [00:39<00:34, 497.08it/s]\u001b[A\n",
      " 53%|█████▎    | 19548/36617 [00:39<00:34, 497.92it/s]\u001b[A\n",
      " 54%|█████▎    | 19598/36617 [00:39<00:34, 492.36it/s]\u001b[A\n",
      " 54%|█████▎    | 19650/36617 [00:39<00:34, 498.78it/s]\u001b[A\n",
      " 54%|█████▍    | 19701/36617 [00:39<00:33, 499.86it/s]\u001b[A\n",
      " 54%|█████▍    | 19752/36617 [00:39<00:35, 480.18it/s]\u001b[A\n",
      " 54%|█████▍    | 19803/36617 [00:40<00:34, 486.40it/s]\u001b[A\n",
      " 54%|█████▍    | 19852/36617 [00:40<00:35, 475.17it/s]\u001b[A\n",
      " 54%|█████▍    | 19901/36617 [00:40<00:34, 479.04it/s]\u001b[A\n",
      " 54%|█████▍    | 19955/36617 [00:40<00:33, 493.69it/s]\u001b[A\n",
      " 55%|█████▍    | 20011/36617 [00:40<00:32, 509.74it/s]\u001b[A\n",
      " 55%|█████▍    | 20064/36617 [00:40<00:32, 514.61it/s]\u001b[A\n",
      " 55%|█████▍    | 20116/36617 [00:40<00:32, 511.58it/s]\u001b[A\n",
      " 55%|█████▌    | 20168/36617 [00:40<00:33, 494.89it/s]\u001b[A\n",
      " 55%|█████▌    | 20218/36617 [00:40<00:33, 493.91it/s]\u001b[A\n",
      " 55%|█████▌    | 20268/36617 [00:41<00:33, 484.95it/s]\u001b[A\n",
      " 55%|█████▌    | 20317/36617 [00:41<00:34, 479.38it/s]\u001b[A\n",
      " 56%|█████▌    | 20366/36617 [00:41<00:34, 472.11it/s]\u001b[A\n",
      " 56%|█████▌    | 20416/36617 [00:41<00:33, 478.58it/s]\u001b[A\n",
      " 56%|█████▌    | 20464/36617 [00:41<00:33, 478.66it/s]\u001b[A\n",
      " 56%|█████▌    | 20514/36617 [00:41<00:33, 484.07it/s]\u001b[A\n",
      " 56%|█████▌    | 20563/36617 [00:41<00:33, 479.16it/s]\u001b[A\n",
      " 56%|█████▋    | 20611/36617 [00:41<00:33, 475.74it/s]\u001b[A\n",
      " 56%|█████▋    | 20661/36617 [00:41<00:33, 480.39it/s]\u001b[A\n",
      " 57%|█████▋    | 20711/36617 [00:41<00:32, 484.49it/s]\u001b[A\n",
      " 57%|█████▋    | 20760/36617 [00:42<00:34, 462.02it/s]\u001b[A\n",
      " 57%|█████▋    | 20811/36617 [00:42<00:33, 472.99it/s]\u001b[A\n",
      " 57%|█████▋    | 20865/36617 [00:42<00:32, 489.97it/s]\u001b[A\n",
      " 57%|█████▋    | 20915/36617 [00:42<00:32, 490.12it/s]\u001b[A\n",
      " 57%|█████▋    | 20967/36617 [00:42<00:31, 496.67it/s]\u001b[A\n",
      " 57%|█████▋    | 21017/36617 [00:42<00:31, 495.75it/s]\u001b[A\n",
      " 58%|█████▊    | 21070/36617 [00:42<00:30, 504.95it/s]\u001b[A\n",
      " 58%|█████▊    | 21123/36617 [00:42<00:30, 510.11it/s]\u001b[A\n",
      " 58%|█████▊    | 21175/36617 [00:42<00:30, 505.24it/s]\u001b[A\n",
      " 58%|█████▊    | 21226/36617 [00:43<00:31, 494.77it/s]\u001b[A\n",
      " 58%|█████▊    | 21276/36617 [00:43<00:31, 493.37it/s]\u001b[A\n",
      " 58%|█████▊    | 21326/36617 [00:43<00:31, 491.73it/s]\u001b[A\n",
      " 58%|█████▊    | 21379/36617 [00:43<00:30, 500.38it/s]\u001b[A\n",
      " 59%|█████▊    | 21430/36617 [00:43<00:30, 499.13it/s]\u001b[A\n",
      " 59%|█████▊    | 21481/36617 [00:43<00:30, 502.02it/s]\u001b[A\n",
      " 59%|█████▉    | 21532/36617 [00:43<00:30, 499.70it/s]\u001b[A\n",
      " 59%|█████▉    | 21584/36617 [00:43<00:29, 503.56it/s]\u001b[A\n",
      " 59%|█████▉    | 21638/36617 [00:43<00:29, 511.48it/s]\u001b[A\n",
      " 59%|█████▉    | 21690/36617 [00:43<00:29, 500.43it/s]\u001b[A\n",
      " 59%|█████▉    | 21741/36617 [00:44<00:30, 489.70it/s]\u001b[A\n",
      " 60%|█████▉    | 21791/36617 [00:44<00:30, 485.98it/s]\u001b[A\n",
      " 60%|█████▉    | 21840/36617 [00:44<00:30, 484.75it/s]\u001b[A\n",
      " 60%|█████▉    | 21889/36617 [00:44<00:30, 483.66it/s]\u001b[A\n",
      " 60%|█████▉    | 21938/36617 [00:44<00:30, 480.39it/s]\u001b[A\n",
      " 60%|██████    | 21988/36617 [00:44<00:30, 485.10it/s]\u001b[A\n",
      " 60%|██████    | 22040/36617 [00:44<00:29, 493.97it/s]\u001b[A\n",
      " 60%|██████    | 22090/36617 [00:44<00:29, 492.49it/s]\u001b[A\n",
      " 60%|██████    | 22143/36617 [00:44<00:28, 501.99it/s]\u001b[A\n",
      " 61%|██████    | 22194/36617 [00:44<00:28, 500.42it/s]\u001b[A\n",
      " 61%|██████    | 22245/36617 [00:45<00:29, 486.00it/s]\u001b[A\n",
      " 61%|██████    | 22295/36617 [00:45<00:29, 488.49it/s]\u001b[A\n",
      " 61%|██████    | 22345/36617 [00:45<00:29, 491.88it/s]\u001b[A\n",
      " 61%|██████    | 22397/36617 [00:45<00:28, 498.08it/s]\u001b[A\n",
      " 61%|██████▏   | 22449/36617 [00:45<00:28, 504.10it/s]\u001b[A\n",
      " 61%|██████▏   | 22500/36617 [00:45<00:27, 505.40it/s]\u001b[A\n",
      " 62%|██████▏   | 22551/36617 [00:45<00:28, 501.04it/s]\u001b[A\n",
      " 62%|██████▏   | 22602/36617 [00:45<00:28, 496.82it/s]\u001b[A\n",
      " 62%|██████▏   | 22652/36617 [00:45<00:28, 495.16it/s]\u001b[A\n",
      " 62%|██████▏   | 22702/36617 [00:45<00:28, 496.20it/s]\u001b[A\n",
      " 62%|██████▏   | 22752/36617 [00:46<00:28, 482.11it/s]\u001b[A\n",
      " 62%|██████▏   | 22804/36617 [00:46<00:28, 491.58it/s]\u001b[A\n",
      " 62%|██████▏   | 22854/36617 [00:46<00:28, 489.78it/s]\u001b[A\n",
      " 63%|██████▎   | 22908/36617 [00:46<00:27, 501.88it/s]\u001b[A\n",
      " 63%|██████▎   | 22959/36617 [00:46<00:27, 498.33it/s]\u001b[A\n",
      " 63%|██████▎   | 23009/36617 [00:46<00:27, 497.55it/s]\u001b[A\n",
      " 63%|██████▎   | 23059/36617 [00:46<00:27, 497.12it/s]\u001b[A\n",
      " 63%|██████▎   | 23113/36617 [00:46<00:26, 508.12it/s]\u001b[A\n",
      " 63%|██████▎   | 23165/36617 [00:46<00:26, 510.96it/s]\u001b[A\n",
      " 63%|██████▎   | 23217/36617 [00:47<00:26, 504.16it/s]\u001b[A\n",
      " 64%|██████▎   | 23268/36617 [00:47<00:28, 467.26it/s]\u001b[A\n",
      " 64%|██████▎   | 23316/36617 [00:47<00:28, 466.47it/s]\u001b[A\n",
      " 64%|██████▍   | 23369/36617 [00:47<00:27, 482.34it/s]\u001b[A\n",
      " 64%|██████▍   | 23422/36617 [00:47<00:26, 494.96it/s]\u001b[A\n",
      " 64%|██████▍   | 23472/36617 [00:47<00:26, 490.19it/s]\u001b[A\n",
      " 64%|██████▍   | 23523/36617 [00:47<00:26, 494.88it/s]\u001b[A\n",
      " 64%|██████▍   | 23573/36617 [00:47<00:26, 486.22it/s]\u001b[A\n",
      " 65%|██████▍   | 23625/36617 [00:47<00:26, 493.76it/s]\u001b[A\n",
      " 65%|██████▍   | 23675/36617 [00:47<00:27, 478.35it/s]\u001b[A\n",
      " 65%|██████▍   | 23726/36617 [00:48<00:26, 486.28it/s]\u001b[A\n",
      " 65%|██████▍   | 23777/36617 [00:48<00:26, 492.63it/s]\u001b[A\n",
      " 65%|██████▌   | 23827/36617 [00:48<00:26, 477.76it/s]\u001b[A\n",
      " 65%|██████▌   | 23877/36617 [00:48<00:26, 482.97it/s]\u001b[A\n",
      " 65%|██████▌   | 23928/36617 [00:48<00:25, 489.97it/s]\u001b[A\n",
      " 65%|██████▌   | 23979/36617 [00:48<00:25, 493.37it/s]\u001b[A\n",
      " 66%|██████▌   | 24031/36617 [00:48<00:25, 499.56it/s]\u001b[A\n",
      " 66%|██████▌   | 24082/36617 [00:48<00:25, 491.64it/s]\u001b[A\n",
      " 66%|██████▌   | 24135/36617 [00:48<00:24, 500.74it/s]\u001b[A\n",
      " 66%|██████▌   | 24186/36617 [00:48<00:25, 495.16it/s]\u001b[A\n",
      " 66%|██████▌   | 24236/36617 [00:49<00:25, 484.15it/s]\u001b[A\n",
      " 66%|██████▋   | 24285/36617 [00:49<00:25, 480.57it/s]\u001b[A\n",
      " 66%|██████▋   | 24334/36617 [00:49<00:26, 467.45it/s]\u001b[A\n",
      " 67%|██████▋   | 24386/36617 [00:49<00:25, 480.08it/s]\u001b[A\n",
      " 67%|██████▋   | 24439/36617 [00:49<00:24, 492.21it/s]\u001b[A\n",
      " 67%|██████▋   | 24489/36617 [00:49<00:24, 489.80it/s]\u001b[A\n",
      " 67%|██████▋   | 24539/36617 [00:49<00:24, 491.92it/s]\u001b[A\n",
      " 67%|██████▋   | 24589/36617 [00:49<00:24, 486.81it/s]\u001b[A\n",
      " 67%|██████▋   | 24638/36617 [00:49<00:24, 480.80it/s]\u001b[A\n",
      " 67%|██████▋   | 24687/36617 [00:50<00:25, 466.36it/s]\u001b[A\n",
      " 68%|██████▊   | 24734/36617 [00:50<00:26, 453.75it/s]\u001b[A\n",
      " 68%|██████▊   | 24780/36617 [00:50<00:26, 445.92it/s]\u001b[A\n",
      " 68%|██████▊   | 24829/36617 [00:50<00:25, 456.41it/s]\u001b[A\n",
      " 68%|██████▊   | 24875/36617 [00:50<00:25, 453.00it/s]\u001b[A\n",
      " 68%|██████▊   | 24922/36617 [00:50<00:25, 456.81it/s]\u001b[A\n",
      " 68%|██████▊   | 24971/36617 [00:50<00:25, 465.09it/s]\u001b[A\n",
      " 68%|██████▊   | 25018/36617 [00:50<00:24, 465.58it/s]\u001b[A\n",
      " 68%|██████▊   | 25068/36617 [00:50<00:24, 475.38it/s]\u001b[A\n",
      " 69%|██████▊   | 25116/36617 [00:50<00:24, 467.16it/s]\u001b[A\n",
      " 69%|██████▊   | 25163/36617 [00:51<00:24, 464.78it/s]\u001b[A\n",
      " 69%|██████▉   | 25212/36617 [00:51<00:24, 470.75it/s]\u001b[A\n",
      " 69%|██████▉   | 25260/36617 [00:51<00:24, 470.02it/s]\u001b[A\n",
      " 69%|██████▉   | 25309/36617 [00:51<00:23, 475.75it/s]\u001b[A\n",
      " 69%|██████▉   | 25358/36617 [00:51<00:23, 478.69it/s]\u001b[A\n",
      " 69%|██████▉   | 25410/36617 [00:51<00:22, 489.32it/s]\u001b[A\n",
      " 70%|██████▉   | 25462/36617 [00:51<00:22, 495.92it/s]\u001b[A\n",
      " 70%|██████▉   | 25512/36617 [00:51<00:22, 486.98it/s]\u001b[A\n",
      " 70%|██████▉   | 25561/36617 [00:51<00:22, 484.80it/s]\u001b[A\n",
      " 70%|██████▉   | 25610/36617 [00:52<00:22, 484.10it/s]\u001b[A\n",
      " 70%|███████   | 25659/36617 [00:52<00:22, 477.37it/s]\u001b[A\n",
      " 70%|███████   | 25708/36617 [00:52<00:22, 480.36it/s]\u001b[A\n",
      " 70%|███████   | 25757/36617 [00:52<00:22, 478.97it/s]\u001b[A\n",
      " 70%|███████   | 25810/36617 [00:52<00:21, 492.63it/s]\u001b[A\n",
      " 71%|███████   | 25860/36617 [00:52<00:22, 488.46it/s]\u001b[A\n",
      " 71%|███████   | 25909/36617 [00:52<00:22, 483.23it/s]\u001b[A\n",
      " 71%|███████   | 25961/36617 [00:52<00:21, 492.47it/s]\u001b[A\n",
      " 71%|███████   | 26011/36617 [00:52<00:21, 490.62it/s]\u001b[A\n",
      " 71%|███████   | 26062/36617 [00:52<00:21, 495.34it/s]\u001b[A\n",
      " 71%|███████▏  | 26112/36617 [00:53<00:21, 490.07it/s]\u001b[A\n",
      " 71%|███████▏  | 26162/36617 [00:53<00:21, 479.38it/s]\u001b[A\n",
      " 72%|███████▏  | 26211/36617 [00:53<00:21, 480.64it/s]\u001b[A\n",
      " 72%|███████▏  | 26260/36617 [00:53<00:21, 472.83it/s]\u001b[A\n",
      " 72%|███████▏  | 26308/36617 [00:53<00:22, 461.33it/s]\u001b[A\n",
      " 72%|███████▏  | 26358/36617 [00:53<00:21, 471.78it/s]\u001b[A\n",
      " 72%|███████▏  | 26407/36617 [00:53<00:21, 475.93it/s]\u001b[A\n",
      " 72%|███████▏  | 26455/36617 [00:53<00:21, 475.74it/s]\u001b[A\n",
      " 72%|███████▏  | 26503/36617 [00:53<00:21, 466.48it/s]\u001b[A\n",
      " 73%|███████▎  | 26553/36617 [00:53<00:21, 473.60it/s]\u001b[A\n",
      " 73%|███████▎  | 26601/36617 [00:54<00:21, 474.83it/s]\u001b[A\n",
      " 73%|███████▎  | 26649/36617 [00:54<00:21, 474.60it/s]\u001b[A\n",
      " 73%|███████▎  | 26701/36617 [00:54<00:20, 485.28it/s]\u001b[A\n",
      " 73%|███████▎  | 26750/36617 [00:54<00:20, 478.87it/s]\u001b[A\n",
      " 73%|███████▎  | 26798/36617 [00:54<00:20, 478.48it/s]\u001b[A\n",
      " 73%|███████▎  | 26848/36617 [00:54<00:20, 483.24it/s]\u001b[A\n",
      " 73%|███████▎  | 26897/36617 [00:54<00:20, 483.57it/s]\u001b[A\n",
      " 74%|███████▎  | 26946/36617 [00:54<00:20, 480.29it/s]\u001b[A\n",
      " 74%|███████▎  | 26996/36617 [00:54<00:19, 483.51it/s]\u001b[A\n",
      " 74%|███████▍  | 27046/36617 [00:54<00:19, 486.45it/s]\u001b[A\n",
      " 74%|███████▍  | 27095/36617 [00:55<00:20, 473.78it/s]\u001b[A\n",
      " 74%|███████▍  | 27144/36617 [00:55<00:19, 476.54it/s]\u001b[A\n",
      " 74%|███████▍  | 27194/36617 [00:55<00:19, 481.55it/s]\u001b[A\n",
      " 74%|███████▍  | 27243/36617 [00:55<00:19, 479.93it/s]\u001b[A\n",
      " 75%|███████▍  | 27294/36617 [00:55<00:19, 486.43it/s]\u001b[A\n",
      " 75%|███████▍  | 27347/36617 [00:55<00:18, 497.88it/s]\u001b[A\n",
      " 75%|███████▍  | 27397/36617 [00:55<00:18, 493.43it/s]\u001b[A\n",
      " 75%|███████▍  | 27447/36617 [00:55<00:18, 485.60it/s]\u001b[A\n",
      " 75%|███████▌  | 27496/36617 [00:55<00:18, 486.88it/s]\u001b[A\n",
      " 75%|███████▌  | 27545/36617 [00:56<00:18, 482.39it/s]\u001b[A\n",
      " 75%|███████▌  | 27594/36617 [00:56<00:18, 482.19it/s]\u001b[A\n",
      " 75%|███████▌  | 27644/36617 [00:56<00:18, 486.43it/s]\u001b[A\n",
      " 76%|███████▌  | 27695/36617 [00:56<00:18, 491.21it/s]\u001b[A\n",
      " 76%|███████▌  | 27745/36617 [00:56<00:18, 484.62it/s]\u001b[A\n",
      " 76%|███████▌  | 27794/36617 [00:56<00:18, 485.94it/s]\u001b[A\n",
      " 76%|███████▌  | 27846/36617 [00:56<00:17, 494.75it/s]\u001b[A\n",
      " 76%|███████▌  | 27897/36617 [00:56<00:17, 497.98it/s]\u001b[A\n",
      " 76%|███████▋  | 27949/36617 [00:56<00:17, 503.78it/s]\u001b[A\n",
      " 76%|███████▋  | 28000/36617 [00:56<00:17, 491.11it/s]\u001b[A\n",
      " 77%|███████▋  | 28050/36617 [00:57<00:17, 489.00it/s]\u001b[A\n",
      " 77%|███████▋  | 28099/36617 [00:57<00:17, 488.97it/s]\u001b[A\n",
      " 77%|███████▋  | 28148/36617 [00:57<00:17, 487.80it/s]\u001b[A\n",
      " 77%|███████▋  | 28197/36617 [00:57<00:17, 484.77it/s]\u001b[A\n",
      " 77%|███████▋  | 28247/36617 [00:57<00:17, 487.89it/s]\u001b[A\n",
      " 77%|███████▋  | 28298/36617 [00:57<00:16, 492.34it/s]\u001b[A\n",
      " 77%|███████▋  | 28348/36617 [00:57<00:16, 490.18it/s]\u001b[A\n",
      " 78%|███████▊  | 28398/36617 [00:57<00:17, 479.18it/s]\u001b[A\n",
      " 78%|███████▊  | 28446/36617 [00:57<00:17, 469.20it/s]\u001b[A\n",
      " 78%|███████▊  | 28494/36617 [00:57<00:17, 471.06it/s]\u001b[A\n",
      " 78%|███████▊  | 28542/36617 [00:58<00:17, 456.53it/s]\u001b[A\n",
      " 78%|███████▊  | 28588/36617 [00:58<00:17, 447.61it/s]\u001b[A\n",
      " 78%|███████▊  | 28638/36617 [00:58<00:17, 460.10it/s]\u001b[A\n",
      " 78%|███████▊  | 28688/36617 [00:58<00:16, 470.96it/s]\u001b[A\n",
      " 78%|███████▊  | 28738/36617 [00:58<00:16, 477.68it/s]\u001b[A\n",
      " 79%|███████▊  | 28788/36617 [00:58<00:16, 482.46it/s]\u001b[A\n",
      " 79%|███████▉  | 28837/36617 [00:58<00:16, 483.68it/s]\u001b[A\n",
      " 79%|███████▉  | 28886/36617 [00:58<00:16, 482.86it/s]\u001b[A\n",
      " 79%|███████▉  | 28935/36617 [00:58<00:16, 475.28it/s]\u001b[A\n",
      " 79%|███████▉  | 28983/36617 [00:59<00:16, 475.73it/s]\u001b[A\n",
      " 79%|███████▉  | 29033/36617 [00:59<00:15, 481.27it/s]\u001b[A\n",
      " 79%|███████▉  | 29084/36617 [00:59<00:15, 487.62it/s]\u001b[A\n",
      " 80%|███████▉  | 29134/36617 [00:59<00:15, 489.62it/s]\u001b[A\n",
      " 80%|███████▉  | 29184/36617 [00:59<00:15, 487.08it/s]\u001b[A\n",
      " 80%|███████▉  | 29233/36617 [00:59<00:15, 481.74it/s]\u001b[A\n",
      " 80%|███████▉  | 29283/36617 [00:59<00:15, 486.37it/s]\u001b[A\n",
      " 80%|████████  | 29332/36617 [00:59<00:14, 486.59it/s]\u001b[A\n",
      " 80%|████████  | 29382/36617 [00:59<00:14, 489.93it/s]\u001b[A\n",
      " 80%|████████  | 29433/36617 [00:59<00:14, 493.37it/s]\u001b[A\n",
      " 81%|████████  | 29483/36617 [01:00<00:15, 474.77it/s]\u001b[A\n",
      " 81%|████████  | 29531/36617 [01:00<00:15, 464.99it/s]\u001b[A\n",
      " 81%|████████  | 29583/36617 [01:00<00:14, 479.65it/s]\u001b[A\n",
      " 81%|████████  | 29636/36617 [01:00<00:14, 491.23it/s]\u001b[A\n",
      " 81%|████████  | 29686/36617 [01:00<00:14, 486.18it/s]\u001b[A\n",
      " 81%|████████  | 29735/36617 [01:00<00:14, 486.64it/s]\u001b[A\n",
      " 81%|████████▏ | 29784/36617 [01:00<00:14, 485.66it/s]\u001b[A\n",
      " 81%|████████▏ | 29833/36617 [01:00<00:14, 479.69it/s]\u001b[A\n",
      " 82%|████████▏ | 29882/36617 [01:00<00:14, 475.91it/s]\u001b[A\n",
      " 82%|████████▏ | 29931/36617 [01:00<00:13, 478.67it/s]\u001b[A\n",
      " 82%|████████▏ | 29979/36617 [01:01<00:14, 464.13it/s]\u001b[A\n",
      " 82%|████████▏ | 30028/36617 [01:01<00:14, 470.21it/s]\u001b[A\n",
      " 82%|████████▏ | 30076/36617 [01:01<00:13, 469.54it/s]\u001b[A\n",
      " 82%|████████▏ | 30124/36617 [01:01<00:13, 464.69it/s]\u001b[A\n",
      " 82%|████████▏ | 30171/36617 [01:01<00:14, 455.44it/s]\u001b[A\n",
      " 83%|████████▎ | 30219/36617 [01:01<00:13, 459.94it/s]\u001b[A\n",
      " 83%|████████▎ | 30267/36617 [01:01<00:13, 464.65it/s]\u001b[A\n",
      " 83%|████████▎ | 30314/36617 [01:01<00:13, 464.55it/s]\u001b[A\n",
      " 83%|████████▎ | 30362/36617 [01:01<00:13, 468.03it/s]\u001b[A\n",
      " 83%|████████▎ | 30409/36617 [01:01<00:13, 467.88it/s]\u001b[A\n",
      " 83%|████████▎ | 30456/36617 [01:02<00:13, 445.92it/s]\u001b[A\n",
      " 83%|████████▎ | 30502/36617 [01:02<00:13, 449.74it/s]\u001b[A\n",
      " 83%|████████▎ | 30552/36617 [01:02<00:13, 461.61it/s]\u001b[A\n",
      " 84%|████████▎ | 30604/36617 [01:02<00:12, 475.07it/s]\u001b[A\n",
      " 84%|████████▎ | 30652/36617 [01:02<00:12, 472.56it/s]\u001b[A\n",
      " 84%|████████▍ | 30700/36617 [01:02<00:12, 471.67it/s]\u001b[A\n",
      " 84%|████████▍ | 30751/36617 [01:02<00:12, 480.09it/s]\u001b[A\n",
      " 84%|████████▍ | 30800/36617 [01:02<00:12, 481.54it/s]\u001b[A\n",
      " 84%|████████▍ | 30849/36617 [01:02<00:12, 471.47it/s]\u001b[A\n",
      " 84%|████████▍ | 30897/36617 [01:03<00:12, 466.83it/s]\u001b[A\n",
      " 85%|████████▍ | 30946/36617 [01:03<00:12, 471.79it/s]\u001b[A\n",
      " 85%|████████▍ | 30994/36617 [01:03<00:12, 458.47it/s]\u001b[A\n",
      " 85%|████████▍ | 31045/36617 [01:03<00:11, 470.90it/s]\u001b[A\n",
      " 85%|████████▍ | 31093/36617 [01:03<00:11, 472.99it/s]\u001b[A\n",
      " 85%|████████▌ | 31142/36617 [01:03<00:11, 476.92it/s]\u001b[A\n",
      " 85%|████████▌ | 31190/36617 [01:03<00:11, 473.32it/s]\u001b[A\n",
      " 85%|████████▌ | 31242/36617 [01:03<00:11, 485.72it/s]\u001b[A\n",
      " 85%|████████▌ | 31293/36617 [01:03<00:10, 491.80it/s]\u001b[A\n",
      " 86%|████████▌ | 31343/36617 [01:03<00:10, 490.53it/s]\u001b[A\n",
      " 86%|████████▌ | 31393/36617 [01:04<00:10, 489.98it/s]\u001b[A\n",
      " 86%|████████▌ | 31443/36617 [01:04<00:10, 475.83it/s]\u001b[A\n",
      " 86%|████████▌ | 31492/36617 [01:04<00:10, 478.67it/s]\u001b[A\n",
      " 86%|████████▌ | 31542/36617 [01:04<00:10, 483.74it/s]\u001b[A\n",
      " 86%|████████▋ | 31591/36617 [01:04<00:10, 469.84it/s]\u001b[A\n",
      " 86%|████████▋ | 31642/36617 [01:04<00:10, 478.52it/s]\u001b[A\n",
      " 87%|████████▋ | 31692/36617 [01:04<00:10, 483.05it/s]\u001b[A\n",
      " 87%|████████▋ | 31741/36617 [01:04<00:10, 466.95it/s]\u001b[A\n",
      " 87%|████████▋ | 31790/36617 [01:04<00:10, 472.04it/s]\u001b[A\n",
      " 87%|████████▋ | 31838/36617 [01:05<00:10, 466.56it/s]\u001b[A\n",
      " 87%|████████▋ | 31887/36617 [01:05<00:10, 470.77it/s]\u001b[A\n",
      " 87%|████████▋ | 31937/36617 [01:05<00:09, 475.82it/s]\u001b[A\n",
      " 87%|████████▋ | 31986/36617 [01:05<00:09, 479.48it/s]\u001b[A\n",
      " 87%|████████▋ | 32035/36617 [01:05<00:09, 480.13it/s]\u001b[A\n",
      " 88%|████████▊ | 32084/36617 [01:05<00:09, 466.89it/s]\u001b[A\n",
      " 88%|████████▊ | 32134/36617 [01:05<00:09, 474.06it/s]\u001b[A\n",
      " 88%|████████▊ | 32184/36617 [01:05<00:09, 480.57it/s]\u001b[A\n",
      " 88%|████████▊ | 32234/36617 [01:05<00:09, 485.58it/s]\u001b[A\n",
      " 88%|████████▊ | 32285/36617 [01:05<00:08, 491.59it/s]\u001b[A\n",
      " 88%|████████▊ | 32335/36617 [01:06<00:08, 484.72it/s]\u001b[A\n",
      " 88%|████████▊ | 32385/36617 [01:06<00:08, 488.22it/s]\u001b[A\n",
      " 89%|████████▊ | 32434/36617 [01:06<00:08, 471.12it/s]\u001b[A\n",
      " 89%|████████▊ | 32482/36617 [01:06<00:08, 465.83it/s]\u001b[A\n",
      " 89%|████████▉ | 32533/36617 [01:06<00:08, 477.71it/s]\u001b[A\n",
      " 89%|████████▉ | 32583/36617 [01:06<00:08, 481.77it/s]\u001b[A\n",
      " 89%|████████▉ | 32632/36617 [01:06<00:08, 470.82it/s]\u001b[A\n",
      " 89%|████████▉ | 32684/36617 [01:06<00:08, 481.18it/s]\u001b[A\n",
      " 89%|████████▉ | 32733/36617 [01:06<00:08, 476.81it/s]\u001b[A\n",
      " 90%|████████▉ | 32782/36617 [01:06<00:08, 478.06it/s]\u001b[A\n",
      " 90%|████████▉ | 32831/36617 [01:07<00:07, 481.47it/s]\u001b[A\n",
      " 90%|████████▉ | 32883/36617 [01:07<00:07, 490.32it/s]\u001b[A\n",
      " 90%|████████▉ | 32935/36617 [01:07<00:07, 498.56it/s]\u001b[A\n",
      " 90%|█████████ | 32985/36617 [01:07<00:07, 495.16it/s]\u001b[A\n",
      " 90%|█████████ | 33035/36617 [01:07<00:07, 472.39it/s]\u001b[A\n",
      " 90%|█████████ | 33083/36617 [01:07<00:07, 469.65it/s]\u001b[A\n",
      " 90%|█████████ | 33131/36617 [01:07<00:07, 463.77it/s]\u001b[A\n",
      " 91%|█████████ | 33178/36617 [01:07<00:07, 451.12it/s]\u001b[A\n",
      " 91%|█████████ | 33226/36617 [01:07<00:07, 457.55it/s]\u001b[A\n",
      " 91%|█████████ | 33272/36617 [01:08<00:07, 454.18it/s]\u001b[A\n",
      " 91%|█████████ | 33320/36617 [01:08<00:07, 460.82it/s]\u001b[A\n",
      " 91%|█████████ | 33367/36617 [01:08<00:07, 443.96it/s]\u001b[A\n",
      " 91%|█████████ | 33412/36617 [01:08<00:07, 428.55it/s]\u001b[A\n",
      " 91%|█████████▏| 33461/36617 [01:08<00:07, 444.72it/s]\u001b[A\n",
      " 92%|█████████▏| 33506/36617 [01:08<00:07, 440.65it/s]\u001b[A\n",
      " 92%|█████████▏| 33553/36617 [01:08<00:06, 448.93it/s]\u001b[A\n",
      " 92%|█████████▏| 33599/36617 [01:08<00:06, 450.67it/s]\u001b[A\n",
      " 92%|█████████▏| 33645/36617 [01:08<00:06, 451.70it/s]\u001b[A\n",
      " 92%|█████████▏| 33691/36617 [01:08<00:06, 446.48it/s]\u001b[A\n",
      " 92%|█████████▏| 33741/36617 [01:09<00:06, 460.21it/s]\u001b[A\n",
      " 92%|█████████▏| 33788/36617 [01:09<00:06, 462.85it/s]\u001b[A\n",
      " 92%|█████████▏| 33837/36617 [01:09<00:05, 469.04it/s]\u001b[A\n",
      " 93%|█████████▎| 33885/36617 [01:09<00:05, 464.63it/s]\u001b[A\n",
      " 93%|█████████▎| 33936/36617 [01:09<00:05, 476.67it/s]\u001b[A\n",
      " 93%|█████████▎| 33985/36617 [01:09<00:05, 478.46it/s]\u001b[A\n",
      " 93%|█████████▎| 34034/36617 [01:09<00:05, 481.42it/s]\u001b[A\n",
      " 93%|█████████▎| 34083/36617 [01:09<00:05, 478.27it/s]\u001b[A\n",
      " 93%|█████████▎| 34131/36617 [01:09<00:05, 476.59it/s]\u001b[A\n",
      " 93%|█████████▎| 34179/36617 [01:09<00:05, 468.43it/s]\u001b[A\n",
      " 93%|█████████▎| 34226/36617 [01:10<00:05, 450.62it/s]\u001b[A\n",
      " 94%|█████████▎| 34272/36617 [01:10<00:05, 448.93it/s]\u001b[A\n",
      " 94%|█████████▎| 34320/36617 [01:10<00:05, 456.14it/s]\u001b[A\n",
      " 94%|█████████▍| 34373/36617 [01:10<00:04, 474.71it/s]\u001b[A\n",
      " 94%|█████████▍| 34422/36617 [01:10<00:04, 476.87it/s]\u001b[A\n",
      " 94%|█████████▍| 34470/36617 [01:10<00:04, 471.11it/s]\u001b[A\n",
      " 94%|█████████▍| 34520/36617 [01:10<00:04, 476.93it/s]\u001b[A\n",
      " 94%|█████████▍| 34571/36617 [01:10<00:04, 486.25it/s]\u001b[A\n",
      " 95%|█████████▍| 34620/36617 [01:10<00:04, 476.89it/s]\u001b[A\n",
      " 95%|█████████▍| 34668/36617 [01:11<00:04, 476.69it/s]\u001b[A\n",
      " 95%|█████████▍| 34720/36617 [01:11<00:03, 487.64it/s]\u001b[A\n",
      " 95%|█████████▍| 34769/36617 [01:11<00:03, 484.48it/s]\u001b[A\n",
      " 95%|█████████▌| 34818/36617 [01:11<00:03, 482.21it/s]\u001b[A\n",
      " 95%|█████████▌| 34868/36617 [01:11<00:03, 487.07it/s]\u001b[A\n",
      " 95%|█████████▌| 34917/36617 [01:11<00:03, 486.78it/s]\u001b[A\n",
      " 95%|█████████▌| 34966/36617 [01:11<00:03, 481.10it/s]\u001b[A\n",
      " 96%|█████████▌| 35016/36617 [01:11<00:03, 484.89it/s]\u001b[A\n",
      " 96%|█████████▌| 35065/36617 [01:11<00:03, 485.42it/s]\u001b[A\n",
      " 96%|█████████▌| 35114/36617 [01:11<00:03, 481.90it/s]\u001b[A\n",
      " 96%|█████████▌| 35163/36617 [01:12<00:03, 468.93it/s]\u001b[A\n",
      " 96%|█████████▌| 35211/36617 [01:12<00:02, 470.36it/s]\u001b[A\n",
      " 96%|█████████▋| 35260/36617 [01:12<00:02, 476.02it/s]\u001b[A\n",
      " 96%|█████████▋| 35309/36617 [01:12<00:02, 478.30it/s]\u001b[A\n",
      " 97%|█████████▋| 35359/36617 [01:12<00:02, 482.70it/s]\u001b[A\n",
      " 97%|█████████▋| 35408/36617 [01:12<00:02, 474.10it/s]\u001b[A\n",
      " 97%|█████████▋| 35456/36617 [01:12<00:02, 463.33it/s]\u001b[A\n",
      " 97%|█████████▋| 35503/36617 [01:12<00:02, 450.08it/s]\u001b[A\n",
      " 97%|█████████▋| 35551/36617 [01:12<00:02, 458.02it/s]\u001b[A\n",
      " 97%|█████████▋| 35597/36617 [01:12<00:02, 453.38it/s]\u001b[A\n",
      " 97%|█████████▋| 35645/36617 [01:13<00:02, 459.42it/s]\u001b[A\n",
      " 97%|█████████▋| 35692/36617 [01:13<00:02, 460.98it/s]\u001b[A\n",
      " 98%|█████████▊| 35739/36617 [01:13<00:01, 453.18it/s]\u001b[A\n",
      " 98%|█████████▊| 35790/36617 [01:13<00:01, 466.73it/s]\u001b[A\n",
      " 98%|█████████▊| 35837/36617 [01:13<00:01, 467.24it/s]\u001b[A\n",
      " 98%|█████████▊| 35888/36617 [01:13<00:01, 477.65it/s]\u001b[A\n",
      " 98%|█████████▊| 35939/36617 [01:13<00:01, 485.39it/s]\u001b[A\n",
      " 98%|█████████▊| 35990/36617 [01:13<00:01, 491.56it/s]\u001b[A\n",
      " 98%|█████████▊| 36042/36617 [01:13<00:01, 497.42it/s]\u001b[A\n",
      " 99%|█████████▊| 36092/36617 [01:14<00:01, 487.93it/s]\u001b[A\n",
      " 99%|█████████▊| 36141/36617 [01:14<00:00, 480.68it/s]\u001b[A\n",
      " 99%|█████████▉| 36190/36617 [01:14<00:00, 483.20it/s]\u001b[A\n",
      " 99%|█████████▉| 36239/36617 [01:14<00:00, 481.98it/s]\u001b[A\n",
      " 99%|█████████▉| 36291/36617 [01:14<00:00, 491.12it/s]\u001b[A\n",
      " 99%|█████████▉| 36341/36617 [01:14<00:00, 489.37it/s]\u001b[A\n",
      " 99%|█████████▉| 36390/36617 [01:14<00:00, 470.19it/s]\u001b[A\n",
      "100%|█████████▉| 36438/36617 [01:14<00:00, 472.80it/s]\u001b[A\n",
      "100%|█████████▉| 36488/36617 [01:14<00:00, 479.69it/s]\u001b[A\n",
      "100%|█████████▉| 36538/36617 [01:14<00:00, 484.38it/s]\u001b[A\n",
      "100%|██████████| 36617/36617 [01:15<00:00, 487.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply find hate function in toxicity data\n",
    "hate_keyword = new_racism_word['Content']\n",
    "hate_tox_train = find_hate_sentence(tox_train, 'comment', 'toxicity', hate_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "dVST5wx6mTWc",
    "outputId": "eb20a7d7-fffc-49d5-dd5c-d1a354a65c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13273 entries, 0 to 19537\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comment   13273 non-null  object \n",
      " 1   toxicity  13273 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 311.1+ KB\n"
     ]
    }
   ],
   "source": [
    "hate_tox_train.info()\n",
    "hate_tox_train.to_csv('Data/data_after_extraction/hate_tox_train.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8SFy2xfaySsO",
    "outputId": "2ff3b44d-2026-4dbe-eb06-7e95af0dc618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/32128 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 44/32128 [00:00<01:13, 436.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 90/32128 [00:00<01:12, 442.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 141/32128 [00:00<01:09, 459.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 194/32128 [00:00<01:06, 477.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 244/32128 [00:00<01:05, 483.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 296/32128 [00:00<01:04, 493.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 350/32128 [00:00<01:02, 505.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 399/32128 [00:00<01:03, 500.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 451/32128 [00:00<01:02, 505.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 500/32128 [00:01<01:03, 498.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 551/32128 [00:01<01:03, 499.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 602/32128 [00:01<01:03, 500.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 654/32128 [00:01<01:02, 503.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 704/32128 [00:01<01:02, 500.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 756/32128 [00:01<01:02, 504.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 807/32128 [00:01<01:02, 499.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 859/32128 [00:01<01:02, 503.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 912/32128 [00:01<01:01, 508.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 967/32128 [00:01<01:00, 518.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1019/32128 [00:02<01:00, 512.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1071/32128 [00:02<01:01, 508.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1122/32128 [00:02<01:02, 498.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 1172/32128 [00:02<01:03, 489.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1223/32128 [00:02<01:02, 493.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1274/32128 [00:02<01:02, 496.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1325/32128 [00:02<01:01, 497.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1376/32128 [00:02<01:01, 499.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1427/32128 [00:02<01:02, 491.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 1477/32128 [00:02<01:02, 486.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 1526/32128 [00:03<01:03, 480.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 1575/32128 [00:03<01:06, 459.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 1622/32128 [00:03<01:06, 460.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 1673/32128 [00:03<01:04, 474.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 1723/32128 [00:03<01:03, 479.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1775/32128 [00:03<01:01, 489.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1828/32128 [00:03<01:00, 499.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1881/32128 [00:03<00:59, 506.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1934/32128 [00:03<00:59, 511.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1987/32128 [00:03<00:58, 515.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 2040/32128 [00:04<00:58, 518.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2092/32128 [00:04<01:00, 492.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2142/32128 [00:04<01:00, 494.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2192/32128 [00:04<01:00, 490.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2242/32128 [00:04<01:01, 482.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2293/32128 [00:04<01:00, 490.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2343/32128 [00:04<01:00, 491.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 2393/32128 [00:04<01:01, 486.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2442/32128 [00:04<01:01, 480.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2491/32128 [00:05<01:02, 474.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2539/32128 [00:05<01:02, 471.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2587/32128 [00:05<01:03, 465.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2638/32128 [00:05<01:02, 475.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2689/32128 [00:05<01:00, 484.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 2742/32128 [00:05<00:59, 495.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 2796/32128 [00:05<00:57, 506.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 2847/32128 [00:05<00:57, 506.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 2898/32128 [00:05<00:57, 506.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 2950/32128 [00:05<00:57, 508.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 3001/32128 [00:06<00:57, 508.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 3052/32128 [00:06<00:59, 492.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 3102/32128 [00:06<00:59, 491.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 3152/32128 [00:06<00:58, 493.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 3205/32128 [00:06<00:57, 503.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3256/32128 [00:06<00:57, 504.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3307/32128 [00:06<00:57, 498.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 3357/32128 [00:06<00:57, 496.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 3410/32128 [00:06<00:56, 505.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 3462/32128 [00:06<00:56, 509.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 3514/32128 [00:07<00:56, 508.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 3565/32128 [00:07<00:56, 501.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 3616/32128 [00:07<00:56, 503.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 3667/32128 [00:07<00:57, 492.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 3719/32128 [00:07<00:56, 500.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 3771/32128 [00:07<00:56, 504.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 3825/32128 [00:07<00:55, 512.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 3877/32128 [00:07<00:55, 510.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 3929/32128 [00:07<00:55, 506.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 3980/32128 [00:08<00:55, 505.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4031/32128 [00:08<00:55, 505.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4082/32128 [00:08<00:55, 502.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4134/32128 [00:08<00:55, 505.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4185/32128 [00:08<00:55, 500.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4236/32128 [00:08<00:55, 502.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4287/32128 [00:08<00:57, 487.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4336/32128 [00:08<00:58, 476.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 4384/32128 [00:08<00:58, 473.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 4436/32128 [00:08<00:57, 485.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 4487/32128 [00:09<00:56, 490.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 4537/32128 [00:09<00:56, 491.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 4587/32128 [00:09<00:58, 471.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 4639/32128 [00:09<00:56, 483.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 4690/32128 [00:09<00:55, 490.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 4742/32128 [00:09<00:55, 497.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 4793/32128 [00:09<00:54, 499.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 4845/32128 [00:09<00:53, 505.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 4896/32128 [00:09<00:53, 506.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 4949/32128 [00:09<00:53, 512.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 5002/32128 [00:10<00:52, 516.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 5054/32128 [00:10<00:54, 497.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 5104/32128 [00:10<00:56, 479.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 5153/32128 [00:10<00:56, 481.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 5202/32128 [00:10<00:55, 480.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 5251/32128 [00:10<00:55, 480.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5303/32128 [00:10<00:54, 491.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5353/32128 [00:10<00:56, 476.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5405/32128 [00:10<00:54, 486.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5454/32128 [00:11<00:54, 485.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5504/32128 [00:11<00:54, 489.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5554/32128 [00:11<00:55, 481.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5607/32128 [00:11<00:53, 494.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 5658/32128 [00:11<00:53, 498.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 5709/32128 [00:11<00:52, 499.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 5761/32128 [00:11<00:52, 505.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 5812/32128 [00:11<00:52, 503.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 5865/32128 [00:11<00:51, 509.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 5917/32128 [00:11<00:51, 507.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 5968/32128 [00:12<00:51, 503.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 6019/32128 [00:12<00:52, 494.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 6069/32128 [00:12<00:53, 484.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 6122/32128 [00:12<00:52, 495.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 6174/32128 [00:12<00:51, 502.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 6228/32128 [00:12<00:50, 510.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 6280/32128 [00:12<00:50, 510.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 6332/32128 [00:12<00:50, 512.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 6384/32128 [00:12<00:51, 503.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6438/32128 [00:12<00:50, 513.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 6490/32128 [00:13<00:50, 511.16it/s]\u001b[A\u001b[A\n",
      " 14%|█▍        | 4563/32128 [00:27<00:55, 493.39it/s]\u001b[A\n",
      "\n",
      " 20%|██        | 6542/32128 [00:13<00:51, 500.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 6593/32128 [00:13<00:54, 467.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 6641/32128 [00:13<00:55, 463.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 6689/32128 [00:13<00:54, 467.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 6740/32128 [00:13<00:52, 479.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 6795/32128 [00:13<00:51, 495.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 6845/32128 [00:13<00:50, 495.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 6896/32128 [00:13<00:50, 498.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 6946/32128 [00:14<00:51, 492.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 6999/32128 [00:14<00:50, 501.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 7052/32128 [00:14<00:49, 507.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 7103/32128 [00:14<00:49, 501.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 7155/32128 [00:14<00:49, 504.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 7206/32128 [00:14<00:49, 503.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7259/32128 [00:14<00:48, 508.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7310/32128 [00:14<00:49, 503.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7363/32128 [00:14<00:48, 508.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7414/32128 [00:14<00:49, 498.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7464/32128 [00:15<00:49, 495.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7514/32128 [00:15<00:49, 496.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 7564/32128 [00:15<00:50, 485.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 7613/32128 [00:15<00:51, 474.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 7661/32128 [00:15<00:51, 474.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 7713/32128 [00:15<00:50, 486.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 7762/32128 [00:15<00:50, 479.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 7813/32128 [00:15<00:49, 487.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 7862/32128 [00:15<00:49, 487.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 7914/32128 [00:15<00:48, 496.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 7964/32128 [00:16<00:48, 494.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 8014/32128 [00:16<00:49, 487.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 8063/32128 [00:16<00:50, 477.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 8111/32128 [00:16<00:50, 476.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 8161/32128 [00:16<00:49, 480.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 8210/32128 [00:16<00:50, 477.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 8258/32128 [00:16<00:51, 459.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 8305/32128 [00:16<00:51, 460.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 8353/32128 [00:16<00:51, 465.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 8402/32128 [00:17<00:50, 469.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 8452/32128 [00:17<00:49, 475.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 8500/32128 [00:17<00:52, 450.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8547/32128 [00:17<00:51, 455.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8595/32128 [00:17<00:50, 462.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8642/32128 [00:17<00:50, 462.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8689/32128 [00:17<00:50, 463.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8740/32128 [00:17<00:49, 476.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8794/32128 [00:17<00:47, 492.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 8845/32128 [00:17<00:46, 495.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 8895/32128 [00:18<00:47, 485.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 8945/32128 [00:18<00:47, 488.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 8994/32128 [00:18<00:47, 487.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 9043/32128 [00:18<00:48, 478.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 9092/32128 [00:18<00:47, 480.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 9143/32128 [00:18<00:47, 486.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 9192/32128 [00:18<00:47, 478.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 9240/32128 [00:18<00:48, 472.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 9288/32128 [00:18<00:48, 471.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 9336/32128 [00:18<00:48, 470.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 9386/32128 [00:19<00:47, 478.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 9434/32128 [00:19<00:47, 474.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 9482/32128 [00:19<00:48, 469.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 9529/32128 [00:19<00:48, 467.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 9580/32128 [00:19<00:47, 478.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 9628/32128 [00:19<00:47, 478.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9680/32128 [00:19<00:45, 488.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9731/32128 [00:19<00:45, 494.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9781/32128 [00:19<00:45, 494.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 9832/32128 [00:19<00:44, 495.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 9882/32128 [00:20<00:44, 496.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 9932/32128 [00:20<00:45, 490.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 9982/32128 [00:20<00:46, 480.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 10031/32128 [00:20<00:45, 480.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 10080/32128 [00:20<00:45, 482.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 10129/32128 [00:20<00:45, 484.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 10182/32128 [00:20<00:44, 495.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 10234/32128 [00:20<00:43, 498.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 10284/32128 [00:20<00:44, 490.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 10334/32128 [00:21<00:44, 490.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 10384/32128 [00:21<00:44, 489.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 10434/32128 [00:21<00:44, 482.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10485/32128 [00:21<00:44, 488.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10535/32128 [00:21<00:44, 490.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10587/32128 [00:21<00:43, 497.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10638/32128 [00:21<00:43, 499.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10690/32128 [00:21<00:42, 503.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10741/32128 [00:21<00:43, 493.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▎      | 10792/32128 [00:21<00:42, 498.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▎      | 10843/32128 [00:22<00:42, 499.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 10894/32128 [00:22<00:42, 499.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 10944/32128 [00:22<00:42, 493.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 10994/32128 [00:22<00:44, 472.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 11044/32128 [00:22<00:43, 479.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 11093/32128 [00:22<00:45, 463.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 11140/32128 [00:22<00:46, 454.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 11186/32128 [00:22<00:46, 453.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 11235/32128 [00:22<00:45, 462.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 11286/32128 [00:22<00:43, 475.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 11334/32128 [00:23<00:44, 462.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 11381/32128 [00:23<00:45, 459.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 11428/32128 [00:23<00:45, 458.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 11474/32128 [00:23<00:45, 451.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 11526/32128 [00:23<00:43, 469.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 11580/32128 [00:23<00:42, 486.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 11629/32128 [00:23<00:42, 484.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 11678/32128 [00:23<00:42, 483.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11728/32128 [00:23<00:42, 485.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11777/32128 [00:24<00:42, 482.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11827/32128 [00:24<00:41, 486.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11876/32128 [00:24<00:42, 478.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11925/32128 [00:24<00:42, 479.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11974/32128 [00:24<00:41, 480.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 12023/32128 [00:24<00:42, 474.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 12074/32128 [00:24<00:41, 483.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 12127/32128 [00:24<00:40, 495.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 12177/32128 [00:24<00:40, 490.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 12227/32128 [00:24<00:41, 474.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 12275/32128 [00:25<00:41, 474.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 12323/32128 [00:25<00:42, 466.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▊      | 12372/32128 [00:25<00:41, 471.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▊      | 12423/32128 [00:25<00:40, 481.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 12472/32128 [00:25<00:41, 473.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 12522/32128 [00:25<00:40, 478.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 12570/32128 [00:25<00:41, 471.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 12618/32128 [00:25<00:42, 460.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 12666/32128 [00:25<00:41, 465.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 12715/32128 [00:25<00:41, 472.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 12763/32128 [00:26<00:41, 471.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 12811/32128 [00:26<00:40, 473.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12859/32128 [00:26<00:41, 460.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12907/32128 [00:26<00:41, 463.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12956/32128 [00:26<00:40, 470.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 13007/32128 [00:26<00:39, 480.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 13060/32128 [00:26<00:38, 492.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 13110/32128 [00:26<00:39, 482.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 13162/32128 [00:26<00:38, 491.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 13212/32128 [00:27<00:38, 487.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████▏     | 13262/32128 [00:27<00:38, 488.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████▏     | 13311/32128 [00:27<00:39, 471.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 13364/32128 [00:27<00:38, 485.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 13413/32128 [00:27<00:38, 486.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 13464/32128 [00:27<00:38, 490.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 13514/32128 [00:27<00:38, 485.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 13564/32128 [00:27<00:38, 487.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 13613/32128 [00:27<00:38, 482.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13663/32128 [00:27<00:37, 485.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13714/32128 [00:28<00:37, 490.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13764/32128 [00:28<00:37, 488.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13813/32128 [00:28<00:38, 473.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13865/32128 [00:28<00:37, 483.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13914/32128 [00:28<00:37, 479.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13966/32128 [00:28<00:37, 487.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 14015/32128 [00:28<00:37, 483.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 14068/32128 [00:28<00:36, 495.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 14118/32128 [00:28<00:37, 480.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 14171/32128 [00:29<00:36, 492.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 14221/32128 [00:29<00:36, 484.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 14270/32128 [00:29<00:37, 475.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 14318/32128 [00:29<00:37, 470.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 14368/32128 [00:29<00:37, 477.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 14416/32128 [00:29<00:37, 470.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 14464/32128 [00:29<00:37, 465.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 14514/32128 [00:29<00:37, 471.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 14566/32128 [00:29<00:36, 483.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 14615/32128 [00:29<00:36, 483.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 14664/32128 [00:30<00:37, 461.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 14712/32128 [00:30<00:37, 465.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 14759/32128 [00:30<00:37, 465.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 14807/32128 [00:30<00:37, 467.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 14854/32128 [00:30<00:38, 449.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▋     | 14900/32128 [00:30<00:38, 449.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14946/32128 [00:30<00:38, 448.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14996/32128 [00:30<00:37, 461.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 15043/32128 [00:30<00:37, 461.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 15093/32128 [00:30<00:36, 471.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 15141/32128 [00:31<00:35, 472.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 15191/32128 [00:31<00:35, 479.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 15240/32128 [00:31<00:36, 463.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 15291/32128 [00:31<00:35, 475.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 15339/32128 [00:31<00:35, 474.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 15391/32128 [00:31<00:34, 485.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 15441/32128 [00:31<00:34, 487.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 15491/32128 [00:31<00:33, 490.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 15541/32128 [00:31<00:34, 486.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 15592/32128 [00:32<00:33, 491.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 15642/32128 [00:32<00:33, 491.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 15692/32128 [00:32<00:33, 487.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 15741/32128 [00:32<00:33, 487.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 15790/32128 [00:32<00:33, 486.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 15839/32128 [00:32<00:34, 475.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 15889/32128 [00:32<00:33, 481.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 15939/32128 [00:32<00:33, 486.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 15990/32128 [00:32<00:32, 491.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 16040/32128 [00:32<00:33, 477.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 16088/32128 [00:33<00:33, 477.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 16139/32128 [00:33<00:32, 485.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 16188/32128 [00:33<00:33, 482.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 16237/32128 [00:33<00:33, 477.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 16288/32128 [00:33<00:32, 485.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 16337/32128 [00:33<00:33, 465.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 16386/32128 [00:33<00:33, 472.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 16434/32128 [00:33<00:33, 474.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████▏    | 16484/32128 [00:33<00:32, 480.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████▏    | 16537/32128 [00:33<00:31, 492.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 16587/32128 [00:34<00:31, 492.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 16637/32128 [00:34<00:32, 479.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 16686/32128 [00:34<00:32, 474.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 16735/32128 [00:34<00:32, 477.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 16784/32128 [00:34<00:31, 480.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 16833/32128 [00:34<00:32, 476.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16883/32128 [00:34<00:31, 481.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16932/32128 [00:34<00:31, 478.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16982/32128 [00:34<00:31, 483.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 17034/32128 [00:35<00:30, 491.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 17085/32128 [00:35<00:30, 495.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 17135/32128 [00:35<00:30, 485.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 17184/32128 [00:35<00:30, 483.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▎    | 17234/32128 [00:35<00:30, 487.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 17286/32128 [00:35<00:29, 495.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 17336/32128 [00:35<00:30, 485.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 17386/32128 [00:35<00:30, 488.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 17438/32128 [00:35<00:29, 496.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 17489/32128 [00:35<00:29, 499.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 17540/32128 [00:36<00:29, 486.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 17594/32128 [00:36<00:29, 499.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 17645/32128 [00:36<00:29, 492.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 17695/32128 [00:36<00:29, 492.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 17745/32128 [00:36<00:29, 484.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 17794/32128 [00:36<00:30, 470.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 17844/32128 [00:36<00:29, 478.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 17895/32128 [00:36<00:29, 486.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 17944/32128 [00:36<00:29, 486.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 17995/32128 [00:36<00:28, 492.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 18045/32128 [00:37<00:28, 491.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 18095/32128 [00:37<00:28, 486.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 18144/32128 [00:37<00:28, 482.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 18197/32128 [00:37<00:28, 494.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 18250/32128 [00:37<00:27, 502.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 18301/32128 [00:37<00:27, 496.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 18351/32128 [00:37<00:28, 482.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 18400/32128 [00:37<00:28, 482.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 18449/32128 [00:37<00:28, 483.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 18498/32128 [00:38<00:29, 469.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 18551/32128 [00:38<00:28, 483.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 18604/32128 [00:38<00:27, 494.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 18654/32128 [00:38<00:28, 473.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 18705/32128 [00:38<00:27, 482.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 18757/32128 [00:38<00:27, 491.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 18807/32128 [00:38<00:27, 491.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 18857/32128 [00:38<00:27, 483.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 18907/32128 [00:38<00:27, 488.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 18956/32128 [00:38<00:27, 476.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 19004/32128 [00:39<00:27, 469.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 19052/32128 [00:39<00:28, 461.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 19104/32128 [00:39<00:27, 475.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 19153/32128 [00:39<00:27, 479.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 19202/32128 [00:39<00:27, 477.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 19252/32128 [00:39<00:26, 482.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 19301/32128 [00:39<00:26, 484.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 19351/32128 [00:39<00:26, 486.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 19400/32128 [00:39<00:26, 486.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 19451/32128 [00:39<00:25, 492.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 19501/32128 [00:40<00:25, 491.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 19551/32128 [00:40<00:26, 476.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 19603/32128 [00:40<00:25, 487.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 19654/32128 [00:40<00:25, 493.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████▏   | 19705/32128 [00:40<00:25, 495.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████▏   | 19755/32128 [00:40<00:25, 481.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 19804/32128 [00:40<00:25, 479.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 19855/32128 [00:40<00:25, 486.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 19904/32128 [00:40<00:25, 481.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 19953/32128 [00:41<00:25, 479.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 20004/32128 [00:41<00:24, 487.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 20053/32128 [00:41<00:24, 486.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 20102/32128 [00:41<00:24, 485.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 20152/32128 [00:41<00:24, 489.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 20201/32128 [00:41<00:24, 489.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 20250/32128 [00:41<00:24, 487.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 20302/32128 [00:41<00:23, 496.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 20352/32128 [00:41<00:23, 493.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▎   | 20402/32128 [00:41<00:23, 493.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▎   | 20452/32128 [00:42<00:23, 488.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 20501/32128 [00:42<00:24, 479.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 20550/32128 [00:42<00:24, 480.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 20603/32128 [00:42<00:23, 493.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 20655/32128 [00:42<00:22, 499.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 20706/32128 [00:42<00:22, 498.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 20756/32128 [00:42<00:22, 496.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 20806/32128 [00:42<00:23, 489.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 20858/32128 [00:42<00:22, 496.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 20908/32128 [00:42<00:23, 484.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 20957/32128 [00:43<00:23, 481.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 21007/32128 [00:43<00:22, 485.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 21056/32128 [00:43<00:23, 463.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 21106/32128 [00:43<00:23, 472.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 21158/32128 [00:43<00:22, 484.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 21208/32128 [00:43<00:22, 488.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 21258/32128 [00:43<00:22, 477.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 21310/32128 [00:43<00:22, 489.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 21360/32128 [00:43<00:22, 474.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 21408/32128 [00:44<00:23, 464.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 21457/32128 [00:44<00:22, 468.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 21505/32128 [00:44<00:22, 471.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 21556/32128 [00:44<00:21, 481.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 21606/32128 [00:44<00:21, 484.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 21658/32128 [00:44<00:21, 493.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 21708/32128 [00:44<00:21, 486.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 21757/32128 [00:44<00:22, 471.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 21807/32128 [00:44<00:21, 476.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 21857/32128 [00:44<00:21, 482.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 21906/32128 [00:45<00:21, 484.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 21955/32128 [00:45<00:21, 480.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 22004/32128 [00:45<00:21, 472.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 22052/32128 [00:45<00:21, 473.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 22101/32128 [00:45<00:21, 476.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 22149/32128 [00:45<00:21, 473.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 22197/32128 [00:45<00:21, 470.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 22246/32128 [00:45<00:20, 474.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 22294/32128 [00:45<00:21, 460.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 22341/32128 [00:45<00:21, 462.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 22388/32128 [00:46<00:21, 460.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 22436/32128 [00:46<00:20, 466.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 22485/32128 [00:46<00:20, 471.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 22538/32128 [00:46<00:19, 485.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 22587/32128 [00:46<00:20, 476.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 22635/32128 [00:46<00:20, 472.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 22683/32128 [00:46<00:20, 470.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 22732/32128 [00:46<00:19, 475.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 22780/32128 [00:46<00:19, 475.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 22830/32128 [00:46<00:19, 480.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 22879/32128 [00:47<00:19, 477.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████▏  | 22927/32128 [00:47<00:19, 477.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 22975/32128 [00:47<00:19, 463.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 23025/32128 [00:47<00:19, 473.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 23076/32128 [00:47<00:18, 483.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 23126/32128 [00:47<00:18, 486.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 23175/32128 [00:47<00:19, 464.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 23227/32128 [00:47<00:18, 477.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 23276/32128 [00:47<00:18, 479.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 23326/32128 [00:48<00:18, 483.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 23377/32128 [00:48<00:17, 488.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 23426/32128 [00:48<00:17, 485.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 23475/32128 [00:48<00:17, 484.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 23526/32128 [00:48<00:17, 491.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 23579/32128 [00:48<00:17, 500.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 23630/32128 [00:48<00:17, 493.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 23680/32128 [00:48<00:17, 483.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 23731/32128 [00:48<00:17, 489.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 23781/32128 [00:48<00:17, 484.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 23831/32128 [00:49<00:17, 487.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 23880/32128 [00:49<00:16, 487.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 23929/32128 [00:49<00:17, 480.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 23979/32128 [00:49<00:16, 484.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 24028/32128 [00:49<00:16, 480.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 24077/32128 [00:49<00:17, 471.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 24125/32128 [00:49<00:16, 472.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 24173/32128 [00:49<00:16, 468.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 24221/32128 [00:49<00:16, 469.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 24272/32128 [00:49<00:16, 477.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 24320/32128 [00:50<00:16, 469.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 24367/32128 [00:50<00:17, 453.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 24413/32128 [00:50<00:17, 452.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 24465/32128 [00:50<00:16, 469.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 24513/32128 [00:50<00:16, 470.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 24561/32128 [00:50<00:16, 471.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 24609/32128 [00:50<00:16, 463.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 24657/32128 [00:50<00:15, 467.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 24704/32128 [00:50<00:16, 457.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 24754/32128 [00:51<00:15, 467.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 24801/32128 [00:51<00:15, 462.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 24848/32128 [00:51<00:15, 460.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 24896/32128 [00:51<00:15, 463.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 24946/32128 [00:51<00:15, 473.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 24996/32128 [00:51<00:14, 479.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 25045/32128 [00:51<00:14, 472.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 25093/32128 [00:51<00:15, 467.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 25142/32128 [00:51<00:14, 473.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 25190/32128 [00:51<00:14, 470.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 25240/32128 [00:52<00:14, 476.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 25291/32128 [00:52<00:14, 484.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 25340/32128 [00:52<00:13, 485.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 25389/32128 [00:52<00:14, 469.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 25439/32128 [00:52<00:14, 475.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 25490/32128 [00:52<00:13, 483.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 25539/32128 [00:52<00:13, 480.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 25588/32128 [00:52<00:13, 475.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 25639/32128 [00:52<00:13, 482.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 25688/32128 [00:52<00:13, 484.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 25737/32128 [00:53<00:13, 474.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 25785/32128 [00:53<00:13, 470.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 25834/32128 [00:53<00:13, 474.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 25883/32128 [00:53<00:13, 476.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 25931/32128 [00:53<00:13, 474.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 25981/32128 [00:53<00:12, 481.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 26030/32128 [00:53<00:13, 461.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 26077/32128 [00:53<00:13, 457.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 26123/32128 [00:53<00:13, 454.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 26169/32128 [00:54<00:13, 444.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 26215/32128 [00:54<00:13, 446.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 26262/32128 [00:54<00:12, 452.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 26308/32128 [00:54<00:13, 437.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 26357/32128 [00:54<00:12, 450.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 26406/32128 [00:54<00:12, 459.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 26453/32128 [00:54<00:12, 462.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 26500/32128 [00:54<00:12, 458.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 26549/32128 [00:54<00:11, 466.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 26596/32128 [00:54<00:12, 459.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 26643/32128 [00:55<00:11, 458.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 26693/32128 [00:55<00:11, 468.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 26743/32128 [00:55<00:11, 477.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 26792/32128 [00:55<00:11, 479.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▎ | 26842/32128 [00:55<00:10, 483.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▎ | 26892/32128 [00:55<00:10, 485.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 26941/32128 [00:55<00:10, 482.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 26991/32128 [00:55<00:10, 487.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 27040/32128 [00:55<00:10, 469.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 27089/32128 [00:55<00:10, 473.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 27137/32128 [00:56<00:10, 466.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 27184/32128 [00:56<00:10, 456.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 27232/32128 [00:56<00:10, 462.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 27279/32128 [00:56<00:10, 444.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 27328/32128 [00:56<00:10, 456.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 27379/32128 [00:56<00:10, 469.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 27430/32128 [00:56<00:09, 479.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 27479/32128 [00:56<00:10, 460.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 27526/32128 [00:56<00:09, 463.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 27573/32128 [00:57<00:09, 463.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 27620/32128 [00:57<00:09, 456.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 27666/32128 [00:57<00:09, 449.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▋ | 27714/32128 [00:57<00:09, 456.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▋ | 27761/32128 [00:57<00:09, 459.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 27808/32128 [00:57<00:09, 454.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 27854/32128 [00:57<00:09, 453.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 27900/32128 [00:57<00:09, 443.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 27945/32128 [00:57<00:09, 431.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 27991/32128 [00:57<00:09, 438.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 28036/32128 [00:58<00:09, 439.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 28081/32128 [00:58<00:09, 428.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28130/32128 [00:58<00:09, 443.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28179/32128 [00:58<00:08, 454.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28225/32128 [00:58<00:08, 439.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28273/32128 [00:58<00:08, 450.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28323/32128 [00:58<00:08, 462.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28371/32128 [00:58<00:08, 465.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 28418/32128 [00:58<00:08, 446.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▊ | 28463/32128 [00:59<00:08, 431.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▊ | 28511/32128 [00:59<00:08, 444.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 28556/32128 [00:59<00:08, 442.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 28603/32128 [00:59<00:07, 448.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 28651/32128 [00:59<00:07, 454.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 28703/32128 [00:59<00:07, 470.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 28751/32128 [00:59<00:07, 470.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 28799/32128 [00:59<00:07, 457.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 28845/32128 [00:59<00:07, 452.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 28892/32128 [00:59<00:07, 455.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 28939/32128 [01:00<00:06, 459.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 28987/32128 [01:00<00:06, 464.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 29036/32128 [01:00<00:06, 469.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 29087/32128 [01:00<00:06, 480.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 29136/32128 [01:00<00:06, 474.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 29184/32128 [01:00<00:06, 468.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 29234/32128 [01:00<00:06, 475.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 29287/32128 [01:00<00:05, 490.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████▏| 29337/32128 [01:00<00:05, 475.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████▏| 29389/32128 [01:01<00:05, 485.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 29438/32128 [01:01<00:05, 479.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 29487/32128 [01:01<00:05, 474.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 29535/32128 [01:01<00:05, 473.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 29586/32128 [01:01<00:05, 481.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 29635/32128 [01:01<00:05, 469.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 29686/32128 [01:01<00:05, 479.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 29736/32128 [01:01<00:04, 483.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 29787/32128 [01:01<00:04, 489.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 29837/32128 [01:01<00:04, 482.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 29888/32128 [01:02<00:04, 489.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 29937/32128 [01:02<00:04, 486.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 29987/32128 [01:02<00:04, 488.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 30036/32128 [01:02<00:04, 483.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▎| 30085/32128 [01:02<00:04, 483.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 30134/32128 [01:02<00:04, 482.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 30183/32128 [01:02<00:04, 479.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 30232/32128 [01:02<00:03, 480.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 30285/32128 [01:02<00:03, 492.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 30335/32128 [01:02<00:03, 479.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 30384/32128 [01:03<00:03, 471.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 30432/32128 [01:03<00:03, 469.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 30480/32128 [01:03<00:03, 472.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 30530/32128 [01:03<00:03, 478.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 30581/32128 [01:03<00:03, 487.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 30632/32128 [01:03<00:03, 493.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 30682/32128 [01:03<00:02, 483.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 30731/32128 [01:03<00:02, 472.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 30783/32128 [01:03<00:02, 485.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 30832/32128 [01:04<00:02, 484.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 30882/32128 [01:04<00:02, 487.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 30931/32128 [01:04<00:02, 466.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 30978/32128 [01:04<00:02, 463.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31025/32128 [01:04<00:02, 463.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31072/32128 [01:04<00:02, 450.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31121/32128 [01:04<00:02, 461.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31168/32128 [01:04<00:02, 463.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31216/32128 [01:04<00:01, 467.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31263/32128 [01:04<00:01, 454.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 31309/32128 [01:05<00:01, 453.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 31355/32128 [01:05<00:01, 445.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 31401/32128 [01:05<00:01, 448.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 31452/32128 [01:05<00:01, 462.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 31501/32128 [01:05<00:01, 470.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 31551/32128 [01:05<00:01, 477.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 31604/32128 [01:05<00:01, 492.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▊| 31654/32128 [01:05<00:00, 483.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▊| 31703/32128 [01:05<00:00, 482.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 31752/32128 [01:05<00:00, 483.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 31802/32128 [01:06<00:00, 487.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 31853/32128 [01:06<00:00, 493.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 31904/32128 [01:06<00:00, 495.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 31954/32128 [01:06<00:00, 496.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 32004/32128 [01:06<00:00, 484.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 32057/32128 [01:06<00:00, 495.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 32128/32128 [01:06<00:00, 481.45it/s]\n"
     ]
    }
   ],
   "source": [
    "hate_tox_val = find_hate_sentence(tox_val, 'comment', 'toxicity', hate_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "7IuVET62m7WH",
    "outputId": "60a46275-2b59-490a-caf6-54dbbddad6ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11343 entries, 0 to 16634\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comment   11343 non-null  object \n",
      " 1   toxicity  11343 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 265.9+ KB\n"
     ]
    }
   ],
   "source": [
    "hate_tox_val.info()\n",
    "hate_tox_val.to_csv('Data/data_after_extraction/hate_tox_val.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XjUgQAPeyqym",
    "outputId": "abec78e4-1c65-4c22-deb0-db38647f8af5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/31866 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 43/31866 [00:00<01:14, 428.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 95/31866 [00:00<01:10, 451.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 147/31866 [00:00<01:07, 469.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 196/31866 [00:00<01:06, 473.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 247/31866 [00:00<01:05, 483.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 297/31866 [00:00<01:04, 486.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 345/31866 [00:00<01:05, 483.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 396/31866 [00:00<01:04, 489.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▏         | 444/31866 [00:00<01:04, 485.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 491/31866 [00:01<01:05, 478.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 538/31866 [00:01<01:06, 473.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 589/31866 [00:01<01:04, 481.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 640/31866 [00:01<01:04, 487.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 692/31866 [00:01<01:02, 495.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 742/31866 [00:01<01:02, 495.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 792/31866 [00:01<01:02, 494.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 842/31866 [00:01<01:02, 495.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 892/31866 [00:01<01:02, 492.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 942/31866 [00:01<01:03, 487.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 991/31866 [00:02<01:04, 480.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 1040/31866 [00:02<01:04, 480.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 1092/31866 [00:02<01:02, 489.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▎         | 1141/31866 [00:02<01:03, 487.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▎         | 1190/31866 [00:02<01:04, 475.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▍         | 1241/31866 [00:02<01:03, 484.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▍         | 1293/31866 [00:02<01:01, 493.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▍         | 1344/31866 [00:02<01:01, 497.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▍         | 1394/31866 [00:02<01:01, 496.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▍         | 1444/31866 [00:02<01:01, 495.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▍         | 1494/31866 [00:03<01:02, 486.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▍         | 1543/31866 [00:03<01:02, 483.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 1594/31866 [00:03<01:01, 489.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 1644/31866 [00:03<01:01, 490.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 1694/31866 [00:03<01:01, 489.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 1745/31866 [00:03<01:01, 493.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|▌         | 1795/31866 [00:03<01:03, 474.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|▌         | 1845/31866 [00:03<01:02, 479.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|▌         | 1896/31866 [00:03<01:01, 488.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|▌         | 1945/31866 [00:03<01:02, 480.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|▋         | 1994/31866 [00:04<01:02, 480.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|▋         | 2043/31866 [00:04<01:02, 474.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 2091/31866 [00:04<01:03, 468.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 2142/31866 [00:04<01:02, 478.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 2195/31866 [00:04<01:00, 489.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 2248/31866 [00:04<00:59, 500.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 2299/31866 [00:04<00:59, 497.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 2349/31866 [00:04<01:03, 465.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 2397/31866 [00:04<01:04, 458.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 2444/31866 [00:05<01:04, 458.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 2495/31866 [00:05<01:02, 471.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 2547/31866 [00:05<01:00, 483.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 2597/31866 [00:05<01:00, 487.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 2648/31866 [00:05<00:59, 493.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 2699/31866 [00:05<00:58, 497.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▊         | 2751/31866 [00:05<00:57, 503.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▉         | 2804/31866 [00:05<00:57, 508.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▉         | 2857/31866 [00:05<00:56, 512.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▉         | 2909/31866 [00:05<00:57, 500.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▉         | 2960/31866 [00:06<00:58, 496.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▉         | 3010/31866 [00:06<00:59, 486.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|▉         | 3062/31866 [00:06<00:58, 495.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|▉         | 3114/31866 [00:06<00:57, 502.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|▉         | 3165/31866 [00:06<00:56, 504.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 3216/31866 [00:06<00:57, 494.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 3267/31866 [00:06<00:57, 496.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 3320/31866 [00:06<00:56, 506.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█         | 3373/31866 [00:06<00:55, 511.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█         | 3425/31866 [00:06<00:57, 497.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█         | 3475/31866 [00:07<00:57, 496.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█         | 3525/31866 [00:07<00:58, 484.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█         | 3576/31866 [00:07<00:57, 491.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█▏        | 3627/31866 [00:07<00:56, 496.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 3680/31866 [00:07<00:55, 504.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 3731/31866 [00:07<00:56, 495.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 3783/31866 [00:07<00:56, 500.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 3834/31866 [00:07<00:57, 487.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 3883/31866 [00:07<00:57, 486.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 3934/31866 [00:08<00:56, 491.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 3987/31866 [00:08<00:55, 501.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 4038/31866 [00:08<00:55, 500.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 4089/31866 [00:08<00:55, 502.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 4140/31866 [00:08<00:55, 496.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 4190/31866 [00:08<00:55, 495.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 4244/31866 [00:08<00:54, 507.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 4295/31866 [00:08<00:54, 503.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▎        | 4346/31866 [00:08<00:54, 501.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▍        | 4397/31866 [00:08<00:55, 492.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▍        | 4447/31866 [00:09<00:55, 489.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▍        | 4498/31866 [00:09<00:55, 494.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▍        | 4548/31866 [00:09<00:56, 479.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▍        | 4603/31866 [00:09<00:54, 497.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▍        | 4655/31866 [00:09<00:54, 503.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▍        | 4707/31866 [00:09<00:53, 507.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▍        | 4758/31866 [00:09<00:53, 502.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 4809/31866 [00:09<00:54, 494.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 4861/31866 [00:09<00:53, 501.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 4912/31866 [00:09<00:54, 497.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 4964/31866 [00:10<00:53, 502.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 5015/31866 [00:10<00:53, 499.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 5065/31866 [00:10<00:54, 493.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 5115/31866 [00:10<00:55, 483.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 5166/31866 [00:10<00:54, 491.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▋        | 5219/31866 [00:10<00:53, 501.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 5270/31866 [00:10<00:53, 494.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 5320/31866 [00:10<00:53, 492.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 5370/31866 [00:10<00:55, 478.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 5420/31866 [00:11<00:54, 484.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 5469/31866 [00:11<00:54, 482.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 5520/31866 [00:11<00:53, 489.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 5571/31866 [00:11<00:53, 493.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 5623/31866 [00:11<00:52, 499.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 5673/31866 [00:11<00:52, 497.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 5723/31866 [00:11<00:53, 487.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 5773/31866 [00:11<00:53, 489.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 5823/31866 [00:11<00:53, 489.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 5872/31866 [00:11<00:54, 473.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 6261/31866 [00:26<00:54, 471.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▊        | 5920/31866 [00:12<00:55, 464.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▊        | 5967/31866 [00:12<00:55, 466.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▉        | 6018/31866 [00:12<00:54, 476.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▉        | 6068/31866 [00:12<00:53, 482.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▉        | 6117/31866 [00:12<00:53, 483.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▉        | 6166/31866 [00:12<00:53, 483.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|█▉        | 6216/31866 [00:12<00:52, 486.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|█▉        | 6266/31866 [00:12<00:52, 489.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|█▉        | 6316/31866 [00:12<00:52, 483.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|█▉        | 6365/31866 [00:12<00:52, 483.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 6414/31866 [00:13<00:53, 474.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 6462/31866 [00:13<00:54, 462.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 6509/31866 [00:13<00:55, 454.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██        | 6561/31866 [00:13<00:53, 472.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██        | 6610/31866 [00:13<00:53, 476.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██        | 6658/31866 [00:13<00:54, 466.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██        | 6705/31866 [00:13<00:54, 463.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██        | 6754/31866 [00:13<00:53, 469.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██▏       | 6802/31866 [00:13<00:54, 459.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██▏       | 6849/31866 [00:14<00:57, 436.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 6897/31866 [00:14<00:55, 447.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 6948/31866 [00:14<00:53, 463.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 6997/31866 [00:14<00:53, 469.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 7047/31866 [00:14<00:52, 476.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 7095/31866 [00:14<00:52, 475.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 7143/31866 [00:14<00:51, 476.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 7193/31866 [00:14<00:51, 481.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 7245/31866 [00:14<00:50, 492.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 7295/31866 [00:14<00:50, 489.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 7345/31866 [00:15<00:50, 481.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 7395/31866 [00:15<00:50, 484.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 7447/31866 [00:15<00:49, 491.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▎       | 7498/31866 [00:15<00:49, 497.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▎       | 7549/31866 [00:15<00:48, 499.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▍       | 7599/31866 [00:15<00:48, 497.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▍       | 7649/31866 [00:15<00:48, 494.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▍       | 7700/31866 [00:15<00:48, 498.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▍       | 7752/31866 [00:15<00:47, 504.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▍       | 7803/31866 [00:15<00:48, 491.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▍       | 7853/31866 [00:16<00:50, 478.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▍       | 7902/31866 [00:16<00:49, 480.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▍       | 7951/31866 [00:16<00:51, 468.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 8000/31866 [00:16<00:50, 473.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 8052/31866 [00:16<00:49, 484.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 8102/31866 [00:16<00:48, 487.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 8153/31866 [00:16<00:48, 492.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 8203/31866 [00:16<00:47, 493.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 8253/31866 [00:16<00:47, 493.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 8303/31866 [00:17<00:50, 466.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 8354/31866 [00:17<00:49, 476.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▋       | 8402/31866 [00:17<00:49, 477.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 8452/31866 [00:17<00:48, 482.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 8502/31866 [00:17<00:48, 486.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 8553/31866 [00:17<00:47, 490.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 8603/31866 [00:17<00:48, 482.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 8654/31866 [00:17<00:47, 489.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 8704/31866 [00:17<00:48, 480.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 8753/31866 [00:17<00:48, 476.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 8801/31866 [00:18<00:48, 473.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 8851/31866 [00:18<00:47, 481.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 8900/31866 [00:18<00:47, 482.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 8949/31866 [00:18<00:47, 481.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 8998/31866 [00:18<00:47, 479.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 9046/31866 [00:18<00:48, 471.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▊       | 9094/31866 [00:18<00:48, 470.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▊       | 9146/31866 [00:18<00:46, 483.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▉       | 9195/31866 [00:18<00:46, 484.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▉       | 9244/31866 [00:18<00:46, 481.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▉       | 9295/31866 [00:19<00:46, 487.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▉       | 9344/31866 [00:19<00:46, 485.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▉       | 9393/31866 [00:19<00:46, 486.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|██▉       | 9442/31866 [00:19<00:46, 484.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|██▉       | 9491/31866 [00:19<00:46, 485.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|██▉       | 9540/31866 [00:19<00:46, 483.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 9589/31866 [00:19<00:46, 480.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 9638/31866 [00:19<00:46, 480.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 9687/31866 [00:19<00:46, 475.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███       | 9737/31866 [00:20<00:45, 481.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███       | 9786/31866 [00:20<00:45, 481.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███       | 9837/31866 [00:20<00:45, 487.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███       | 9886/31866 [00:20<00:45, 486.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███       | 9935/31866 [00:20<00:46, 474.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███▏      | 9983/31866 [00:20<00:47, 465.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███▏      | 10035/31866 [00:20<00:45, 479.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▏      | 10087/31866 [00:20<00:44, 490.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▏      | 10137/31866 [00:20<00:44, 491.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▏      | 10187/31866 [00:20<00:44, 481.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▏      | 10236/31866 [00:21<00:45, 479.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▏      | 10285/31866 [00:21<00:45, 475.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▏      | 10333/31866 [00:21<00:45, 474.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███▎      | 10384/31866 [00:21<00:44, 483.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███▎      | 10433/31866 [00:21<00:44, 483.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███▎      | 10482/31866 [00:21<00:44, 484.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███▎      | 10531/31866 [00:21<00:44, 480.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███▎      | 10580/31866 [00:21<00:44, 477.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███▎      | 10629/31866 [00:21<00:44, 479.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▎      | 10678/31866 [00:21<00:43, 482.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▎      | 10729/31866 [00:22<00:43, 487.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 10778/31866 [00:22<00:43, 484.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 10828/31866 [00:22<00:43, 488.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 10877/31866 [00:22<00:43, 479.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 10929/31866 [00:22<00:42, 489.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 10980/31866 [00:22<00:42, 492.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▍      | 11030/31866 [00:22<00:42, 492.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▍      | 11080/31866 [00:22<00:43, 481.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▍      | 11132/31866 [00:22<00:42, 489.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 11182/31866 [00:22<00:42, 485.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 11231/31866 [00:23<00:43, 471.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 11280/31866 [00:23<00:43, 475.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▌      | 11328/31866 [00:23<00:43, 470.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▌      | 11376/31866 [00:23<00:44, 459.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▌      | 11428/31866 [00:23<00:43, 474.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▌      | 11476/31866 [00:23<00:43, 473.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▌      | 11524/31866 [00:23<00:44, 460.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▋      | 11572/31866 [00:23<00:43, 465.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▋      | 11619/31866 [00:23<00:44, 457.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|███▋      | 11669/31866 [00:24<00:43, 468.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|███▋      | 11717/31866 [00:24<00:42, 470.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|███▋      | 11765/31866 [00:24<00:43, 465.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|███▋      | 11815/31866 [00:24<00:42, 475.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|███▋      | 11863/31866 [00:24<00:42, 467.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|███▋      | 11914/31866 [00:24<00:41, 479.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 11964/31866 [00:24<00:41, 482.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 12013/31866 [00:24<00:41, 481.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 12062/31866 [00:24<00:41, 476.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 12110/31866 [00:24<00:41, 476.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 12160/31866 [00:25<00:40, 482.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 12210/31866 [00:25<00:40, 486.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 12260/31866 [00:25<00:40, 489.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▊      | 12310/31866 [00:25<00:40, 487.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 12359/31866 [00:25<00:40, 475.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 12410/31866 [00:25<00:40, 485.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 12461/31866 [00:25<00:39, 490.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 12513/31866 [00:25<00:38, 498.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 12563/31866 [00:25<00:40, 474.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███▉      | 12613/31866 [00:25<00:40, 480.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███▉      | 12665/31866 [00:26<00:39, 491.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███▉      | 12715/31866 [00:26<00:38, 492.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 12765/31866 [00:26<00:38, 492.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 12818/31866 [00:26<00:38, 500.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 12869/31866 [00:26<00:39, 484.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|████      | 12920/31866 [00:26<00:38, 490.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|████      | 12973/31866 [00:26<00:37, 499.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|████      | 13024/31866 [00:26<00:38, 488.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|████      | 13074/31866 [00:26<00:38, 488.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|████      | 13127/31866 [00:27<00:37, 497.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|████▏     | 13177/31866 [00:27<00:38, 485.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 13226/31866 [00:27<00:39, 468.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 13275/31866 [00:27<00:39, 473.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 13323/31866 [00:27<00:40, 462.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 13371/31866 [00:27<00:39, 466.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 13418/31866 [00:27<00:40, 455.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 13464/31866 [00:27<00:40, 450.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 13513/31866 [00:27<00:39, 460.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 13560/31866 [00:27<00:39, 458.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 13612/31866 [00:28<00:38, 475.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 13660/31866 [00:28<00:39, 462.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 13707/31866 [00:28<00:39, 462.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 13757/31866 [00:28<00:38, 473.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 13805/31866 [00:28<00:38, 471.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 13853/31866 [00:28<00:39, 460.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████▎     | 13900/31866 [00:28<00:38, 460.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████▍     | 13947/31866 [00:28<00:39, 457.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████▍     | 13993/31866 [00:28<00:39, 457.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████▍     | 14039/31866 [00:29<00:38, 458.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████▍     | 14085/31866 [00:29<00:39, 451.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████▍     | 14136/31866 [00:29<00:38, 466.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▍     | 14188/31866 [00:29<00:36, 480.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▍     | 14237/31866 [00:29<00:36, 480.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▍     | 14286/31866 [00:29<00:37, 467.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▍     | 14338/31866 [00:29<00:36, 481.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 14389/31866 [00:29<00:35, 489.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 14441/31866 [00:29<00:34, 497.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 14491/31866 [00:29<00:35, 483.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▌     | 14540/31866 [00:30<00:36, 476.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▌     | 14591/31866 [00:30<00:35, 484.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▌     | 14643/31866 [00:30<00:34, 493.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▌     | 14693/31866 [00:30<00:35, 482.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▋     | 14747/31866 [00:30<00:34, 496.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▋     | 14797/31866 [00:30<00:34, 487.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 14846/31866 [00:30<00:35, 480.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 14895/31866 [00:30<00:35, 481.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 14946/31866 [00:30<00:34, 488.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 14995/31866 [00:30<00:34, 485.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 15048/31866 [00:31<00:33, 495.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 15098/31866 [00:31<00:33, 496.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 15148/31866 [00:31<00:34, 485.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 15197/31866 [00:31<00:34, 482.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 15246/31866 [00:31<00:34, 482.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 15296/31866 [00:31<00:34, 486.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 15347/31866 [00:31<00:33, 492.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 15397/31866 [00:31<00:33, 492.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 15447/31866 [00:31<00:33, 488.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████▊     | 15496/31866 [00:32<00:34, 471.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████▉     | 15544/31866 [00:32<00:34, 469.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████▉     | 15595/31866 [00:32<00:33, 480.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████▉     | 15644/31866 [00:32<00:34, 465.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████▉     | 15691/31866 [00:32<00:34, 464.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████▉     | 15738/31866 [00:32<00:35, 460.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|████▉     | 15790/31866 [00:32<00:33, 475.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|████▉     | 15841/31866 [00:32<00:33, 484.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|████▉     | 15893/31866 [00:32<00:32, 494.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 15943/31866 [00:32<00:32, 495.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 15993/31866 [00:33<00:32, 495.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 16043/31866 [00:33<00:31, 496.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████     | 16093/31866 [00:33<00:31, 496.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████     | 16143/31866 [00:33<00:31, 491.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████     | 16195/31866 [00:33<00:31, 499.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████     | 16245/31866 [00:33<00:31, 493.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████     | 16295/31866 [00:33<00:31, 490.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████▏    | 16345/31866 [00:33<00:31, 488.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████▏    | 16394/31866 [00:33<00:32, 475.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▏    | 16444/31866 [00:33<00:31, 482.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▏    | 16496/31866 [00:34<00:31, 492.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▏    | 16547/31866 [00:34<00:30, 496.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▏    | 16597/31866 [00:34<00:31, 482.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▏    | 16646/31866 [00:34<00:32, 461.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▏    | 16696/31866 [00:34<00:32, 470.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 16744/31866 [00:34<00:32, 466.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 16792/31866 [00:34<00:32, 469.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 16840/31866 [00:34<00:32, 466.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 16889/31866 [00:34<00:31, 472.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 16937/31866 [00:35<00:31, 467.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 16988/31866 [00:35<00:31, 478.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 17040/31866 [00:35<00:30, 488.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▎    | 17089/31866 [00:35<00:30, 480.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▍    | 17140/31866 [00:35<00:30, 487.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▍    | 17189/31866 [00:35<00:30, 486.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▍    | 17238/31866 [00:35<00:30, 474.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▍    | 17289/31866 [00:35<00:30, 482.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▍    | 17340/31866 [00:35<00:29, 490.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▍    | 17390/31866 [00:35<00:30, 472.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▍    | 17441/31866 [00:36<00:29, 482.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▍    | 17491/31866 [00:36<00:29, 487.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 17540/31866 [00:36<00:29, 482.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 17593/31866 [00:36<00:28, 493.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 17647/31866 [00:36<00:28, 503.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████▌    | 17698/31866 [00:36<00:29, 480.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████▌    | 17750/31866 [00:36<00:28, 491.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████▌    | 17800/31866 [00:36<00:28, 493.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████▌    | 17850/31866 [00:36<00:28, 491.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████▌    | 17901/31866 [00:36<00:28, 495.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████▋    | 17954/31866 [00:37<00:27, 502.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 18005/31866 [00:37<00:28, 488.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 18056/31866 [00:37<00:28, 492.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 18108/31866 [00:37<00:27, 499.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 18159/31866 [00:37<00:28, 488.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 18208/31866 [00:37<00:28, 474.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 18256/31866 [00:37<00:28, 474.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 18304/31866 [00:37<00:28, 475.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████▊    | 18353/31866 [00:37<00:28, 479.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████▊    | 18405/31866 [00:38<00:27, 490.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████▊    | 18457/31866 [00:38<00:26, 496.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████▊    | 18507/31866 [00:38<00:26, 496.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████▊    | 18559/31866 [00:38<00:26, 501.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████▊    | 18610/31866 [00:38<00:26, 496.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▊    | 18660/31866 [00:38<00:26, 494.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▊    | 18710/31866 [00:38<00:26, 493.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▉    | 18760/31866 [00:38<00:26, 485.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▉    | 18812/31866 [00:38<00:26, 493.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▉    | 18862/31866 [00:38<00:26, 494.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▉    | 18912/31866 [00:39<00:26, 489.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████▉    | 18962/31866 [00:39<00:26, 491.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████▉    | 19015/31866 [00:39<00:25, 499.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████▉    | 19066/31866 [00:39<00:25, 501.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████▉    | 19117/31866 [00:39<00:25, 498.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 19167/31866 [00:39<00:25, 494.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 19217/31866 [00:39<00:26, 484.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 19266/31866 [00:39<00:26, 477.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████    | 19316/31866 [00:39<00:26, 481.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████    | 19365/31866 [00:39<00:25, 480.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████    | 19414/31866 [00:40<00:26, 470.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████    | 19465/31866 [00:40<00:25, 479.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████    | 19515/31866 [00:40<00:25, 484.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████▏   | 19565/31866 [00:40<00:25, 487.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▏   | 19615/31866 [00:40<00:25, 489.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▏   | 19667/31866 [00:40<00:24, 497.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▏   | 19717/31866 [00:40<00:25, 480.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▏   | 19767/31866 [00:40<00:24, 484.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▏   | 19818/31866 [00:40<00:24, 490.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▏   | 19868/31866 [00:41<00:25, 479.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 19919/31866 [00:41<00:24, 488.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 19971/31866 [00:41<00:23, 496.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 20021/31866 [00:41<00:24, 485.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 20070/31866 [00:41<00:24, 485.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 20124/31866 [00:41<00:23, 499.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 20175/31866 [00:41<00:23, 498.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 20226/31866 [00:41<00:23, 488.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▎   | 20275/31866 [00:41<00:24, 478.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▍   | 20324/31866 [00:41<00:24, 467.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▍   | 20373/31866 [00:42<00:24, 472.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▍   | 20423/31866 [00:42<00:23, 479.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▍   | 20475/31866 [00:42<00:23, 489.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▍   | 20528/31866 [00:42<00:22, 499.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▍   | 20579/31866 [00:42<00:22, 498.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▍   | 20629/31866 [00:42<00:22, 494.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▍   | 20679/31866 [00:42<00:23, 477.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 20728/31866 [00:42<00:23, 480.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 20781/31866 [00:42<00:22, 491.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 20831/31866 [00:42<00:22, 494.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|██████▌   | 20881/31866 [00:43<00:22, 494.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|██████▌   | 20932/31866 [00:43<00:21, 498.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|██████▌   | 20982/31866 [00:43<00:22, 490.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|██████▌   | 21034/31866 [00:43<00:21, 496.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|██████▌   | 21084/31866 [00:43<00:21, 494.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|██████▋   | 21135/31866 [00:43<00:21, 498.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|██████▋   | 21185/31866 [00:43<00:21, 492.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████▋   | 21236/31866 [00:43<00:21, 496.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████▋   | 21286/31866 [00:43<00:21, 487.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████▋   | 21335/31866 [00:44<00:22, 472.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████▋   | 21383/31866 [00:44<00:22, 468.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████▋   | 21436/31866 [00:44<00:21, 483.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████▋   | 21489/31866 [00:44<00:20, 495.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 21539/31866 [00:44<00:20, 493.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 21592/31866 [00:44<00:20, 500.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 21643/31866 [00:44<00:20, 487.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 21692/31866 [00:44<00:21, 483.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 21743/31866 [00:44<00:20, 489.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 21793/31866 [00:44<00:20, 484.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▊   | 21843/31866 [00:45<00:20, 488.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▊   | 21895/31866 [00:45<00:20, 495.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 21945/31866 [00:45<00:20, 483.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 21997/31866 [00:45<00:20, 492.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 22047/31866 [00:45<00:19, 494.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 22097/31866 [00:45<00:19, 492.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|██████▉   | 22148/31866 [00:45<00:19, 496.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|██████▉   | 22198/31866 [00:45<00:19, 487.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|██████▉   | 22247/31866 [00:45<00:19, 486.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|██████▉   | 22296/31866 [00:45<00:20, 474.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 22347/31866 [00:46<00:19, 483.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 22396/31866 [00:46<00:19, 481.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 22448/31866 [00:46<00:19, 491.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████   | 22498/31866 [00:46<00:19, 491.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████   | 22548/31866 [00:46<00:19, 489.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████   | 22600/31866 [00:46<00:18, 495.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████   | 22650/31866 [00:46<00:18, 488.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████   | 22701/31866 [00:46<00:18, 493.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████▏  | 22753/31866 [00:46<00:18, 498.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 22803/31866 [00:47<00:19, 474.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 22852/31866 [00:47<00:18, 476.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 22903/31866 [00:47<00:18, 484.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 22955/31866 [00:47<00:18, 492.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 23006/31866 [00:47<00:17, 497.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 23057/31866 [00:47<00:17, 501.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 23108/31866 [00:47<00:17, 489.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 23158/31866 [00:47<00:17, 486.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 23211/31866 [00:47<00:17, 497.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 23261/31866 [00:47<00:18, 463.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 23308/31866 [00:48<00:18, 456.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 23359/31866 [00:48<00:18, 470.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 23409/31866 [00:48<00:17, 476.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████▎  | 23462/31866 [00:48<00:17, 489.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████▍  | 23512/31866 [00:48<00:17, 486.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████▍  | 23561/31866 [00:48<00:17, 480.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████▍  | 23613/31866 [00:48<00:16, 490.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████▍  | 23663/31866 [00:48<00:17, 479.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████▍  | 23714/31866 [00:48<00:16, 486.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▍  | 23763/31866 [00:48<00:16, 478.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▍  | 23812/31866 [00:49<00:16, 479.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▍  | 23861/31866 [00:49<00:17, 466.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 23912/31866 [00:49<00:16, 477.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 23960/31866 [00:49<00:16, 465.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 24008/31866 [00:49<00:16, 469.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 24057/31866 [00:49<00:16, 474.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|███████▌  | 24105/31866 [00:49<00:17, 455.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|███████▌  | 24152/31866 [00:49<00:16, 458.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|███████▌  | 24199/31866 [00:49<00:16, 460.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|███████▌  | 24248/31866 [00:50<00:16, 467.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|███████▌  | 24297/31866 [00:50<00:15, 473.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|███████▋  | 24347/31866 [00:50<00:15, 479.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 24396/31866 [00:50<00:15, 473.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 24446/31866 [00:50<00:15, 480.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 24495/31866 [00:50<00:15, 463.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 24546/31866 [00:50<00:15, 474.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 24595/31866 [00:50<00:15, 477.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 24646/31866 [00:50<00:14, 486.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 24695/31866 [00:50<00:14, 480.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 24744/31866 [00:51<00:15, 471.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 24793/31866 [00:51<00:14, 474.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 24841/31866 [00:51<00:14, 471.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 24893/31866 [00:51<00:14, 483.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 24942/31866 [00:51<00:14, 484.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 24991/31866 [00:51<00:14, 485.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████▊  | 25040/31866 [00:51<00:14, 483.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████▊  | 25089/31866 [00:51<00:14, 474.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████▉  | 25138/31866 [00:51<00:14, 478.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████▉  | 25189/31866 [00:51<00:13, 486.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████▉  | 25238/31866 [00:52<00:13, 485.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████▉  | 25287/31866 [00:52<00:13, 474.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|███████▉  | 25338/31866 [00:52<00:13, 483.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|███████▉  | 25389/31866 [00:52<00:13, 491.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|███████▉  | 25439/31866 [00:52<00:13, 490.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|███████▉  | 25490/31866 [00:52<00:12, 494.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 25542/31866 [00:52<00:12, 501.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 25593/31866 [00:52<00:12, 486.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 25642/31866 [00:52<00:12, 482.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████  | 25691/31866 [00:53<00:12, 480.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████  | 25740/31866 [00:53<00:13, 460.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████  | 25791/31866 [00:53<00:12, 473.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████  | 25844/31866 [00:53<00:12, 487.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████▏ | 25895/31866 [00:53<00:12, 493.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████▏ | 25945/31866 [00:53<00:12, 482.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 25995/31866 [00:53<00:12, 486.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 26044/31866 [00:53<00:12, 477.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 26092/31866 [00:53<00:12, 473.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 26142/31866 [00:53<00:11, 478.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 26192/31866 [00:54<00:11, 484.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 26241/31866 [00:54<00:11, 478.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 26292/31866 [00:54<00:11, 485.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 26341/31866 [00:54<00:11, 472.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 26393/31866 [00:54<00:11, 482.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 26445/31866 [00:54<00:11, 490.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 26495/31866 [00:54<00:11, 482.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 26544/31866 [00:54<00:11, 481.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 26594/31866 [00:54<00:10, 486.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████▎ | 26643/31866 [00:55<00:11, 465.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████▍ | 26691/31866 [00:55<00:11, 468.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████▍ | 26743/31866 [00:55<00:10, 482.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████▍ | 26792/31866 [00:55<00:10, 477.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████▍ | 26841/31866 [00:55<00:10, 479.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████▍ | 26892/31866 [00:55<00:10, 486.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▍ | 26943/31866 [00:55<00:10, 491.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▍ | 26993/31866 [00:55<00:09, 489.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▍ | 27043/31866 [00:55<00:09, 491.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 27093/31866 [00:55<00:10, 468.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 27143/31866 [00:56<00:09, 475.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 27194/31866 [00:56<00:09, 485.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 27243/31866 [00:56<00:09, 478.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▌ | 27291/31866 [00:56<00:09, 478.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▌ | 27339/31866 [00:56<00:09, 479.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▌ | 27387/31866 [00:56<00:09, 476.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▌ | 27435/31866 [00:56<00:09, 457.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▌ | 27481/31866 [00:56<00:09, 457.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▋ | 27527/31866 [00:56<00:09, 454.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 27575/31866 [00:56<00:09, 460.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 27622/31866 [00:57<00:09, 463.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 27670/31866 [00:57<00:08, 468.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 27720/31866 [00:57<00:08, 475.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 27773/31866 [00:57<00:08, 489.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 27823/31866 [00:57<00:08, 482.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 27872/31866 [00:57<00:08, 483.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 27921/31866 [00:57<00:08, 474.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 27971/31866 [00:57<00:08, 480.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 28020/31866 [00:57<00:08, 454.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 28066/31866 [00:58<00:08, 447.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 28118/31866 [00:58<00:08, 465.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 28167/31866 [00:58<00:07, 472.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▊ | 28216/31866 [00:58<00:07, 476.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▊ | 28265/31866 [00:58<00:07, 478.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▉ | 28315/31866 [00:58<00:07, 483.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▉ | 28364/31866 [00:58<00:07, 476.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▉ | 28414/31866 [00:58<00:07, 481.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▉ | 28467/31866 [00:58<00:06, 494.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▉ | 28517/31866 [00:58<00:06, 482.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|████████▉ | 28566/31866 [00:59<00:06, 471.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|████████▉ | 28614/31866 [00:59<00:07, 463.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|████████▉ | 28661/31866 [00:59<00:06, 459.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 28711/31866 [00:59<00:06, 469.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 28759/31866 [00:59<00:06, 471.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 28809/31866 [00:59<00:06, 477.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████ | 28857/31866 [00:59<00:06, 473.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████ | 28905/31866 [00:59<00:06, 469.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████ | 28953/31866 [00:59<00:06, 463.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████ | 29000/31866 [00:59<00:06, 459.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████ | 29049/31866 [01:00<00:06, 467.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████▏| 29097/31866 [01:00<00:05, 469.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████▏| 29151/31866 [01:00<00:05, 487.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 29201/31866 [01:00<00:05, 489.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 29254/31866 [01:00<00:05, 499.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 29305/31866 [01:00<00:05, 497.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 29355/31866 [01:00<00:05, 496.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 29405/31866 [01:00<00:05, 484.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 29454/31866 [01:00<00:05, 476.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 29502/31866 [01:01<00:04, 473.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 29552/31866 [01:01<00:04, 478.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 29604/31866 [01:01<00:04, 488.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 29656/31866 [01:01<00:04, 495.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 29706/31866 [01:01<00:04, 487.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 29758/31866 [01:01<00:04, 493.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|█████████▎| 29809/31866 [01:01<00:04, 497.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|█████████▎| 29859/31866 [01:01<00:04, 496.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|█████████▍| 29912/31866 [01:01<00:03, 504.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|█████████▍| 29963/31866 [01:01<00:03, 478.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|█████████▍| 30014/31866 [01:02<00:03, 486.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|█████████▍| 30067/31866 [01:02<00:03, 497.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▍| 30118/31866 [01:02<00:03, 500.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▍| 30169/31866 [01:02<00:03, 493.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▍| 30220/31866 [01:02<00:03, 496.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▍| 30270/31866 [01:02<00:03, 492.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 30323/31866 [01:02<00:03, 501.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 30374/31866 [01:02<00:03, 492.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 30426/31866 [01:02<00:02, 498.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▌| 30476/31866 [01:02<00:02, 474.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▌| 30528/31866 [01:03<00:02, 485.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▌| 30578/31866 [01:03<00:02, 488.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▌| 30628/31866 [01:03<00:02, 489.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▋| 30678/31866 [01:03<00:02, 491.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▋| 30729/31866 [01:03<00:02, 494.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 30779/31866 [01:03<00:02, 482.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 30828/31866 [01:03<00:02, 473.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 30876/31866 [01:03<00:02, 461.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 30923/31866 [01:03<00:02, 462.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 30971/31866 [01:04<00:01, 465.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 31019/31866 [01:04<00:01, 468.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 31069/31866 [01:04<00:01, 477.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 31119/31866 [01:04<00:01, 482.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 31170/31866 [01:04<00:01, 490.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 31222/31866 [01:04<00:01, 497.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 31273/31866 [01:04<00:01, 499.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 31325/31866 [01:04<00:01, 504.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 31378/31866 [01:04<00:00, 507.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████▊| 31429/31866 [01:04<00:00, 472.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████▉| 31477/31866 [01:05<00:00, 470.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████▉| 31527/31866 [01:05<00:00, 477.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████▉| 31579/31866 [01:05<00:00, 487.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████▉| 31633/31866 [01:05<00:00, 501.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████▉| 31684/31866 [01:05<00:00, 490.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████▉| 31734/31866 [01:05<00:00, 476.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████▉| 31787/31866 [01:05<00:00, 490.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 31866/31866 [01:05<00:00, 483.90it/s]\n"
     ]
    }
   ],
   "source": [
    "hate_tox_test = find_hate_sentence(tox_test, 'comment', 'toxicity', hate_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "kKAsnBFtnD63",
    "outputId": "2d12415f-e5f4-4804-e865-ae711ee19d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11084 entries, 0 to 16187\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comment   11084 non-null  object \n",
      " 1   toxicity  11084 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 259.8+ KB\n"
     ]
    }
   ],
   "source": [
    "hate_tox_test.info()\n",
    "hate_tox_test.to_csv('Data/data_after_extraction/hate_tox_test.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "bPj_8YNkScfM"
   },
   "outputs": [],
   "source": [
    "# combine four tables as final table \n",
    "import re\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#define a list\n",
    "data_list = []\n",
    "file_in = \"Data/data_after_extraction/\"\n",
    "file_out = \"Data/final.csv\"\n",
    "\n",
    "for info in os.listdir(file_in): \n",
    "    domain = os.path.abspath(file_in)    #get file path\n",
    "    info = os.path.join(domain,info)    #combing file path and file name as final path\n",
    "    data = pd.read_csv(info,encoding='GBK')\n",
    "    data_list.append(data)\n",
    "\n",
    "# concat all data\n",
    "all_data = pd.concat(data_list)\n",
    "all_data.tail()\n",
    "\n",
    "#save to csv table\n",
    "all_data.to_csv(file_out,index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "uQvSx18jzHHA",
    "outputId": "8a3e57ab-d87d-46f1-e959-a5870711b0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41560 entries, 0 to 41559\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comment   41560 non-null  object \n",
      " 1   toxicity  41560 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 649.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# final table \n",
    "final = pd.read_csv('Data/final.csv', sep=',',encoding = \"ISO-8859-1\")\n",
    "final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohzNU5W6SnZa"
   },
   "source": [
    "\n",
    "# start model contruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "z88nrJBKu2Y3"
   },
   "outputs": [],
   "source": [
    "# data split into train val and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "text = final['comment']\n",
    "label = final['toxicity']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=42)\n",
    "\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "2wQBt448vZL8"
   },
   "outputs": [],
   "source": [
    "def pack_data(text, label, name):\n",
    "    GAB = {'text':text,'hate':label}\n",
    "    GAB_final = pd.DataFrame(GAB)\n",
    "    file_name = name+'.csv'\n",
    "    GAB_final.to_csv(file_name,index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Om_AyQtZvatI"
   },
   "outputs": [],
   "source": [
    "pack_data(x_train, y_train, 'Data/train')\n",
    "pack_data(x_test, y_test, 'Data/test')\n",
    "pack_data(x_valid, y_valid, 'Data/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "TFdWoyRy3qgh"
   },
   "outputs": [],
   "source": [
    "train_comment = pd.read_csv('Data/train.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "val_comment = pd.read_csv('Data/valid.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "test_comment = pd.read_csv('Data/test.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VN4hEtpNv2T1"
   },
   "source": [
    "# import package and strat building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "T_sbe2PfvxMN"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Preliminaries\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "# Models\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "KkRrBukSv_Md",
    "outputId": "fc72e012-7571-49f2-dd66-b09cc1e7beb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "cf964982dab9479a819404a399e988b1",
      "f646063967594ceeb081b58fe43c21d3",
      "422e0e70433e48cb9e0a5752e9e79c9f",
      "bc7dc0d20c57460eacce4436a5f656c8",
      "da0ee68a30b34f30bc5d975eb80bdb0f",
      "0a6332572d654876a58030b067e59545",
      "ab34625a4b7b4c918cf22ffda4617d2e",
      "2d161a14b91246d19fdc7e23fb09f9ff"
     ]
    },
    "id": "DUefzEn0wCOP",
    "outputId": "2d2262a0-1b14-4bca-a867-cfc922984ba2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf964982dab9479a819404a399e988b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "LLZsnwfNzm0l",
    "outputId": "ab616d85-c438-4025-c251-537248324b08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fae7179f1d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in final['comment']]\n",
    "pd.Series(seq_len).hist(bins = 50,range=[0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Uuq2xJ7j1_Qx"
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 300\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('comment', text_field),('toxicity', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
    "                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=8, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=8, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=8, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "_YBrN1G20L2j"
   },
   "outputs": [],
   "source": [
    "# model import \n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "        \n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "ZdxHvxVg0RDX"
   },
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "NDGfxSbr0UB-"
   },
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0 \n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_no = 1\n",
    "        for (comment, toxicity), _ in train_loader:\n",
    "            toxicity = toxicity.type(torch.LongTensor)           \n",
    "            toxicity = toxicity.to(device)\n",
    "            comment = comment.type(torch.LongTensor)  \n",
    "            comment = comment.to(device)\n",
    "            output = model(comment, toxicity)\n",
    "            loss, _ = output\n",
    "            print('batch_no [{}/{}]:'.format(batch_no, int(len(train_comment)/16)),'training_loss:',loss)\n",
    "            batch_no+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                    # validation loop\n",
    "                    for (comment, toxicity), _ in valid_loader:\n",
    "                        toxicity = toxicity.type(torch.LongTensor)           \n",
    "                        toxicity = toxicity.to(device)\n",
    "                        comment = comment.type(torch.LongTensor)  \n",
    "                        comment = comment.to(device)\n",
    "                        output = model(comment, toxicity)\n",
    "                        loss, _ = output\n",
    "                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NN2MApQz0Wm2",
    "outputId": "b260ac86-5636-4bc1-8656-c7d7966ce94c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "batch_no [3319/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/2078]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/2078]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/2078]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/2078]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/2078]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/2078]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/2078]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/2078]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/2078]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/2078]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/2078]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/2078]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/2078]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/2078]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/2078]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/2078]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/2078]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/2078]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/2078]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/2078]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/2078]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/2078]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/2078]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/2078]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/2078]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/2078]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/2078]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/2078]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/2078]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/2078]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/2078]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/2078]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/2078]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/2078]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/2078]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/2078]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/2078]: training_loss: tensor(0.2041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/2078]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/2078]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/2078]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/2078]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/2078]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/2078]: training_loss: tensor(0.5854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/2078]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/2078]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/2078]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/2078]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/2078]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/2078]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/2078]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/2078]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/2078]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/2078]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/2078]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/2078]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/2078]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/2078]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/2078]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/2078]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/2078]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/2078]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/2078]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/2078]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/2078]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/2078]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/2078]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/2078]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/2078]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/2078]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/2078]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/2078]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/2078]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/2078]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/2078]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/2078]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/2078]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/2078]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/2078]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/2078]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/2078]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/2078]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/2078]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/2078]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/2078]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/2078]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/2078]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/2078]: training_loss: tensor(0.9640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/2078]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/2078]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/2078]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/2078]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/2078]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/2078]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/2078]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/2078]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/2078]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/2078]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/2078]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/2078]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/2078]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/2078]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/2078]: training_loss: tensor(0.7436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/2078]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/2078]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/2078]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/2078]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/2078]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/2078]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/2078]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/2078]: training_loss: tensor(0.4907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/2078]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/2078]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/2078]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/2078]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/2078]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/2078]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/2078]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/2078]: training_loss: tensor(0.7026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/2078]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/2078]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/2078]: training_loss: tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/2078]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/2078]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/2078]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/2078]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/2078]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/2078]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/2078]: training_loss: tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/2078]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/2078]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/2078]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/2078]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/2078]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/2078]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/2078]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/2078]: training_loss: tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/2078]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/2078]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/2078]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/2078]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/2078]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/2078]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/2078]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/2078]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/2078]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/2078]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/2078]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/2078]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/2078]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/2078]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/2078]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/2078]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/2078]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/2078]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/2078]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/2078]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/2078]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/2078]: training_loss: tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/2078]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/2078]: training_loss: tensor(0.3550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/2078]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/2078]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/2078]: training_loss: tensor(0.9154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/2078]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/2078]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/2078]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/2078]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/2078]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/2078]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/2078]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/2078]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/2078]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/2078]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/2078]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/2078]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/2078]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/2078]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/2078]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/2078]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/2078]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/2078]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/2078]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/2078]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/2078]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/2078]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/2078]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/2078]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/2078]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/5], Step [16624/20780], Train Loss: 0.0260, Valid Loss: 0.2159\n",
      "batch_no [1/2078]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/2078]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/2078]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/2078]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/2078]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/2078]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/2078]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/2078]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/2078]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/2078]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/2078]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/2078]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/2078]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/2078]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/2078]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/2078]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/2078]: training_loss: tensor(0.4388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/2078]: training_loss: tensor(0.6243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/2078]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/2078]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/2078]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/2078]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/2078]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/2078]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/2078]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/2078]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/2078]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/2078]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/2078]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/2078]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/2078]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/2078]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/2078]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/2078]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/2078]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/2078]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/2078]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/2078]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/2078]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/2078]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/2078]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/2078]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/2078]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/2078]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/2078]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/2078]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/2078]: training_loss: tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/2078]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/2078]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/2078]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/2078]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/2078]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/2078]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/2078]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/2078]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/2078]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/2078]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/2078]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/2078]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/2078]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/2078]: training_loss: tensor(0.4647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/2078]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/2078]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/2078]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/2078]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/2078]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/2078]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/2078]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/2078]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/2078]: training_loss: tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/2078]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/2078]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/2078]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/2078]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/2078]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/2078]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/2078]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/2078]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/2078]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/2078]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/2078]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/2078]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/2078]: training_loss: tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/2078]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/2078]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/2078]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/2078]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/2078]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/2078]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/2078]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/2078]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/2078]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/2078]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/2078]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/2078]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/2078]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/2078]: training_loss: tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/2078]: training_loss: tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/2078]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/2078]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/2078]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/2078]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/2078]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/2078]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/2078]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/2078]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/2078]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/2078]: training_loss: tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/2078]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/2078]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/2078]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/2078]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/2078]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/2078]: training_loss: tensor(0.9722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/2078]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/2078]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/2078]: training_loss: tensor(0.8906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/2078]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/2078]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/2078]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/2078]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/2078]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/2078]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/2078]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/2078]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/2078]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/2078]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/2078]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/2078]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/2078]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/2078]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/2078]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/2078]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/2078]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/2078]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/2078]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/2078]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/2078]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/2078]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/2078]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/2078]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/2078]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/2078]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/2078]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/2078]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/2078]: training_loss: tensor(0.4140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/2078]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/2078]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/2078]: training_loss: tensor(0.5162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/2078]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/2078]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/2078]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/2078]: training_loss: tensor(0.3463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/2078]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/2078]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/2078]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/2078]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/2078]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/2078]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/2078]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/2078]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/2078]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/2078]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/2078]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/2078]: training_loss: tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/2078]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/2078]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/2078]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/2078]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/2078]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/2078]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/2078]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/2078]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/2078]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/2078]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/2078]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/2078]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/2078]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/2078]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/2078]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/2078]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/2078]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/2078]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/2078]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/2078]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/2078]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/2078]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/2078]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/2078]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/2078]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/2078]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/2078]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/2078]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/2078]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/2078]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/2078]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/2078]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/2078]: training_loss: tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/2078]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/2078]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/2078]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/2078]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/2078]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/2078]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/2078]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/2078]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/2078]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/2078]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/2078]: training_loss: tensor(0.4877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/2078]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/2078]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/2078]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/2078]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/2078]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/2078]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/2078]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/2078]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/2078]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/2078]: training_loss: tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/2078]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/2078]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/2078]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/2078]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/2078]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/2078]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/2078]: training_loss: tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/2078]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/2078]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/2078]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/2078]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/2078]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/2078]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/2078]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/2078]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/2078]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/2078]: training_loss: tensor(0.7822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/2078]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/2078]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/2078]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/2078]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/2078]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/2078]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/2078]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/2078]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/2078]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/2078]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/2078]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/2078]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/2078]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/2078]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/2078]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/2078]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/2078]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/2078]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/2078]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/2078]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/2078]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/2078]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/2078]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/2078]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/2078]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/2078]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/2078]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/2078]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/2078]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/2078]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/2078]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/2078]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/2078]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/2078]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/2078]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/2078]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/2078]: training_loss: tensor(0.8017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/2078]: training_loss: tensor(0.4820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/2078]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/2078]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/2078]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/2078]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/2078]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/2078]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/2078]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/2078]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/2078]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/2078]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/2078]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/2078]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/2078]: training_loss: tensor(0.4389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/2078]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/2078]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/2078]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/2078]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/2078]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/2078]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/2078]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/2078]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/2078]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/2078]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/2078]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/2078]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/2078]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/2078]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/2078]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/2078]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/2078]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/2078]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/2078]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/2078]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/2078]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/2078]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/2078]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/2078]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/2078]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/2078]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/2078]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/2078]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/2078]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/2078]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/2078]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/2078]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/2078]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/2078]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/2078]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/2078]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/2078]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/2078]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/2078]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/2078]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/2078]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/2078]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/2078]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/2078]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/2078]: training_loss: tensor(0.1726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/2078]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/2078]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/2078]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/2078]: training_loss: tensor(0.6659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/2078]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/2078]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/2078]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/2078]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/2078]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/2078]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/2078]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/2078]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/2078]: training_loss: tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/2078]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/2078]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/2078]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/2078]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/2078]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/2078]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/2078]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/2078]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/2078]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/2078]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/2078]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/2078]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/2078]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/2078]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/2078]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/2078]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/2078]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/2078]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/2078]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/2078]: training_loss: tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/2078]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/2078]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/2078]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/2078]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/2078]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/2078]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/2078]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/2078]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/2078]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/2078]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/2078]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/2078]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/2078]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/2078]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/2078]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/2078]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/2078]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/2078]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/2078]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/2078]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/2078]: training_loss: tensor(0.2726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/2078]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/2078]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/2078]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/2078]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/2078]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/2078]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/2078]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/2078]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/2078]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/2078]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/2078]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/2078]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/2078]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/2078]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/2078]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/2078]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/2078]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/2078]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/2078]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/2078]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/2078]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/2078]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/2078]: training_loss: tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/2078]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/2078]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/2078]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/2078]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/2078]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/2078]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/2078]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/2078]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/2078]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/2078]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/2078]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/2078]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/2078]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/2078]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/2078]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/2078]: training_loss: tensor(1.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/2078]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/2078]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/2078]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/2078]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/2078]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/2078]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/2078]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/2078]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/2078]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/2078]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/2078]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/2078]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/2078]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/2078]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/2078]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/2078]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/2078]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/2078]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/2078]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/2078]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/2078]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/2078]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/2078]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/2078]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/2078]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/2078]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/2078]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/2078]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/2078]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/2078]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/2078]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/2078]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/2078]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/2078]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/2078]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/2078]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/2078]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/2078]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/2078]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/2078]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/2078]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/2078]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/2078]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/2078]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/2078]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/2078]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/2078]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/2078]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/2078]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/2078]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/2078]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/2078]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/2078]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/2078]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/2078]: training_loss: tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/2078]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/2078]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/2078]: training_loss: tensor(1.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/2078]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/2078]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/2078]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/2078]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/2078]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/2078]: training_loss: tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/2078]: training_loss: tensor(0.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/2078]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/2078]: training_loss: tensor(0.8795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/2078]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/2078]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/2078]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/2078]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/2078]: training_loss: tensor(0.9563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/2078]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/2078]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/2078]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/2078]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/2078]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/2078]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/2078]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/2078]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/2078]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/2078]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/2078]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/2078]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/2078]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/2078]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/2078]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/2078]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/2078]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/2078]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/2078]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/2078]: training_loss: tensor(0.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/2078]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/2078]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/2078]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/2078]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/2078]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/2078]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/2078]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/2078]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/2078]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/2078]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/2078]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/2078]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/2078]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/2078]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/2078]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/2078]: training_loss: tensor(0.8491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/2078]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/2078]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/2078]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/2078]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/2078]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/2078]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/2078]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/2078]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/2078]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/2078]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/2078]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/2078]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/2078]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/2078]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/2078]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/2078]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/2078]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/2078]: training_loss: tensor(0.9150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/2078]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/2078]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/2078]: training_loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/2078]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/2078]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/2078]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/2078]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/2078]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/2078]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/2078]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/2078]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/2078]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/2078]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/2078]: training_loss: tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/2078]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/2078]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/2078]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/2078]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/2078]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/2078]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/2078]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/2078]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/2078]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/2078]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/2078]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/2078]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/2078]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/2078]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/2078]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/2078]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/2078]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/2078]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/2078]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/2078]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/2078]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/2078]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/2078]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/2078]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/2078]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/2078]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/2078]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/2078]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/2078]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/2078]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/2078]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/2078]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/2078]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/2078]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/2078]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/2078]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/2078]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/2078]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/2078]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/2078]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/2078]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/2078]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/2078]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/2078]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/2078]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/2078]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/2078]: training_loss: tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/2078]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/2078]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/2078]: training_loss: tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/2078]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/2078]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/2078]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/2078]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/2078]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/2078]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/2078]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/2078]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/2078]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/2078]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/2078]: training_loss: tensor(0.6157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/2078]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/2078]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/2078]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/2078]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/2078]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/2078]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/2078]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/2078]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/2078]: training_loss: tensor(0.4671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/2078]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/2078]: training_loss: tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/2078]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/2078]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/2078]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/2078]: training_loss: tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/2078]: training_loss: tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/2078]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/2078]: training_loss: tensor(0.5328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/2078]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/2078]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/2078]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/2078]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/2078]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/2078]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/2078]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/2078]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/2078]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/2078]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/2078]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/2078]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/2078]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/2078]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/2078]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/2078]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/2078]: training_loss: tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/2078]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/2078]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/2078]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/2078]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/2078]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/2078]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/2078]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/2078]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/2078]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/2078]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/2078]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/2078]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/2078]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/2078]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/2078]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/2078]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/2078]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/2078]: training_loss: tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/2078]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/2078]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/2078]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/2078]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/2078]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/2078]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/2078]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/2078]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/2078]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/2078]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/2078]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/2078]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/2078]: training_loss: tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/2078]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/2078]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/2078]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/2078]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/2078]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/2078]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/2078]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/2078]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/2078]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/2078]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/2078]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/2078]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/2078]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/2078]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/2078]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/2078]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/2078]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/2078]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/2078]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/2078]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/2078]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/2078]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/2078]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/2078]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/2078]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/2078]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/2078]: training_loss: tensor(0.3942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/2078]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/2078]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/2078]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/2078]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/2078]: training_loss: tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/2078]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/2078]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/2078]: training_loss: tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/2078]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/2078]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/2078]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/2078]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/2078]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/2078]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/2078]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/2078]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/2078]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/2078]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/2078]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/2078]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/2078]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/2078]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/2078]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/2078]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/2078]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/2078]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/2078]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/2078]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/2078]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/2078]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/2078]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/2078]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/2078]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/2078]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/2078]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/2078]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/2078]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/2078]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/2078]: training_loss: tensor(0.4210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/2078]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/2078]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/2078]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/2078]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/2078]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/2078]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/2078]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/2078]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/2078]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/2078]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/2078]: training_loss: tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/2078]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/2078]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/2078]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/2078]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/2078]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/2078]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/2078]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/2078]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/2078]: training_loss: tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/2078]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/2078]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/2078]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/2078]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/2078]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/2078]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/2078]: training_loss: tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/2078]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/2078]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/2078]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/2078]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/2078]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/2078]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/2078]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/2078]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/2078]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/2078]: training_loss: tensor(0.5362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/2078]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/2078]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/2078]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/2078]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/2078]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/2078]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/2078]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/2078]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/2078]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/2078]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/2078]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/2078]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/2078]: training_loss: tensor(0.5506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/2078]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/2078]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/2078]: training_loss: tensor(1.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/2078]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/2078]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/2078]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/2078]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/2078]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/2078]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/2078]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/2078]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/2078]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/2078]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/2078]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/2078]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/2078]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/2078]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/2078]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/2078]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/2078]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/2078]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/2078]: training_loss: tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/2078]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/2078]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/2078]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/2078]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/2078]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/2078]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/2078]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/2078]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/2078]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/2078]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/2078]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/2078]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/2078]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/2078]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/2078]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/2078]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/2078]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/2078]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/2078]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/2078]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/2078]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/2078]: training_loss: tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/2078]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/2078]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/2078]: training_loss: tensor(0.5560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/2078]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/2078]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/2078]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/2078]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/2078]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5], Step [18702/20780], Train Loss: 0.0459, Valid Loss: 0.2342\n",
      "batch_no [2079/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/2078]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/2078]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/2078]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/2078]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/2078]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/2078]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/2078]: training_loss: tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/2078]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/2078]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/2078]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/2078]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/2078]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/2078]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/2078]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/2078]: training_loss: tensor(0.4656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/2078]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/2078]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/2078]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/2078]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/2078]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/2078]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/2078]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/2078]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/2078]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/2078]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/2078]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/2078]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/2078]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/2078]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/2078]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/2078]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/2078]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/2078]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/2078]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/2078]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/2078]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/2078]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/2078]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/2078]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/2078]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/2078]: training_loss: tensor(0.4585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/2078]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/2078]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/2078]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/2078]: training_loss: tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/2078]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/2078]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/2078]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/2078]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/2078]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/2078]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/2078]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/2078]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/2078]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/2078]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/2078]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/2078]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/2078]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/2078]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/2078]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/2078]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/2078]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/2078]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/2078]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/2078]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/2078]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/2078]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/2078]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/2078]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/2078]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/2078]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/2078]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/2078]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/2078]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/2078]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/2078]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/2078]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/2078]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/2078]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/2078]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/2078]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/2078]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/2078]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/2078]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/2078]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/2078]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/2078]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/2078]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/2078]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/2078]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/2078]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/2078]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/2078]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/2078]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/2078]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/2078]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/2078]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/2078]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/2078]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/2078]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/2078]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/2078]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/2078]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/2078]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/2078]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/2078]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/2078]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/2078]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/2078]: training_loss: tensor(0.4158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/2078]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/2078]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/2078]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/2078]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/2078]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/2078]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/2078]: training_loss: tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/2078]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/2078]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/2078]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/2078]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/2078]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/2078]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/2078]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/2078]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/2078]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/2078]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/2078]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/2078]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/2078]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/2078]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/2078]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/2078]: training_loss: tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/2078]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/2078]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/2078]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/2078]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/2078]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/2078]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/2078]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/2078]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/2078]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/2078]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/2078]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/2078]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/2078]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/2078]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/2078]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/2078]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/2078]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/2078]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/2078]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/2078]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/2078]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/2078]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/2078]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/2078]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/2078]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/2078]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/2078]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/2078]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/2078]: training_loss: tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/2078]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/2078]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/2078]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/2078]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/2078]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/2078]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/2078]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/2078]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/2078]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/2078]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/2078]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/2078]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/2078]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/2078]: training_loss: tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/2078]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/2078]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/2078]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/2078]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/2078]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/2078]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/2078]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/2078]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/2078]: training_loss: tensor(0.9071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/2078]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/2078]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/2078]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/2078]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/2078]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/2078]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/2078]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/2078]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/2078]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/2078]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/2078]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/2078]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/2078]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/2078]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/2078]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/2078]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/2078]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/2078]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/2078]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/2078]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/2078]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/2078]: training_loss: tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/2078]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/2078]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/2078]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/2078]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/2078]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/2078]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/2078]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/2078]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/2078]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/2078]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/2078]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/2078]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/2078]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/2078]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/2078]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/2078]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/2078]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/2078]: training_loss: tensor(0.5217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/2078]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/2078]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/2078]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/2078]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/2078]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/2078]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/2078]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/2078]: training_loss: tensor(0.4229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/2078]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/2078]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/2078]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/2078]: training_loss: tensor(0.4857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/2078]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/2078]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/2078]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/2078]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/2078]: training_loss: tensor(0.5283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/2078]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/2078]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/2078]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/2078]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/2078]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/2078]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/2078]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/2078]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/2078]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/2078]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/2078]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/2078]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/2078]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/2078]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/2078]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/2078]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/2078]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/2078]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/2078]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/2078]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/2078]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/2078]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/2078]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/2078]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/2078]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/2078]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/2078]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/2078]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/2078]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/2078]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/2078]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/2078]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/2078]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/2078]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/2078]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/2078]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/2078]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/2078]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/2078]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/2078]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/2078]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/2078]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/2078]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/2078]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/2078]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/2078]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/2078]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/2078]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/2078]: training_loss: tensor(9.8164e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/2078]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/2078]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/2078]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/2078]: training_loss: tensor(9.7374e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/2078]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/2078]: training_loss: tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/2078]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/2078]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/2078]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/2078]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/2078]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/2078]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/2078]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/2078]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/2078]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/2078]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/2078]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/2078]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/2078]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/2078]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/2078]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/2078]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/2078]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/2078]: training_loss: tensor(0.8253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/2078]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/2078]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/2078]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/2078]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/2078]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/2078]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/2078]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/2078]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/2078]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/2078]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/2078]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/2078]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/2078]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/2078]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/2078]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/2078]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/2078]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/2078]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/2078]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/2078]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/2078]: training_loss: tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/2078]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/2078]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/2078]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/2078]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/2078]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/2078]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/2078]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/2078]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/2078]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/2078]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/2078]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/2078]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/2078]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/2078]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/2078]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/2078]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/2078]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/2078]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/2078]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/2078]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/2078]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/2078]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/2078]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/2078]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/2078]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/2078]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/2078]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/2078]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/2078]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/2078]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/2078]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/2078]: training_loss: tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/2078]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/2078]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/2078]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/2078]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/2078]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/2078]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/2078]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/2078]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/2078]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/2078]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/2078]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/2078]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/2078]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/2078]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/2078]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/2078]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/2078]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/2078]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/2078]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/2078]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/2078]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/2078]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/2078]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/2078]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/2078]: training_loss: tensor(0.6138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/2078]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/2078]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/2078]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/2078]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/2078]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/2078]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/2078]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/2078]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/2078]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/2078]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/2078]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/2078]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/2078]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/2078]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/2078]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/2078]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/2078]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/2078]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/2078]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/2078]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/2078]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/2078]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/2078]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/2078]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/2078]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/2078]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/2078]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/2078]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/2078]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/2078]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/2078]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/2078]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/2078]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/2078]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/2078]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5], Step [20780/20780], Train Loss: 0.0177, Valid Loss: 0.2719\n",
      "Model saved to ==> Model/metrics.pt\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "model = BERT().to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']    \n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "1QKFMmcxlMmt",
    "outputId": "6a3a289e-7dbf-473e-a216-5b7167595a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/metrics.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "5Gm5-o3d0a-v"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (comment,attack ), _ in test_loader:\n",
    "\n",
    "                comment = comment.type(torch.LongTensor)           \n",
    "                comment = comment.to(device)\n",
    "                attack = attack.type(torch.LongTensor)  \n",
    "                attack = attack.to(device)\n",
    "                output = model(comment, attack)\n",
    "\n",
    "                _, output = output\n",
    "                y_prob.extend(output.tolist())\n",
    "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "                y_true.extend(attack.tolist())\n",
    "    return y_true, y_pred,y_prob    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "NorsOCLulvA0",
    "outputId": "f251f0c4-48de-42df-adba-9446a89a2291"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model = BERT().to(device)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "\n",
    "y_true, y_pred,y_prob = evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "eUghL1pH0f6A"
   },
   "outputs": [],
   "source": [
    "label_true = []\n",
    "for i in y_true:\n",
    "    if i == 1:\n",
    "        label_true.append([1,0])\n",
    "    else:\n",
    "        label_true.append([0,1])\n",
    "\n",
    "y_prob_final = []\n",
    "for i in range(len(y_prob)):\n",
    "    tempA = abs(y_prob[i][0])\n",
    "    tempB = abs(y_prob[i][1])\n",
    "    y_prob_final.append(tempA/(tempA+tempB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "AIc6djX3mx-i",
    "outputId": "50cee812-486d-4cac-d0f8-44a10fda0135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6852    0.3279    0.4436       677\n",
      "           0     0.9430    0.9866    0.9643      7635\n",
      "\n",
      "    accuracy                         0.9330      8312\n",
      "   macro avg     0.8141    0.6573    0.7040      8312\n",
      "weighted avg     0.9220    0.9330    0.9219      8312\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score: 0.6572788768199006\n"
     ]
    }
   ],
   "source": [
    "# in domain test\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print('auc score:', roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "gQCiJOfPnnaD"
   },
   "outputs": [],
   "source": [
    "# test on 2400 hand labelled data\n",
    "# Model parameter\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('comment', text_field),('toxicity', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
    "                                           test='test5.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "test_iter = Iterator(test, batch_size=8, device=device, train=False, shuffle=False, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "CHivv2NhnZBx",
    "outputId": "3393f055-dc9f-4892-bc4a-dfdec1cceddb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model = BERT().to(device)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "\n",
    "y_true, y_pred,y_prob = evaluate(best_model, test_iter)\n",
    "\n",
    "label_true = []\n",
    "for i in y_true:\n",
    "    if i == 1:\n",
    "        label_true.append([1,0])\n",
    "    else:\n",
    "        label_true.append([0,1])\n",
    "        \n",
    "\n",
    "y_prob_final = []\n",
    "for i in range(len(y_prob)):\n",
    "    tempA = abs(y_prob[i][0])\n",
    "    tempB = abs(y_prob[i][1])\n",
    "    y_prob_final.append(tempA/(tempA+tempB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "sC7Dl76WnjxO",
    "outputId": "8ed26344-7399-4431-cc87-be075b5754c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4355    0.4233    0.4293       678\n",
      "           0     0.7645    0.7733    0.7689      1641\n",
      "\n",
      "    accuracy                         0.6710      2319\n",
      "   macro avg     0.6000    0.5983    0.5991      2319\n",
      "weighted avg     0.6683    0.6710    0.6696      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score: 0.5983063963803638\n"
     ]
    }
   ],
   "source": [
    "# test on 2400 hand labelled data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Toxicity', 'Not Toxicity'])\n",
    "ax.yaxis.set_ticklabels(['Toxicity', 'Not Toxicity'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# auc score\n",
    "print('auc score:', roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-IDkDudomhm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Handle_GAB_relabel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a6332572d654876a58030b067e59545": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d161a14b91246d19fdc7e23fb09f9ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "422e0e70433e48cb9e0a5752e9e79c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a6332572d654876a58030b067e59545",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da0ee68a30b34f30bc5d975eb80bdb0f",
      "value": 231508
     }
    },
    "ab34625a4b7b4c918cf22ffda4617d2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc7dc0d20c57460eacce4436a5f656c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d161a14b91246d19fdc7e23fb09f9ff",
      "placeholder": "​",
      "style": "IPY_MODEL_ab34625a4b7b4c918cf22ffda4617d2e",
      "value": " 232k/232k [00:00&lt;00:00, 3.54MB/s]"
     }
    },
    "cf964982dab9479a819404a399e988b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_422e0e70433e48cb9e0a5752e9e79c9f",
       "IPY_MODEL_bc7dc0d20c57460eacce4436a5f656c8"
      ],
      "layout": "IPY_MODEL_f646063967594ceeb081b58fe43c21d3"
     }
    },
    "da0ee68a30b34f30bc5d975eb80bdb0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f646063967594ceeb081b58fe43c21d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
