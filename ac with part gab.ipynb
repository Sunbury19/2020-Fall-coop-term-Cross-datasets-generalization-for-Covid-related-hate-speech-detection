{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "racism_word = pd.read_csv('racism_word.csv')\n",
    "GAB = pd.read_csv('gab_data/GabHateCorpus_annotations.tsv', encoding='utf-8',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "racist_word_list = racism_word['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "racist_word_list = [i.lower() for i in racist_word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "\n",
    "class Word_Preprocessing():\n",
    "    def eliminate_url(self,df,target):\n",
    "        print('Start eliminate url: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        text = df_temp[target_column_name]\n",
    "        for i in tqdm(text):\n",
    "            urls = re.findall(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', i)\n",
    "            for i in urls:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_username(self,df,target):\n",
    "        print('Start eliminate username: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'@\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "     \n",
    "    def eliminate_hashtag(self, df, target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'#\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "        \n",
    "    \n",
    "    def eliminate_symbol(self,df,target):\n",
    "        print('Start eliminate symbol: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        symbol_list = [',',\"'\",'!','@','$','%','^','&','*','(',')','-','+','?','>','<','=','.',':',';','  ','  ','   ','    ','      ','      ','  ']\n",
    "        for i in tqdm(symbol_list):\n",
    "            df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, ' '))\n",
    "        return df_temp\n",
    "    \n",
    "    def process_all(self, df,target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        df_remove_url = self.eliminate_url(df_temp,target_column_name)\n",
    "        df_eliminate_hashtag = self.eliminate_hashtag(df_remove_url, target_column_name)\n",
    "        df_remove_username = self.eliminate_username(df_eliminate_hashtag, target_column_name)\n",
    "        df_remove_symbol = self.eliminate_symbol(df_remove_username, target_column_name)\n",
    "        print(\"finished!!\")\n",
    "        return df_remove_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_preprocesser = Word_Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 28/86529 [00:00<05:57, 242.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate url: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86529/86529 [02:25<00:00, 594.42it/s] \n",
      "100%|██████████| 86529/86529 [00:02<00:00, 37267.54it/s]\n",
      "  0%|          | 123/86529 [00:00<01:50, 781.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate username: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86529/86529 [00:02<00:00, 34388.98it/s]\n",
      " 11%|█         | 3/27 [00:00<00:00, 29.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate symbol: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:01<00:00, 26.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GAB = word_preprocesser.process_all(GAB, 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def find_hate_sentence(df):\n",
    "    hate_sentence = []\n",
    "    for sentence in tqdm(df):\n",
    "        for word in racist_word_list:\n",
    "            if sentence.find(word)>0:\n",
    "                hate_sentence.append(sentence)\n",
    "    return list(np.unique(hate_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86529/86529 [00:09<00:00, 9334.91it/s]\n"
     ]
    }
   ],
   "source": [
    "new_gab = find_hate_sentence(GAB['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4899"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_gab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86529"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_gab is the filtered gab data based on rasicm word table\n",
    "# this new table should be divided in a specific ratio and then add to crosponding\n",
    "# train/dev/test attack comment tables and train again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
